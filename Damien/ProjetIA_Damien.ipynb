{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMW4aZUWKKgr"
   },
   "source": [
    "# Projet IA - GODELLE Damien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après plusieurs tentatives de faire la récupération des images et profils des caractères via les API en local, j'ai finalement basculé sur un environnement virtuel dont ce notebook reprend le contenu. J'avais également tenté de récupérer l'audio ou de récupérer plusieurs images mais cela ne fonctionnait pas correctement voir pas du tout en local donc j'ai abandonné cette partie et ai essayé d'avoir une API Wikipédia et AniList fonctionnel.\n",
    "\n",
    "Actuellement j'ai donc:\n",
    "- Utilisé l'API Wikipédia pour récupérer la description des pages que l'on souhaite puis la création d'un résumé de ces pages pour que cela soit plus pratique pour la suite.\n",
    "- Utilisé l'API de Anilist pour récupérer la description d'un caractère ou d'un manga/animé \n",
    "\n",
    "Les 2 codes permettent de télécharger des fichiers en .json et .html avec les descriptions souhaitées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après plusieurs tentatives de faire la récupération des images et profils des caractères via les API en local, j'ai finalement basculé sur un environnement virtuel dont ce notebook reprend le contenu. J'avais également tenté de récupérer l'audio ou de récupérer plusieurs images mais cela ne fonctionnait pas correctement voir pas du tout en local donc j'ai abandonné cette partie et ai essayé d'avoir une API Wikipédia et AniList fonctionnel.\n",
    "\n",
    "Actuellement j'ai donc:\n",
    "- Utilisé l'API Wikipédia pour récupérer la description des pages que l'on souhaite puis la création d'un résumé de ces pages pour que cela soit plus pratique pour la suite.\n",
    "- Utilisé l'API de Anilist pour récupérer la description d'un caractère ou d'un manga/animé \n",
    "\n",
    "Les 2 codes permettent de télécharger des fichiers en .json et .html avec les descriptions souhaitées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkTW5gzGOppC"
   },
   "source": [
    "## Préliminaire: Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3LtZ8zbXNu-z",
    "outputId": "6f3055c8-bbb2-4d6c-a40b-8478cdac944c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from wikipedia) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from wikipedia-api) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyannote.audio in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: asteroid-filterbanks>=0.4 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (0.4.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (0.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (0.27.1)\n",
      "Requirement already satisfied: lightning>=2.0.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (2.5.0.post0)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (2.3.0)\n",
      "Requirement already satisfied: pyannote.core>=5.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (5.0.0)\n",
      "Requirement already satisfied: pyannote.database>=5.0.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (5.1.3)\n",
      "Requirement already satisfied: pyannote.metrics>=3.2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (3.2.1)\n",
      "Requirement already satisfied: pyannote.pipeline>=3.0.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (3.0.1)\n",
      "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (2.8.1)\n",
      "Requirement already satisfied: rich>=12.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (13.3.5)\n",
      "Requirement already satisfied: semver>=3.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (3.0.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (0.13.1)\n",
      "Requirement already satisfied: speechbrain>=1.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (1.0.2)\n",
      "Requirement already satisfied: tensorboardX>=2.6 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (2.6.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (2.5.1)\n",
      "Requirement already satisfied: torch-audiomentations>=0.11.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (0.12.0)\n",
      "Requirement already satisfied: torchaudio>=2.2.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (2.5.1)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.audio) (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.66.4)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from lightning>=2.0.1->pyannote.audio) (0.11.9)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from lightning>=2.0.1->pyannote.audio) (2.5.0.post0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.14.1)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
      "Requirement already satisfied: typer>=0.12.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.15.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.4.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.9.2)\n",
      "Requirement already satisfied: sympy>=1.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.13.1)\n",
      "Requirement already satisfied: optuna>=3.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (4.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (2.15.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->pyannote.audio) (1.16.0)\n",
      "Requirement already satisfied: hyperpyyaml in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (1.2.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (1.4.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboardX>=2.6->pyannote.audio) (3.20.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.2.7)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (1.2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.21)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.9.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.14.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.30)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (2.2.0)\n",
      "Requirement already satisfied: primePy>=1.3 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio) (1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.0->pyannote.audio) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.18.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->pyannote.audio) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2024.8.30)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.9.3)\n",
      "Requirement already satisfied: Mako in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.16.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.2.12)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydub in c:\\users\\mbiver\\appdata\\local\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia\n",
    "%pip install wikipedia-api\n",
    "%pip install pyannote.audio\n",
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USf85oKLKRxY"
   },
   "source": [
    "## Partie récupération avec API Wikipéddia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vZ5L4ddOKZPd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données enregistrées en HTML dans : C:\\Users\\mbiver\\Desktop\\IA\\Projet\\wikipedia_data.html\n",
      "{'titre': 'Rāmen', 'resume': \"Un rāmen (ラーメン, prononcé en français [ʀamɛn] ou [lamɛn]) est une recette de cuisine japonaise héritée de la cuisine chinoise et adaptée au goût des habitants de l'archipel nippon. Le mot rāmen est emprunté par le japonais de son nom original chinois en mandarin [lāmiàn] (拉麵, 拉面 'nouilles tirées'). Dans sa forme traditionnelle, il s'agit d'une soupe de nouilles, à base de bouillon agrémenté de nombreuses variantes d'ingrédients animaux, végétaux et aromates (poissons, viandes, légumes, algues, œuf, etc.). Importé de Chine à la fin du XIXe siècle, il est à ce jour considéré comme faisant partie des plats emblématiques de la gastronomie japonaise.\", 'sections': [{'titre': 'Historique', 'contenu': \"Plat dont les premières versions étaient d'origine chinoise, le rāmen (ラーメン／拉麺／老麺／柳麺) tirerait son nom actuel des lamians (拉面 / 拉麺, lā miàn, « nouilles tirées »), des pâtes de blé tirées à la main par le cuisinier, une des spécialités de la minorité musulmane Hui de Lanzhou dans la province du Gansu en Chine, dont la version la plus connue est au bœuf.\\nLe premier Japonais à avoir goûté au rāmen serait Tokugawa Mitsukuni (1628-1701), seigneur du clan Mito, à l’époque d'Edo,. Un lettré chinois en exil au Japon, Zhu Zhiyu (en) (1600-1682), lui aurait présenté des nouilles composées de farine de blé tendre et de poudre de racine de lotus, une soupe de nouilles proche des udon. Une variété de rāmen est commercialisée sous l’appellation Mito-han rāmen (rāmen du clan Mito) sur l’appui de cette légende, dans la ville de Mito.\\nLe rāmen a véritablement été importé au Japon fin XIXe siècle, début XXe siècle (ère Meiji) par des immigrés chinois vivant dans le quartier de Yokohama. C'est également à Yokohama que la première boutique de rāmen a été créé, cuisiné par des immigrés chinois. Elle proposait une soupe aux nouilles chinoises dans un bouillon, avec du rôti de porc, des pousses de bambou et un demi-œuf dur. Ces nouilles étaient appelées alors « soba chinoises » (中華そば, chūka soba), « soba de Chine » (支那蕎麦, Shina soba) ou « soba de Nankin ». Le rāmen est maintenant considéré comme un plat japonais.\\n\\nD'autres sources renvoient vers le restaurant Rairaiken (来々軒) d'Asakusa, qui avait embauché un chef du quartier chinois de Yokohama pour son ouverture en 1910. Il s'agissait alors de shio rāmen à base de sel, les Japonais y ajoutant plus tard leur sauce de soja, créant le shōyu rāmen qui s'est répandu dans l'archipel dans les années 1920. Dans les années 1930, les Chinois font connaître le rāmen dans d'autres régions du Japon, notamment à Sapporo (Hokkaidō), Kitakata (Fukushima) ou Kurume (Fukuoka).\\nLe rāmen devient un plat japonais populaire après la Seconde Guerre mondiale, via l’émergence de stands ambulants de rāmen, qui est alors un plat complet chaud, économique et nourrissant, la farine de blé utilisée pour les préparer étant à cette époque plus facile à se procurer que le riz. En 1958, Nissin Foods lance les premiers rāmen instantanés, puis en 1971 ses fameuses Cup Noodle, la réponse locale à McDonald's qui s'implante la même année au Japon. Les rāmen locaux (rāmen au miso de Sapporo ou tonkotsu rāmen de Hakata), jusque-là considérés comme des spécialités régionales, conquièrent le Japon à partir de 1965, en une décennie, sous forme de ces nouilles instantanées ou grâce à l’ouverture d’enseignes franchisées.\", 'sous_sections': []}, {'titre': 'Description', 'contenu': \"Les nouilles rāmen sont servies dans un grand bol de bouillon et peuvent être cuisinées selon d'innombrables variantes de recettes. Il est cependant communément admis que cinq grands principes sont réunis pour composer un plat de rāmen à la japonaise traditionnel :\\n\\nle tare (une sorte de fond riche en goût) ;\\nle bouillon ;\\nles huiles aromatiques ;\\nles accompagnements ;\\nles nouilles.\\n\\n\\tQuelques exemples\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\nChaque région du Japon dispose de ses propres recettes de rāmen, de cuisine régionale japonaise, qui a évolué avec le temps, avec en particulier (classées du sud au nord de l'archipel) :\\n\\nKagoshima rāmen (鹿児島ラーメン).\\nKumamoto rāmen (熊本ラーメン).\\nKurume rāmen (久留米ラーメン).\\nHakata rāmen (博多ラーメン).\\nOnomichi rāmen (尾道ラーメン).\\nTokushima rāmen (徳島ラーメン).\\nWakayama rāmen (和歌山ラーメン).\\nKyoto rāmen (京都ラーメン).\\nTaïwan rāmen (台湾ラーメン) de Nagoya.\\nYokohama Iekei ramen (ja) (横浜家系ラーメン).\\nAburasoba (en) (油そば) de Tokyo.\\nTokyo tsukemen (東京つけ麺).\\nTokyo rāmen (東京ラーメン).\\nTsubame-Sanjo rāmen (燕三条ラーメン).\\nShirakawa rāmen (白河ラーメン).\\nKitakata rāmen (喜多方ラーメン).\\nAkayu rāmen (赤湯ラーメン).\\nHakodate rāmen (函館ラーメン).\\nSapporo rāmen (札幌ラーメン).\\nAsahikawa rāmen (旭川ラーメン).\", 'sous_sections': [{'titre': 'Types de bouillon et tare', 'contenu': \"Les grandes familles de rāmen sont définies suivant leur base aromatique (tare) et/ou le type de bouillon qu'elles emploient.\\nLes recettes de tare les plus régulièrement proposées au Japon sont à base de sauce soja (shoyu rāmen), de sels (shio rāmen) ou de miso (miso rāmen).\\nComposé de légumes et le plus souvent de produits animaux, le bouillon est décrit comme étant léger (assari / chintan) ou épais (kotteri / paitan) suivant son mode de cuisson. Les bases les plus couramment citées sont le tonkotsu (os de porc), gyokai (fruits de mer), tori (volaille) et gyukotsu (os de bœuf).\\nIl existe un très grand nombre de variétés de rāmen accompagnés ou non de viandes ou de poisson et certaines régions du Japon sont réputées pour leurs spécialités de rāmen, comme Hokkaidō ou Kyūshū,.\", 'sous_sections': []}, {'titre': 'Saveurs', 'contenu': \"Les résultats de combinaison sont généralement divisés en de nombreuses catégories de saveur. Plusieurs anciennes variétés existent.\\n\\nLe ramen shōyu est un bouillon brun clair à base de poulet et de légumes (ou parfois de poisson ou de bœuf) contenant amplement de sauce de soja, ce qui donne une soupe à la fois piquante, salée tout en demeurant légère au goût. Le ramen shōyu contient habituellement des nouilles ondulées plutôt que droites. Il est souvent agrémenté de menma, d'oignons verts, de carottes, de kamaboko, une tranche de rouleau de poisson transformé et servie en forme de cercle dentelé avec une spirale rose ou rouge appelée narutomaki, de nori, d'œufs à la coque, de pousses de soja ou de poivre. La soupe peut aussi contenir de l'huile de chili ou des épices chinoises, et certains commerces servent du bœuf tranché plutôt que l'habituel chāshū.\\nLe ramen shio est le plus ancien des quatre variétés. Celui-ci a un bouillon pâle, clair et jaunâtre très salé et accompagné de toute combinaison de poulet, de légumes, de poisson et d'algues. Des os de porc sont parfois aussi ajoutés, mais ils ne sont pas bouillis aussi longtemps qu'ils le sont pour le ramen tonkotsu. Ainsi, la soupe demeure légère et claire. Le chāshū est parfois substitué à des boulettes de poulet maigre, et de prunes marinées ainsi que de kamaboko sont également des garnitures populaires pour ce type de ramen. Les nouilles varient de la texture à l'épaisseur pour le ramen shio, mais elles sont généralement droites plutôt qu'ondulées. « Le ramen de Hakodate » est une version reconnue du ramen shio au Japon.\\n\\nLe ramen miso est un ramen relativement nouveau ayant connu une notoriété nationale vers 1965. Ce ramen uniquement japonais, qui a été créé à Sapporo (Hokkaidō), se caractérise par un bouillon combinant un miso mélangé à du gras de poulet ou du bouillon de poisson, parfois avec du tonkotsu ou du lard, pour créer une soupe épaisse dont la texture est comparable à du beurre de noix, légèrement sucrée et consistante. Le bouillon du ramen miso a tendance à avoir une saveur robuste et piquante qui s'accompagne communément de la variété de garnitures suivantes : pâte de haricots piquante ou tōbanjan, beurre et maïs, poireaux, oignons, pousses de soja, bœuf haché, chou, graines de sésame, poivre blanc et ail haché. Les nouilles sont épaisses, ondulées et légèrement moelleuses.\\nLe ramen karē est un ramen cuisiné avec de la soupe de curry. Il semble avoir été créé plutôt récemment au Japon. Plusieurs villes japonaises revendiquent être l'endroit d'origine. La ville de Muroran revendique sa création en 1965, alors que la ville de Sanjō réclame avoir eu le ramen karē depuis plus de quatre-vingts ans, et la ville de Katori revendique aussi avoir été le site de son origine. La soupe est principalement faite d'os de porc et de légumes et est assaisonnée avec du curry. Les nouilles sont épaisses et ondulées. Les garnitures incluent le chāshū, le wakame et de pousses de soja.\", 'sous_sections': []}, {'titre': 'Accompagnements', 'contenu': \"Il existe de nombreuses variantes de rāmen, et nombre d'accompagnement possibles. Parmi les plus classiques on retrouve de la viande, souvent du chāshū (porc longuement braisé), des œufs marinés (ajitsuke tamago, sorte d'œufs mollets marinés dans un bouillon de sauce soja), du menma (bambou fermenté), de la ciboule et des nori (algues séchées). D'autres versions peuvent inclure des champignons noirs, du beurre, du maïs, de l'ail ou un morceau de narutomaki en forme de spirale (uzumaki).\", 'sous_sections': []}, {'titre': 'Les nouilles', 'contenu': \"Les nouilles sont produites droites ou ondulées, de diverses épaisseurs et longueurs. La plupart des nouilles sont faites à partir de quatre ingrédients de base : farine de blé, eau, sel et kansui (un type d'eau minérale alcaline, contenant du carbonate de sodium et de potassium, ainsi que, parfois, une petite quantité d'acide phosphorique). À l'origine, le kansui était récupéré dans certains lacs de Mongolie qui contenaient de grandes quantités de ces minéraux ou était puisée dans certains puits aux eaux particulières. L'utilisation du kansui donne aux nouilles une teinte jaunâtre ainsi qu'une texture ferme. On peut aussi rajouter des œufs pour améliorer la couleur, le goût et la texture. Pendant une brève période après la Seconde Guerre mondiale, de faibles quantités de kansui contaminé ont été vendues, mais le kansui est maintenant utilisé selon les normes JAS (Japanese Agricultural Standard)[réf. nécessaire]. Du carbonate de sodium peut également être substitué au kansui.\", 'sous_sections': []}, {'titre': 'Les bols à rāmen', 'contenu': 'De nombreux types de bol existent pour servir le rāmen. Le bol choisi pour servir le plat dépend du type de rāmen, de sa composition et de ses garnitures. Chaque bol à rāmen présente des formes variées dont les plus connues sont :[réf. nécessaire]', 'sous_sections': []}]}, {'titre': 'Économie', 'contenu': 'En 2013, un site de « statistiques et classements par préfectures » comptait 35 330 restaurants de rāmen au Japon, la densité la plus élevée étant dans la préfecture de Yamagata. En 2012, un bol de rāmen coûte entre 600 et 900 yens (entre cinq et huit euros).\\nOn comptabilise en 2008 plus de 65 milliards de bols de rāmen instantanées vendus dans le monde.', 'sous_sections': []}, {'titre': 'Culture', 'contenu': '', 'sous_sections': [{'titre': 'Restaurants et yatai', 'contenu': \"Il existe plusieurs types d'endroits où acheter des ramen. Par exemple les yatai, des établissements temporaires généralement ambulants (tirés via des charrettes à bras ou des véhicules à moteur) ou des restaurants en dur, souvent ouverts tard le soir (une soirée nomikai dans un bar-restaurant izakaya peut par exemple se terminer par un grand bol de rāmen).\\n\\n\\tQuelques lieux de restauration\", 'sous_sections': []}, {'titre': 'Magazines', 'contenu': 'Il existe au Japon des magazines consacrés aux rāmen et aux amateurs de rāmen, avec reportages sur des restaurants célèbres, des comparatifs, etc..', 'sous_sections': []}, {'titre': 'Manga', 'contenu': 'Naruto Uzumaki, personnage principal du manga Naruto, ne se nourrit que de rāmen et de nouilles instantanées.', 'sous_sections': []}, {'titre': 'Télévision', 'contenu': \"Dans l'épisode 128 de Naruto Shippûden, on apprend que son prénom fait référence au narutomaki, l'un des composants additionnels du rāmen, un ingrédient souple et plat, la plupart du temps blanc avec une spirale rose, s'apparentant au surimi.\", 'sous_sections': []}, {'titre': 'Cinéma', 'contenu': \"1985 : Tampopo (Pissenlit), de Jūzō Itami. Ce film, qui relate la quête d'une restauratrice japonaise pour trouver le rāmen idéal, a renforcé la popularité de ce plat et lui a donné ses lettres de noblesse,.\\n2018 : La Saveur des rāmen, du réalisateur singapourien  Eric Khoo, mettant en scène de nombreuses préparations culinaires du Japon et de Singapour au travers de la quête d'identité du personnage principal.\\n2022 : Umami, de Slony Sow, avec Gérard Depardieu et Pierre Richard.\", 'sous_sections': []}, {'titre': 'Musées', 'contenu': \"Le rāmen est tellement populaire au Japon qu'un musée lui est entièrement consacré depuis 1994, le musée du rāmen de Shin-Yokohama.\\nUn second musée CupNoodles Museum Yokohama (en) a été inauguré en 2017 à Yokohama.\", 'sous_sections': []}]}, {'titre': 'Notes et références', 'contenu': '(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Ramen » (voir la liste des auteurs).', 'sous_sections': []}, {'titre': 'Voir aussi', 'contenu': '', 'sous_sections': [{'titre': 'Articles connexes', 'contenu': 'Art culinaire\\nBento\\nCuisine japonaise\\nCuisine régionale japonaise\\nCulture japonaise\\nGastronomie japonaise\\nHistoire de la cuisine japonaise\\nJjapaguri\\nListe de soupes\\nSoupe miso\\nSoupe de nouilles\\nYakisoba, des nouilles chinoises sautées utilisant des pâtes similaires à celles du rāmen.', 'sous_sections': []}, {'titre': 'Liens externes', 'contenu': '\\n(ja) Ramen Database\\n(ja) Ramen Walker\\n[vidéo] « Ramen Museum In Yokohama, Japan », sur YouTube\\n\\n Portail de la cuisine japonaise', 'sous_sections': []}]}]}\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import wikipediaapi\n",
    "import json\n",
    "import os\n",
    "\n",
    "class WikipediaFetcher:\n",
    "    \"\"\"\n",
    "    Classe pour récupérer les données d'une page Wikipédia avec des fonctionnalités avancées.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, language=\"en\", user_agent=\"DamienGodelleProject/1.0 (damien.godelle@gmail.com)\"):\n",
    "        self.language = language\n",
    "        self.user_agent = user_agent\n",
    "        self.wiki_client = wikipediaapi.Wikipedia(language=language, user_agent=user_agent)\n",
    "        wikipedia.set_lang(language)\n",
    "\n",
    "    def extract_sections(self, sections):\n",
    "        \"\"\"\n",
    "        Parcourt récursivement les sections et sous-sections d'une page Wikipédia.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"titre\": section.title,\n",
    "                \"contenu\": section.text[:],\n",
    "                \"sous_sections\": self.extract_sections(section.sections),\n",
    "            }\n",
    "            for section in sections\n",
    "        ]\n",
    "\n",
    "    def search_titles(self, query, max_results=5):\n",
    "        \"\"\"\n",
    "        Recherche les titres de pages correspondantes à une requête donnée.\n",
    "        \"\"\"\n",
    "        results = wikipedia.search(query, results=max_results, suggestion=True)\n",
    "        if not results or not results[0]:\n",
    "            raise ValueError(\"Aucun résultat trouvé pour cette recherche.\")\n",
    "        return results[0]  # Renvoie les propositions\n",
    "\n",
    "    def fetch_page(self, title):\n",
    "        \"\"\"\n",
    "        Récupère une page Wikipédia par son titre.\n",
    "        \"\"\"\n",
    "        page = self.wiki_client.page(title)\n",
    "        if not page.exists():\n",
    "            raise ValueError(f\"La page '{title}' n'existe pas.\")\n",
    "        return page\n",
    "\n",
    "    def get_page_data(self, query, summary_only=False, save_path=None, save_as_html=False):\n",
    "        \"\"\"\n",
    "        Récupère les données d'une page Wikipédia avec ses sections.\n",
    "        \"\"\"\n",
    "        title = self.search_titles(query)[0]\n",
    "        page = self.fetch_page(title)\n",
    "        data = {\n",
    "            \"titre\": page.title,\n",
    "            \"resume\": page.summary,\n",
    "        }\n",
    "\n",
    "        if not summary_only:\n",
    "            data[\"sections\"] = self.extract_sections(page.sections)\n",
    "\n",
    "        if save_path:\n",
    "            if save_as_html:\n",
    "                self.save_to_html(data, save_path)\n",
    "            else:\n",
    "                self.save_to_json(data, save_path)\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(data, path):\n",
    "        \"\"\"\n",
    "        Enregistre les données dans un fichier JSON.\n",
    "        \"\"\"\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "            print(f\"Données enregistrées dans : {os.path.abspath(path)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_html(data, path):\n",
    "        \"\"\"\n",
    "        Enregistre les données dans un fichier HTML.\n",
    "        \"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>{data['titre']}</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>{data['titre']}</h1>\n",
    "            <p><strong>Résumé :</strong> {data['resume']}</p>\n",
    "        \"\"\"\n",
    "\n",
    "        if \"sections\" in data:\n",
    "            html_content += \"<h2>Sections</h2><ul>\"\n",
    "            for section in data[\"sections\"]:\n",
    "                html_content += WikipediaFetcher.format_section_to_html(section)\n",
    "            html_content += \"</ul>\"\n",
    "\n",
    "        html_content += \"\"\"\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "            print(f\"Données enregistrées en HTML dans : {os.path.abspath(path)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def format_section_to_html(section):\n",
    "        \"\"\"\n",
    "        Convertit une section et ses sous-sections en HTML.\n",
    "        \"\"\"\n",
    "        html = f\"<li><h3>{section['titre']}</h3><p>{section['contenu']}</p>\"\n",
    "        if \"sous_sections\" in section and section[\"sous_sections\"]:\n",
    "            html += \"<ul>\"\n",
    "            for subsection in section[\"sous_sections\"]:\n",
    "                html += WikipediaFetcher.format_section_to_html(subsection)\n",
    "            html += \"</ul>\"\n",
    "        html += \"</li>\"\n",
    "        return html\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    fetcher = WikipediaFetcher(language=\"fr\")\n",
    "    data = fetcher.get_page_data(\"Ramen\", summary_only=False, save_path=\"wikipedia_data.html\", save_as_html=True)\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USf85oKLKRxY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Résumé de la page wiki récupéré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OK9JNyxPKfeV",
    "outputId": "80085553-28f0-4376-d706-432112ad103c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résumé :\n",
      "Un rāmen (ラーメン, prononcé en français [ʀamɛn] ou [lamɛn]) est une recette de cuisine japonaise héritée de la cuisine chinoise et adaptée au goût des habitants de l'archipel nippon. Le mot rāmen est emprunté par le japonais de son nom original chinois en mandarin [lāmiàn] (拉麵, 拉面 'nouilles tirées'). Dans sa forme traditionnelle, il s'agit d'une soupe de nouilles, à base de bouillon agrémenté de nombreuses variantes d'ingrédients animaux, végétaux et aromates (poissons, viandes, légumes, algues, œuf, etc.). Importé de Chine à la fin du XIXe siècle, il est à ce jour considéré comme faisant partie des plats emblématiques de la gastronomie japonaise.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Crée une instance de WikipediaFetcher\n",
    "    fetcher = WikipediaFetcher(language=\"fr\")\n",
    "    \n",
    "    # Requête pour obtenir uniquement le résumé\n",
    "    try:\n",
    "        data = fetcher.get_page_data(\n",
    "            query=\"Ramen\",  # Le sujet de la recherche\n",
    "            summary_only=True  # Indique qu'on ne veut que le résumé\n",
    "        )\n",
    "        print(\"Résumé :\")\n",
    "        print(data[\"resume\"])  # Affiche le résumé\n",
    "    except ValueError as e:\n",
    "        print(f\"Erreur : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldvkyf57KsPs"
   },
   "source": [
    "## Partie récupération avec API AniList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-FQWhHjyVeR5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats de recherche : [{'id': 16498, 'titles': {'romaji': 'Shingeki no Kyojin', 'english': 'Attack on Titan', 'native': '進撃の巨人'}}, {'id': 110277, 'titles': {'romaji': 'Shingeki no Kyojin: The Final Season', 'english': 'Attack on Titan Final Season', 'native': '進撃の巨人 The Final Season'}}, {'id': 18397, 'titles': {'romaji': 'Shingeki no Kyojin OVA', 'english': 'Attack on Titan OVA', 'native': '進撃の巨人 OVA'}}, {'id': 20811, 'titles': {'romaji': 'Shingeki no Kyojin Gaiden: Kuinaki Sentaku', 'english': 'Attack on Titan: No Regrets', 'native': '進撃の巨人 外伝 悔いなき選択'}}, {'id': 20958, 'titles': {'romaji': 'Shingeki no Kyojin 2', 'english': 'Attack on Titan Season 2', 'native': '進撃の巨人２'}}]\n",
      "Détails : {'id': 16498, 'title': {'romaji': 'Shingeki no Kyojin', 'english': 'Attack on Titan', 'native': '進撃の巨人'}, 'description': 'Several hundred years ago, humans were nearly exterminated by titans. Titans are typically several stories tall, seem to have no intelligence, devour human beings and, worst of all, seem to do it for the pleasure rather than as a food source. A small percentage of humanity survived by walling themselves in a city protected by extremely high walls, even taller than the biggest of titans.<br><br>\\r\\nFlash forward to the present and the city has not seen a titan in over 100 years. Teenage boy Eren and his foster sister Mikasa witness something horrific as the city walls are destroyed by a colossal titan that appears out of thin air. As the smaller titans flood the city, the two kids watch in horror as their mother is eaten alive. Eren vows that he will murder every single titan and take revenge for all of mankind.<br><br>\\r\\n(Source: MangaHelpers) ', 'genres': ['Action', 'Drama', 'Fantasy', 'Mystery'], 'episodes': 25, 'chapters': None, 'volumes': None, 'coverImage': {'large': 'https://s4.anilist.co/file/anilistcdn/media/anime/cover/medium/bx16498-73IhOXpJZiMF.jpg'}, 'averageScore': 84, 'popularity': 834587}\n",
      "Données enregistrées dans : C:\\Users\\mbiver\\Desktop\\IA\\Projet\\anilist_data.json\n",
      "Données enregistrées en HTML dans : C:\\Users\\mbiver\\Desktop\\IA\\Projet\\anilist_data.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "class AniListFetcher:\n",
    "    \"\"\"\n",
    "    Classe pour interagir avec l'API AniList et récupérer des données sur les animes, mangas, ou personnages.\n",
    "    \"\"\"\n",
    "\n",
    "    API_URL = \"https://graphql.anilist.co\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def search_titles(self, query, media_type=\"ANIME\", max_results=5):\n",
    "        \"\"\"\n",
    "        Recherche des titres sur AniList par type de média (ANIME, MANGA, CHARACTER).\n",
    "        \"\"\"\n",
    "        query_body = {\n",
    "            \"query\": \"\"\"\n",
    "                query ($search: String, $type: MediaType, $page: Int, $perPage: Int) {\n",
    "                    Page(page: $page, perPage: $perPage) {\n",
    "                        media(search: $search, type: $type) {\n",
    "                            id\n",
    "                            title {\n",
    "                                romaji\n",
    "                                english\n",
    "                                native\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            \"\"\",\n",
    "            \"variables\": {\n",
    "                \"search\": query,\n",
    "                \"type\": media_type,\n",
    "                \"page\": 1,\n",
    "                \"perPage\": max_results\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.API_URL, json=query_body)\n",
    "        data = response.json()\n",
    "\n",
    "        if \"errors\" in data:\n",
    "            raise ValueError(f\"Erreur lors de la recherche : {data['errors']}\")\n",
    "\n",
    "        results = [\n",
    "            {\n",
    "                \"id\": media[\"id\"],\n",
    "                \"titles\": media[\"title\"]\n",
    "            }\n",
    "            for media in data[\"data\"][\"Page\"][\"media\"]\n",
    "        ]\n",
    "\n",
    "        if not results:\n",
    "            raise ValueError(\"Aucun résultat trouvé pour cette recherche.\")\n",
    "        return results\n",
    "\n",
    "    def fetch_details(self, media_id, media_type=\"ANIME\"):\n",
    "        \"\"\"\n",
    "        Récupère les détails d'un média (anime ou manga) par son ID.\n",
    "        \"\"\"\n",
    "        query_body = {\n",
    "            \"query\": \"\"\"\n",
    "                query ($id: Int, $type: MediaType) {\n",
    "                    Media(id: $id, type: $type) {\n",
    "                        id\n",
    "                        title {\n",
    "                            romaji\n",
    "                            english\n",
    "                            native\n",
    "                        }\n",
    "                        description\n",
    "                        genres\n",
    "                        episodes\n",
    "                        chapters\n",
    "                        volumes\n",
    "                        coverImage {\n",
    "                            large\n",
    "                        }\n",
    "                        averageScore\n",
    "                        popularity\n",
    "                    }\n",
    "                }\n",
    "            \"\"\",\n",
    "            \"variables\": {\n",
    "                \"id\": media_id,\n",
    "                \"type\": media_type\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.API_URL, json=query_body)\n",
    "        data = response.json()\n",
    "\n",
    "        if \"errors\" in data:\n",
    "            raise ValueError(f\"Erreur lors de la récupération des détails : {data['errors']}\")\n",
    "\n",
    "        return data[\"data\"][\"Media\"]\n",
    "\n",
    "    def save_to_json(self, data, path):\n",
    "        \"\"\"\n",
    "        Enregistre les données dans un fichier JSON.\n",
    "        \"\"\"\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "            print(f\"Données enregistrées dans : {os.path.abspath(path)}\")\n",
    "\n",
    "    def save_to_html(self, data, path):\n",
    "        \"\"\"\n",
    "        Enregistre les données dans un fichier HTML.\n",
    "        \"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>{data['title']['romaji']}</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>{data['title']['romaji']} ({data['title']['english'] or 'N/A'})</h1>\n",
    "            <img src=\"{data['coverImage']['large']}\" alt=\"{data['title']['romaji']}\">\n",
    "            <p><strong>Description :</strong> {data['description']}</p>\n",
    "            <p><strong>Genres :</strong> {', '.join(data['genres'])}</p>\n",
    "            <p><strong>Score moyen :</strong> {data['averageScore']}</p>\n",
    "            <p><strong>Popularité :</strong> {data['popularity']}</p>\n",
    "            <p><strong>Épisodes :</strong> {data.get('episodes', 'N/A')}</p>\n",
    "            <p><strong>Chapitres :</strong> {data.get('chapters', 'N/A')}</p>\n",
    "            <p><strong>Volumes :</strong> {data.get('volumes', 'N/A')}</p>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "            print(f\"Données enregistrées en HTML dans : {os.path.abspath(path)}\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    fetcher = AniListFetcher()\n",
    "\n",
    "    # Recherche de titres\n",
    "    search_results = fetcher.search_titles(\"Attack on Titan\", media_type=\"ANIME\")\n",
    "    print(\"Résultats de recherche :\", search_results)\n",
    "\n",
    "    # Récupération des détails\n",
    "    media_id = search_results[0][\"id\"]\n",
    "    details = fetcher.fetch_details(media_id, media_type=\"ANIME\")\n",
    "    print(\"Détails :\", details)\n",
    "\n",
    "    # Sauvegarde des données\n",
    "    fetcher.save_to_json(details, \"anilist_data.json\")\n",
    "    fetcher.save_to_html(details, \"anilist_data.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USf85oKLKRxY"
   },
   "source": [
    "### Modification pour récupérer les personnages et pas juste les animé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats de recherche : [{'id': 45627, 'name': {'full': 'Levi', 'native': 'リヴァイ'}}, {'id': 52885, 'name': {'full': 'Levi Kazama', 'native': '風間レヴィ'}}, {'id': 216491, 'name': {'full': 'Leviathan', 'native': 'レヴィアタン'}}, {'id': 41838, 'name': {'full': 'Shion Eliphas Levi', 'native': 'シオン エリファス レビィ '}}, {'id': 183509, 'name': {'full': 'Serafall Leviathan', 'native': 'セラフォルー'}}]\n",
      "Détails : {'id': 45627, 'name': {'full': 'Levi', 'native': 'リヴァイ'}, 'image': {'large': 'https://s4.anilist.co/file/anilistcdn/character/large/b45627-CR68RyZmddGG.png'}, 'description': '__Height:__ 160 cm (5\\'3\")\\n\\nSquad Captain of the Special Operations Squad (known as Squad Levi) within the Survey Corps. He is known as humanity\\'s strongest soldier. \\n\\nLevi is abrasive, blunt, and has a strong respect for structure and discipline. He is a clean-freak, demanding spotless cleanliness of himself, his equipment, and his environment. Rumors say he was involved in underground crime in his life before coming a soldier. Despite his emotionless and unfriendly demeanor, Levi cares deeply for his squad and greatly values human life. ', 'media': {'nodes': [{'title': {'romaji': 'Shingeki no Kyojin', 'english': 'Attack on Titan'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin OVA', 'english': 'Attack on Titan OVA'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin Zenpen: Guren no Yumiya', 'english': 'Attack on Titan Part I: Crimson Bow and Arrow'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin Kouhen: Jiyuu no Tsubasa', 'english': 'Attack on Titan Part II: Wings of Freedom'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin Gaiden: Kuinaki Sentaku', 'english': 'Attack on Titan: No Regrets'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki! Kyojin Chuugakkou', 'english': 'Attack on Titan: Junior High'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin', 'english': 'Attack on Titan'}, 'type': 'MANGA'}, {'title': {'romaji': 'Shingeki no Kyojin Gaiden: Kuinaki Sentaku', 'english': 'Attack on Titan: No Regrets'}, 'type': 'MANGA'}, {'title': {'romaji': 'Shingeki no Kyojin Gaiden: Kuinaki Sentaku - Prologue', 'english': 'Attack on Titan: No Regrets - Prologue'}, 'type': 'MANGA'}, {'title': {'romaji': 'Shingeki no Kyojin 2', 'english': 'Attack on Titan Season 2'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin 3', 'english': 'Attack on Titan Season 3'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin Season 2: Kakusei no Houkou', 'english': 'Attack on Titan: The Roar of Awakening'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin 3 Part 2', 'english': 'Attack on Titan Season 3 Part 2'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin: Chimi Kyara Gekijou - Tondeke! Kunren Heidan', 'english': 'Attack on Titan Picture Drama'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki! Kyojin Chuugakkou', 'english': 'Attack on Titan: Junior High'}, 'type': 'MANGA'}, {'title': {'romaji': 'Shingeki no Kyojin: Chimi Kyara Gekijou - Rivai-han', 'english': None}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin: The Final Season', 'english': 'Attack on Titan Final Season'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyotou', 'english': 'Attack on Skytree'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin: Chronicle', 'english': 'Attack on Titan ~Chronicle~'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin: Chimi Kyara Gekijou - Rivai-han Part 2', 'english': None}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin: The Final Season Part 2', 'english': 'Attack on Titan Final Season Part 2'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin: Chimi Kyara Gekijou - Final', 'english': 'Attack On Titan: The Final Season Specials'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki! Kyojin Chuugakkou: Seishun! Tonari no Marley Gakuen', 'english': None}, 'type': 'MANGA'}, {'title': {'romaji': 'Shingeki no Kyojin: The Final Season - Kanketsu-hen Zenpen', 'english': 'Attack on Titan Final Season THE FINAL CHAPTERS Special 1'}, 'type': 'ANIME'}, {'title': {'romaji': 'Shingeki no Kyojin: The Final Season - Kanketsu-hen Kouhen', 'english': 'Attack on Titan Final Season THE FINAL CHAPTERS Special 2'}, 'type': 'ANIME'}]}}\n",
      "Données enregistrées dans : C:\\Users\\mbiver\\Desktop\\IA\\Projet\\character_data.json\n",
      "Données enregistrées en HTML dans : C:\\Users\\mbiver\\Desktop\\IA\\Projet\\character_data.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "class AniListFetcher:\n",
    "    \"\"\"\n",
    "    Classe pour interagir avec l'API AniList et récupérer des données sur les animes, mangas ou personnages.\n",
    "    \"\"\"\n",
    "\n",
    "    API_URL = \"https://graphql.anilist.co\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def search_titles(self, query, media_type=\"ANIME\", max_results=5):\n",
    "        \"\"\"\n",
    "        Recherche des titres sur AniList par type de média (ANIME, MANGA, CHARACTER).\n",
    "        \"\"\"\n",
    "        if media_type == \"CHARACTER\":\n",
    "            query_body = {\n",
    "                \"query\": \"\"\"\n",
    "                    query ($search: String, $page: Int, $perPage: Int) {\n",
    "                        Page(page: $page, perPage: $perPage) {\n",
    "                            characters(search: $search) {\n",
    "                                id\n",
    "                                name {\n",
    "                                    full\n",
    "                                    native\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                \"\"\",\n",
    "                \"variables\": {\n",
    "                    \"search\": query,\n",
    "                    \"page\": 1,\n",
    "                    \"perPage\": max_results\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            query_body = {\n",
    "                \"query\": \"\"\"\n",
    "                    query ($search: String, $type: MediaType, $page: Int, $perPage: Int) {\n",
    "                        Page(page: $page, perPage: $perPage) {\n",
    "                            media(search: $search, type: $type) {\n",
    "                                id\n",
    "                                title {\n",
    "                                    romaji\n",
    "                                    english\n",
    "                                    native\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                \"\"\",\n",
    "                \"variables\": {\n",
    "                    \"search\": query,\n",
    "                    \"type\": media_type,\n",
    "                    \"page\": 1,\n",
    "                    \"perPage\": max_results\n",
    "                }\n",
    "            }\n",
    "\n",
    "        response = requests.post(self.API_URL, json=query_body)\n",
    "        data = response.json()\n",
    "\n",
    "        if \"errors\" in data:\n",
    "            raise ValueError(f\"Erreur lors de la recherche : {data['errors']}\")\n",
    "\n",
    "        if media_type == \"CHARACTER\":\n",
    "            results = [\n",
    "                {\n",
    "                    \"id\": character[\"id\"],\n",
    "                    \"name\": character[\"name\"]\n",
    "                }\n",
    "                for character in data[\"data\"][\"Page\"][\"characters\"]\n",
    "            ]\n",
    "        else:\n",
    "            results = [\n",
    "                {\n",
    "                    \"id\": media[\"id\"],\n",
    "                    \"titles\": media[\"title\"]\n",
    "                }\n",
    "                for media in data[\"data\"][\"Page\"][\"media\"]\n",
    "            ]\n",
    "\n",
    "        if not results:\n",
    "            raise ValueError(\"Aucun résultat trouvé pour cette recherche.\")\n",
    "        return results\n",
    "\n",
    "    def fetch_details(self, media_id, media_type=\"ANIME\"):\n",
    "        \"\"\"\n",
    "        Récupère les détails d'un média (anime ou manga) par son ID.\n",
    "        \"\"\"\n",
    "        query_body = {\n",
    "            \"query\": \"\"\"\n",
    "                query ($id: Int, $type: MediaType) {\n",
    "                    Media(id: $id, type: $type) {\n",
    "                        id\n",
    "                        title {\n",
    "                            romaji\n",
    "                            english\n",
    "                            native\n",
    "                        }\n",
    "                        description\n",
    "                        genres\n",
    "                        episodes\n",
    "                        chapters\n",
    "                        volumes\n",
    "                        coverImage {\n",
    "                            large\n",
    "                        }\n",
    "                        averageScore\n",
    "                        popularity\n",
    "                    }\n",
    "                }\n",
    "            \"\"\",\n",
    "            \"variables\": {\n",
    "                \"id\": media_id,\n",
    "                \"type\": media_type\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.API_URL, json=query_body)\n",
    "        data = response.json()\n",
    "\n",
    "        if \"errors\" in data:\n",
    "            raise ValueError(f\"Erreur lors de la récupération des détails : {data['errors']}\")\n",
    "\n",
    "        return data[\"data\"][\"Media\"]\n",
    "\n",
    "    def fetch_character_details(self, character_id):\n",
    "        \"\"\"\n",
    "        Récupère les détails d'un personnage par son ID.\n",
    "        \"\"\"\n",
    "        query_body = {\n",
    "            \"query\": \"\"\"\n",
    "                query ($id: Int) {\n",
    "                    Character(id: $id) {\n",
    "                        id\n",
    "                        name {\n",
    "                            full\n",
    "                            native\n",
    "                        }\n",
    "                        image {\n",
    "                            large\n",
    "                        }\n",
    "                        description\n",
    "                        media {\n",
    "                            nodes {\n",
    "                                title {\n",
    "                                    romaji\n",
    "                                    english\n",
    "                                }\n",
    "                                type\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            \"\"\",\n",
    "            \"variables\": {\n",
    "                \"id\": character_id\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.API_URL, json=query_body)\n",
    "        data = response.json()\n",
    "\n",
    "        if \"errors\" in data:\n",
    "            raise ValueError(f\"Erreur lors de la récupération des détails : {data['errors']}\")\n",
    "\n",
    "        return data[\"data\"][\"Character\"]\n",
    "\n",
    "    def save_to_json(self, data, path):\n",
    "        \"\"\"\n",
    "        Enregistre les données dans un fichier JSON.\n",
    "        \"\"\"\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "            print(f\"Données enregistrées dans : {os.path.abspath(path)}\")\n",
    "\n",
    "    def save_to_html(self, data, path):\n",
    "        \"\"\"\n",
    "        Enregistre les données dans un fichier HTML.\n",
    "        \"\"\"\n",
    "        if \"name\" in data:  # Gestion des personnages\n",
    "            html_content = f\"\"\"\n",
    "            <!DOCTYPE html>\n",
    "            <html lang=\"en\">\n",
    "            <head>\n",
    "                <meta charset=\"UTF-8\">\n",
    "                <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "                <title>{data['name']['full']}</title>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1>{data['name']['full']} ({data['name']['native'] or 'N/A'})</h1>\n",
    "                <img src=\"{data['image']['large']}\" alt=\"{data['name']['full']}\">\n",
    "                <p><strong>Description :</strong> {data['description']}</p>\n",
    "                <h2>Médias associés</h2>\n",
    "                <ul>\n",
    "            \"\"\"\n",
    "            for media in data[\"media\"][\"nodes\"]:\n",
    "                html_content += f\"<li>{media['title']['romaji']} ({media['type']})</li>\"\n",
    "\n",
    "            html_content += \"\"\"\n",
    "                </ul>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\"\n",
    "        else:  # Gestion des animes/mangas\n",
    "            html_content = f\"\"\"\n",
    "            <!DOCTYPE html>\n",
    "            <html lang=\"en\">\n",
    "            <head>\n",
    "                <meta charset=\"UTF-8\">\n",
    "                <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "                <title>{data['title']['romaji']}</title>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1>{data['title']['romaji']} ({data['title']['english'] or 'N/A'})</h1>\n",
    "                <img src=\"{data['coverImage']['large']}\" alt=\"{data['title']['romaji']}\">\n",
    "                <p><strong>Description :</strong> {data['description']}</p>\n",
    "                <p><strong>Genres :</strong> {', '.join(data['genres'])}</p>\n",
    "                <p><strong>Score moyen :</strong> {data['averageScore']}</p>\n",
    "                <p><strong>Popularité :</strong> {data['popularity']}</p>\n",
    "                <p><strong>Épisodes :</strong> {data.get('episodes', 'N/A')}</p>\n",
    "                <p><strong>Chapitres :</strong> {data.get('chapters', 'N/A')}</p>\n",
    "                <p><strong>Volumes :</strong> {data.get('volumes', 'N/A')}</p>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\"\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "            print(f\"Données enregistrées en HTML dans : {os.path.abspath(path)}\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    fetcher = AniListFetcher()\n",
    "\n",
    "    # Recherche d'un personnage\n",
    "    search_results = fetcher.search_titles(\"Levi\", media_type=\"CHARACTER\")\n",
    "    print(\"Résultats de recherche :\", search_results)\n",
    "\n",
    "    # Récupération des détails du personnage\n",
    "    character_id = search_results[0][\"id\"]\n",
    "    details = fetcher.fetch_character_details(character_id)\n",
    "    print(\"Détails :\", details)\n",
    "\n",
    "    # Sauvegarde des données\n",
    "    fetcher.save_to_json(details, \"character_data.json\")\n",
    "    fetcher.save_to_html(details, \"character_data.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
