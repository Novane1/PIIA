{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwaJ0eGHCkw"
      },
      "source": [
        "# LoRA Easy Training Colab\n",
        "[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/jelosus1)\n",
        "\n",
        "### Colab powered by [Lora_Easy_Training_Scripts_Backend](https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Learn how to use the colab [here](https://civitai.com/articles/4409).\n",
        "\n",
        "If you feel something is missing, want something to be added or simply found a bug, open an [issue](https://github.com/Jelosus2/Lora_Easy_Training_Colab/issues).\n",
        "\n",
        "---\n",
        "\n",
        "Last Update: February 3, 2025. Check the [full changelog](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#changelog)\n",
        "\n",
        "Changes:\n",
        "- Fixed slow tagging.\n",
        "- Fixed path issues when setting up directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CSz_rmldHZvh",
        "cellView": "form",
        "outputId": "9a6716be-4a6e-4f1e-f2fd-6fe0af3c5c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing trainer...\n",
            "22 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 python3-pip-whl python3-setuptools-whl\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 python3-pip-whl python3-setuptools-whl python3.10-venv\n",
            "0 upgraded, 6 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 3,987 kB of archives.\n",
            "After this operation, 8,325 kB of additional disk space will be used.\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../1-libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../2-aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Selecting previously unselected package python3-pip-whl.\n",
            "Preparing to unpack .../3-python3-pip-whl_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Selecting previously unselected package python3-setuptools-whl.\n",
            "Preparing to unpack .../4-python3-setuptools-whl_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
            "Unpacking python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3.10-venv.\n",
            "Preparing to unpack .../5-python3.10-venv_3.10.12-1~22.04.8_amd64.deb ...\n",
            "Unpacking python3.10-venv (3.10.12-1~22.04.8) ...\n",
            "Setting up python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Setting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up python3.10-venv (3.10.12-1~22.04.8) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Cloning into '/content/trainer'...\n",
            "remote: Enumerating objects: 338, done.\u001b[K\n",
            "remote: Counting objects: 100% (181/181), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 338 (delta 158), reused 132 (delta 132), pack-reused 157 (from 2)\u001b[K\n",
            "Receiving objects: 100% (338/338), 7.79 MiB | 11.99 MiB/s, done.\n",
            "Resolving deltas: 100% (195/195), done.\n",
            "Submodule 'sd_scripts' (https://github.com/kohya-ss/sd-scripts) registered for path 'sd_scripts'\n",
            "Cloning into '/content/trainer/sd_scripts'...\n",
            "Submodule path 'sd_scripts': checked out 'e89653975ddf429cdf0c0fd268da0a5a3e8dba1f'\n",
            "creating venv and installing requirements\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.5.1\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp310-cp310-linux_x86_64.whl (908.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m735.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.20.1\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.1%2Bcu124-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy==1.13.1\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0\n",
            "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
            "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
            "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.0.0 sympy-1.13.1 torch-2.5.1+cu124 torchvision-0.20.1+cu124 triton-3.1.0 typing-extensions-4.12.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting xformers==0.0.29.post1\n",
            "  Downloading https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl (44.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from xformers==0.0.29.post1) (2.1.2)\n",
            "Requirement already satisfied: torch==2.5.1 in ./venv/lib/python3.10/site-packages (from xformers==0.0.29.post1) (2.5.1+cu124)\n",
            "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (2024.6.1)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (11.6.1.9)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (10.3.5.147)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (11.2.1.3)\n",
            "Requirement already satisfied: triton==3.1.0 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (3.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (12.4.127)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (12.3.1.170)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1->xformers==0.0.29.post1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch==2.5.1->xformers==0.0.29.post1) (2.1.5)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.29.post1\n",
            "Obtaining file:///content/trainer/sd_scripts (from -r requirements.txt (line 45))\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate==0.33.0\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.44.0\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers[torch]==0.25.0\n",
            "  Downloading diffusers-0.25.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.1.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.8.1.78\n",
            "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.7.0\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.9.0\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.44.0\n",
            "  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prodigyopt==1.0\n",
            "  Downloading prodigyopt-1.0-py3-none-any.whl (5.5 kB)\n",
            "Collecting lion-pytorch==0.0.6\n",
            "  Downloading lion_pytorch-0.0.6-py3-none-any.whl (4.2 kB)\n",
            "Collecting schedulefree==1.4\n",
            "  Downloading schedulefree-1.4.tar.gz (22 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors==0.4.4\n",
            "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 KB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting altair==4.2.2\n",
            "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 KB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting easygui==0.98.3\n",
            "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting toml==0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting voluptuous==0.13.1\n",
            "  Downloading voluptuous-0.13.1-py3-none-any.whl (29 kB)\n",
            "Collecting huggingface-hub==0.24.5\n",
            "  Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 KB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imagesize==1.4.1\n",
            "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Collecting rich==13.7.0\n",
            "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 KB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.2.0\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in ./venv/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (2.5.1+cu124)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 KB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0.0,>=1.17\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (3.13.1)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (from diffusers[torch]==0.25.0->-r requirements.txt (line 3)) (11.0.0)\n",
            "Collecting wcwidth>=0.2.5\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 KB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in ./venv/lib/python3.10/site-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (4.12.2)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in ./venv/lib/python3.10/site-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (2024.6.1)\n",
            "Collecting jsonschema>=3.0\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting entrypoints\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from altair==4.2.2->-r requirements.txt (line 16)) (3.1.4)\n",
            "Collecting pandas>=0.18\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting toolz\n",
            "  Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.48.2\n",
            "  Downloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six>1.9\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 13)) (59.6.0)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=22.2.0\n",
            "  Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonschema-specifications>=2023.03.6\n",
            "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
            "Collecting rpds-py>=0.7.1\n",
            "  Downloading rpds_py-0.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.0/382.0 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting referencing>=0.28.4\n",
            "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting python-dateutil>=2.8.2\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzdata>=2022.7\n",
            "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.1.0 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.33.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 13)) (2.1.5)\n",
            "Collecting zipp>=3.20\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting propcache>=0.2.0\n",
            "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0\n",
            "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.17.0\n",
            "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: schedulefree\n",
            "  Building wheel for schedulefree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for schedulefree: filename=schedulefree-1.4-py3-none-any.whl size=39303 sha256=44363f1bbae34952e3b3e72e2ec61d2b05df815418e9b7b13c142e8530dbd96b\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/4f/8e/15e0ac6c13041e94078d9e4f88d14c2cd00ec198bc0fc78e91\n",
            "Successfully built schedulefree\n",
            "Installing collected packages: wcwidth, voluptuous, sentencepiece, pytz, library, easygui, zipp, werkzeug, urllib3, tzdata, tqdm, toolz, toml, tensorboard-data-server, six, schedulefree, safetensors, rpds-py, regex, pyyaml, pygments, psutil, protobuf, propcache, prodigyopt, packaging, numpy, multidict, mdurl, markdown, imagesize, idna, grpcio, ftfy, frozenlist, entrypoints, einops, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, absl-py, yarl, tensorboard, requests, referencing, python-dateutil, opencv-python, markdown-it-py, lightning-utilities, importlib-metadata, aiosignal, rich, pandas, jsonschema-specifications, huggingface-hub, aiohttp, torchmetrics, tokenizers, lion-pytorch, jsonschema, diffusers, bitsandbytes, accelerate, transformers, pytorch-lightning, altair\n",
            "  Running setup.py develop for library\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.2\n",
            "    Uninstalling numpy-2.1.2:\n",
            "      Successfully uninstalled numpy-2.1.2\n",
            "Successfully installed absl-py-2.1.0 accelerate-0.33.0 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 altair-4.2.2 async-timeout-5.0.1 attrs-25.1.0 bitsandbytes-0.44.0 certifi-2025.1.31 charset-normalizer-3.4.1 diffusers-0.25.0 easygui-0.98.3 einops-0.7.0 entrypoints-0.4 frozenlist-1.5.0 ftfy-6.1.1 grpcio-1.70.0 huggingface-hub-0.24.5 idna-3.10 imagesize-1.4.1 importlib-metadata-8.6.1 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 library-0.0.0 lightning-utilities-0.12.0 lion-pytorch-0.0.6 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 numpy-1.26.4 opencv-python-4.8.1.78 packaging-24.2 pandas-2.2.3 prodigyopt-1.0 propcache-0.2.1 protobuf-5.29.3 psutil-7.0.0 pygments-2.19.1 python-dateutil-2.9.0.post0 pytorch-lightning-1.9.0 pytz-2025.1 pyyaml-6.0.2 referencing-0.36.2 regex-2024.11.6 requests-2.32.3 rich-13.7.0 rpds-py-0.22.3 safetensors-0.4.4 schedulefree-1.4 sentencepiece-0.2.0 six-1.17.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tokenizers-0.19.1 toml-0.10.2 toolz-1.0.0 torchmetrics-1.6.1 tqdm-4.67.1 transformers-4.44.0 tzdata-2025.1 urllib3-2.3.0 voluptuous-0.13.1 wcwidth-0.2.13 werkzeug-3.1.3 yarl-1.18.3 zipp-3.21.0\n",
            "Processing /content/trainer/custom_scheduler\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Using legacy 'setup.py install' for LoraEasyCustomOptimizer, since package 'wheel' is not installed.\n",
            "Installing collected packages: LoraEasyCustomOptimizer\n",
            "  Running setup.py install for LoraEasyCustomOptimizer ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed LoraEasyCustomOptimizer-1.0.0\n",
            "Collecting starlette\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (2.32.3)\n",
            "Collecting dadaptation\n",
            "  Downloading dadaptation-3.2.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.19.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Collecting pycloudflared\n",
            "  Downloading pycloudflared-0.2.0-py3-none-any.whl (7.3 kB)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting came-pytorch\n",
            "  Downloading came_pytorch-0.1.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting lycoris-lora==3.0.1.dev10\n",
            "  Downloading lycoris_lora-3.0.1.dev10.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch_optimizer==3.1.2\n",
            "  Downloading pytorch_optimizer-3.1.2-py3-none-any.whl (187 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Requirement already satisfied: einops in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: toml in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (0.10.2)\n",
            "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from pytorch_optimizer==3.1.2->-r ../requirements.txt (line 11)) (1.26.4)\n",
            "Collecting anyio<5,>=3.6.2\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=7.0\n",
            "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]->-r ../requirements.txt (line 2)) (4.12.2)\n",
            "Collecting watchfiles>=0.13\n",
            "  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.6.3\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 KB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]->-r ../requirements.txt (line 2)) (6.0.2)\n",
            "Collecting websockets>=10.4\n",
            "  Downloading websockets-14.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.3/169.3 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->-r ../requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: psutil>=5.0.0 in ./venv/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 5)) (7.0.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./venv/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 5)) (5.29.3)\n",
            "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 5)) (59.6.0)\n",
            "Collecting pydantic<3,>=2.6\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 KB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 KB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting platformdirs\n",
            "  Downloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=2.0.0\n",
            "  Downloading sentry_sdk-2.21.0-py2.py3-none-any.whl (324 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 KB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tomli\n",
            "  Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
            "Collecting exceptiongroup>=1.0.2\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in ./venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r ../requirements.txt (line 5)) (1.17.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.6.0\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting pydantic-core==2.27.2\n",
            "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (3.1.0)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (10.3.5.147)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (3.3)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (12.4.127)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch->lycoris-lora==3.0.1.dev10->-r ../requirements.txt (line 10)) (2.1.5)\n",
            "Using legacy 'setup.py install' for lycoris-lora, since package 'wheel' is not installed.\n",
            "Building wheels for collected packages: dadaptation\n",
            "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dadaptation: filename=dadaptation-3.2-py3-none-any.whl size=23255 sha256=cb6b26abb5ac900090b532fb29bda8eb8bd982b1c91a9150deeaa221b3f68e90\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/03/6d/feba04df15ef39d9ac4e3504058ac2a88fb2ef9183ba92b111\n",
            "Successfully built dadaptation\n",
            "Installing collected packages: wheel, websockets, uvloop, tomli, sniffio, smmap, setproctitle, sentry-sdk, scipy, python-dotenv, pyngrok, pydantic-core, platformdirs, httptools, h11, exceptiongroup, docker-pycreds, dadaptation, click, annotated-types, uvicorn, pydantic, pycloudflared, gitdb, anyio, watchfiles, starlette, gitpython, wandb, pytorch_optimizer, lycoris-lora, came-pytorch\n",
            "  Running setup.py install for lycoris-lora ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.8.0 came-pytorch-0.1.3 click-8.1.8 dadaptation-3.2 docker-pycreds-0.4.0 exceptiongroup-1.2.2 gitdb-4.0.12 gitpython-3.1.44 h11-0.14.0 httptools-0.6.4 lycoris-lora-3.0.1.dev10 platformdirs-4.3.6 pycloudflared-0.2.0 pydantic-2.10.6 pydantic-core-2.27.2 pyngrok-7.2.3 python-dotenv-1.0.1 pytorch_optimizer-3.1.2 scipy-1.15.1 sentry-sdk-2.21.0 setproctitle-1.3.4 smmap-5.0.2 sniffio-1.3.1 starlette-0.45.3 tomli-2.2.1 uvicorn-0.34.0 uvloop-0.21.0 wandb-0.19.6 watchfiles-1.0.4 websockets-14.2 wheel-0.45.1\n",
            "completed installing\n",
            "Installation complete!\n",
            "Downloading tagger script that allows v3 taggers...\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "9448ef|\u001b[1;32mOK\u001b[0m  |   1.5MiB/s|//content/trainer/sd_scripts/finetune/tag_images_by_wd14_tagger.py\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Fixing sd_scripts logging issue on colab...\n",
            "Found existing installation: rich 13.7.0\n",
            "Uninstalling rich-13.7.0:\n",
            "  Would remove:\n",
            "    /content/trainer/sd_scripts/venv/lib/python3.10/site-packages/rich-13.7.0.dist-info/*\n",
            "    /content/trainer/sd_scripts/venv/lib/python3.10/site-packages/rich/*\n",
            "Proceed (Y/n)?   Successfully uninstalled rich-13.7.0\n",
            "Finished installation!\n"
          ]
        }
      ],
      "source": [
        "# @title ## 1. Install the trainer ![doro](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "root_path = Path(\"/content\")\n",
        "trainer_dir = root_path.joinpath(\"trainer\")\n",
        "\n",
        "venv_pip = trainer_dir.joinpath(\"sd_scripts/venv/bin/pip\")\n",
        "venv_python = trainer_dir.joinpath(\"sd_scripts/venv/bin/python\")\n",
        "\n",
        "# @markdown Execute the cell to install the trainer\n",
        "\n",
        "installed_dependencies = False\n",
        "first_step_done = False\n",
        "\n",
        "def install_trainer():\n",
        "  global installed_dependencies, first_step_done\n",
        "\n",
        "  print(\"Installing trainer...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y python3.10-venv aria2 -qq\n",
        "\n",
        "  installed_dependencies = True\n",
        "\n",
        "  !git clone https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n",
        "\n",
        "  !chmod 755 /content/trainer/colab_install.sh\n",
        "  os.chdir(trainer_dir)\n",
        "  !./colab_install.sh\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "  first_step_done = True\n",
        "  print(\"Installation complete!\")\n",
        "\n",
        "def download_custom_wd_tagger():\n",
        "  global wd_path\n",
        "\n",
        "  wd_path = trainer_dir.joinpath(\"sd_scripts/finetune/tag_images_by_wd14_tagger.py\")\n",
        "\n",
        "  print(\"Downloading tagger script that allows v3 taggers...\")\n",
        "  !rm \"{wd_path}\"\n",
        "  !aria2c \"https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/main/custom/tag_images_by_wd14_tagger.py\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{wd_path}\"\n",
        "\n",
        "def fix_scripts_logging():\n",
        "  print(\"Fixing sd_scripts logging issue on colab...\")\n",
        "  !yes | {venv_pip} uninstall rich\n",
        "\n",
        "def main():\n",
        "  install_trainer()\n",
        "  download_custom_wd_tagger()\n",
        "  fix_scripts_logging()\n",
        "  print(\"Finished installation!\")\n",
        "\n",
        "try:\n",
        "  main()\n",
        "except Exception as e:\n",
        "  print(f\"Error intalling the trainer!\\n{e}\")\n",
        "  first_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "oS4dJqXoiyC5",
        "outputId": "6c9f8a71-c99b-497d-9736-a29b8423deb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up directories...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# @title ## 2. Setup the directories ![doro diamond](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_diamond.png)\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "if not globals().get(\"first_step_done\"):\n",
        "  root_path = Path(\"/content\")\n",
        "  trainer_dir = root_path.joinpath(\"trainer\")\n",
        "\n",
        "drive_dir = root_path.joinpath(\"drive/MyDrive\")\n",
        "pretrained_model_dir = root_path.joinpath(\"pretrained_model\")\n",
        "vae_dir = root_path.joinpath(\"vae\")\n",
        "tagger_models_dir = root_path.joinpath(\"tagger_models\")\n",
        "\n",
        "# @markdown The base path for your project. Make sure it can be used as a folder name\n",
        "project_path = \"Leonard/LoRa\" # @param {type: \"string\"}\n",
        "# @markdown Specify the name for the directories. If you have multiple datasets, separate each with a comma `(,)` like this: **dataset1, dataset2, ...**\n",
        "\n",
        "# @markdown The directory where the results of the training will be stored.\n",
        "output_dir_name = \"output\" # @param {type: \"string\"}\n",
        "# @markdown The directory where your dataset(s) will be located.\n",
        "dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown Use Drive to store all the files and directories\n",
        "use_drive = True # @param {type: \"boolean\"}\n",
        "\n",
        "project_path = project_path.replace(\" \", \"_\")\n",
        "output_dir_name = output_dir_name.replace(\" \", \"_\")\n",
        "\n",
        "second_step_done = False\n",
        "\n",
        "def is_valid_folder_name(folder_name: str) -> bool:\n",
        "  invalid_characters = '<>:\"/\\|?*'\n",
        "\n",
        "  if any(char in invalid_characters for char in folder_name):\n",
        "    return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def mount_drive_dir() -> Path:\n",
        "  base_dir = root_path.joinpath(project_path)\n",
        "\n",
        "  if use_drive:\n",
        "    if not Path(drive_dir).exists():\n",
        "      drive.mount(Path(drive_dir).parent.as_posix())\n",
        "    base_dir = drive_dir.joinpath(project_path)\n",
        "\n",
        "  return base_dir\n",
        "\n",
        "def make_directories():\n",
        "  mount_drive = mount_drive_dir()\n",
        "  output_dir = mount_drive.joinpath(output_dir_name)\n",
        "\n",
        "  if not Path(mount_drive).exists():\n",
        "    Path(mount_drive).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  for dir in [pretrained_model_dir, vae_dir, output_dir, tagger_models_dir]:\n",
        "    Path(dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  for dataset_m_dir in dataset_dir_name.replace(\" \", \"\").split(','):\n",
        "    if is_valid_folder_name(dataset_m_dir):\n",
        "      Path(mount_drive.joinpath(dataset_m_dir)).mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "      print(f\"{dataset_m_dir} is not a valid name for a folder\")\n",
        "      return\n",
        "\n",
        "def main():\n",
        "  for name in [project_path, output_dir_name]:\n",
        "      if not is_valid_folder_name(name.replace(\"/\", \"\") if project_path == name else name):\n",
        "        print(f\"{name} is not a valid name for a folder\")\n",
        "        return\n",
        "\n",
        "  print(\"Setting up directories...\")\n",
        "  make_directories()\n",
        "  print(\"Done!\")\n",
        "\n",
        "try:\n",
        "  main()\n",
        "  second_step_done = True\n",
        "except Exception as e:\n",
        "  print(f\"Error setting up the directories!\\n{e}\")\n",
        "  second_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b0_HNDa7Zdei",
        "cellView": "form",
        "outputId": "834d30f8-f96e-44fe-8d96-d0d60e79b748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model from https://huggingface.co/AstraliteHeart/pony-diffusion-v6/resolve/main/v6.safetensors...\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "852c75|\u001b[1;32mOK\u001b[0m  |   191MiB/s|//content/pretrained_model/v6.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Downloading vae from https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors...\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "559ffe|\u001b[1;32mOK\u001b[0m  |   236MiB/s|//content/vae/sdxl_vae.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# @title ## 3. Download the base model and/or VAE used for training ![doro fubuki](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_fubuki.png)\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "model_url = \"\"\n",
        "vae_url = \"\"\n",
        "\n",
        "# @markdown Default models are provided here for training. If you want to use another one, introduce the URL in the input below. The link must be pointing to either Civitai or Hugging Face and have the correct format. You can check how to get the correct link [here](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae).\n",
        "training_model = \"(XL) PonyDiffusion v6\" # @param [\"(XL) PonyDiffusion v6\", \"(XL) NoobAI Epsilon v1.0\", \"(XL) Illustrious v0.1\", \"(XL) Animagine 3.1\", \"(XL) SDXL 1.0\", \"(1.5) anime-full-final-pruned (Most used on Anime LoRAs)\", \"(1.5) AnyLora\", \"(1.5) SD 1.5\"]\n",
        "custom_training_model = \"\" # @param {type: \"string\"}\n",
        "# @markdown The name you want to give to the downloaded model file, if not specified default ones will be used.\n",
        "model_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown VAE used for training. It's not needed for 1.5 nor XL, but it's recommended to use the SDXL base VAE for XL training. If you want to use a custom one, introduce the URL in the input below.\n",
        "vae = \"SDXL VAE\" # @param [\"SDXL VAE\", \"None\"]\n",
        "custom_vae = \"\" # @param {type: \"string\"}\n",
        "# @markdown The name you want to give to the downloaded VAE file, if not specified default ones will be used.\n",
        "vae_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown Introduce your [Civitai API Token](https://civitai.com/user/account) or [HuggingFace Access Token](https://huggingface.co/settings/tokens) if the authentication fails while downloading the model and/or VAE.\n",
        "api_token = \"\" # @param {type: \"string\"}\n",
        "# @markdown You can optionally download the model and/or VAE on your drive so you don't need to download them again in the next session. You only would need to specify their path on the UI for the next time you want to use them.\n",
        "download_in_drive = False # @param {type: \"boolean\"}\n",
        "\n",
        "thrid_step_done = False\n",
        "\n",
        "if custom_training_model:\n",
        "  model_url = custom_training_model\n",
        "elif \"Pony\" in training_model:\n",
        "  model_url = \"https://huggingface.co/AstraliteHeart/pony-diffusion-v6/resolve/main/v6.safetensors\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/resolve/main/animagine-xl-3.1.safetensors\"\n",
        "elif \"SDXL\" in training_model:\n",
        "  model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
        "elif \"anime\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "elif \"Any\" in training_model:\n",
        "  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.safetensors\"\n",
        "elif \"SD 1.5\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "elif \"Illustrious\" in training_model:\n",
        "  model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n",
        "elif \"NoobAI\" in training_model:\n",
        "  model_url = \"https://huggingface.co/Laxhar/noobai-XL-1.0/resolve/main/NoobAI-XL-v1.0.safetensors\"\n",
        "\n",
        "if custom_vae:\n",
        "  vae_url = custom_vae\n",
        "elif \"SDXL\" in vae:\n",
        "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
        "\n",
        "model_file = \"\"\n",
        "vae_file = \"\"\n",
        "\n",
        "header = \"\"\n",
        "\n",
        "if not \"installed_dependencies\" in globals():\n",
        "  print(\"Installing missing dependency...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y aria2 -qq\n",
        "  globals().setdefault(\"installed_dependencies\", True)\n",
        "\n",
        "def download_model():\n",
        "  global model_file, model_url, pretrained_model_dir\n",
        "\n",
        "  if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url):\n",
        "    model_url = model_url.replace(\"blob\", \"resolve\")\n",
        "  elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", model_url):\n",
        "    if m := re.search(r\"modelVersionId=(\\d+)\", model_url):\n",
        "      model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "  elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", model_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", model_url):\n",
        "    print(\"Invalid model download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n",
        "    return\n",
        "\n",
        "  if \"civitai.com\" in model_url and api_token and not \"hf\" in api_token:\n",
        "    model_url = f\"{model_url}&token={api_token}\" if \"?\" in model_url else f\"{model_url}?token={api_token}\"\n",
        "  elif \"huggingface.co\" in model_url and api_token:\n",
        "    header = f\"Authorization: Bearer {api_token}\"\n",
        "\n",
        "  stripped_model_url = model_url.strip()\n",
        "\n",
        "  if download_in_drive:\n",
        "    pretrained_model_dir = Path(drive_dir).joinpath(\"Downloaded_models\")\n",
        "\n",
        "    if not Path(pretrained_model_dir).exists():\n",
        "      Path(pretrained_model_dir).mkdir(exist_ok=True)\n",
        "\n",
        "  if model_name:\n",
        "    validated_name = model_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n",
        "\n",
        "    if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n",
        "      model_file = pretrained_model_dir.joinpath(f\"{validated_name}.safetensors\")\n",
        "    else:\n",
        "      model_file = pretrained_model_dir.joinpath(validated_name)\n",
        "  elif stripped_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = pretrained_model_dir.joinpath(stripped_model_url[stripped_model_url.rfind('/'):].replace(\"/\", \"\"))\n",
        "  else:\n",
        "    model_file = pretrained_model_dir.joinpath(\"downloaded_model.safetensors\")\n",
        "    if Path(model_file).exists() and not download_in_drive:\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  print(f\"Downloading model from {model_url}...\")\n",
        "  !aria2c \"{model_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "def download_vae():\n",
        "  global vae_file, vae_url, vae_dir\n",
        "\n",
        "  if not vae == \"None\":\n",
        "    if re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url):\n",
        "      vae_url = vae_url.replace(\"blob\", \"resolve\")\n",
        "    elif re.search(r\"https:\\/\\/civitai\\.com\\/models\\/\\d+\", vae_url):\n",
        "      if m := re.search(r\"modelVersionId=(\\d+)\", vae_url):\n",
        "        vae_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "    elif not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\", vae_url) and not re.search(r\"https:\\/\\/civitai\\.com\\/api\\/download\\/models\\/(\\d+)\", vae_url):\n",
        "      print(\"Invalid VAE download URL!\\nCheck how to get the correct link in https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-get-the-link-for-custom-modelvae\")\n",
        "      return\n",
        "\n",
        "    if \"civitai.com\" in vae_url and api_token and not \"hf\" in api_token:\n",
        "      vae_url = f\"{vae_url}&token={api_token}\" if \"?\" in vae_url else f\"{vae_url}?token={api_token}\"\n",
        "    elif \"huggingface.co\" in vae_url and api_token:\n",
        "      header = f\"Authorization: Bearer {api_token}\"\n",
        "\n",
        "    stripped_model_vae = vae_url.strip()\n",
        "\n",
        "    if download_in_drive:\n",
        "      vae_dir = Path(drive_dir).joinpath(\"Downloaded_VAEs\")\n",
        "\n",
        "      if not Path(vae_dir).exists():\n",
        "        Path(vae_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    if vae_name:\n",
        "      validated_name = vae_name.translate(str.maketrans('', '', '\\\\/:*?\"<>|'))\n",
        "\n",
        "      if not validated_name.endswith((\".ckpt\", \".safetensors\")):\n",
        "        vae_file = vae_dir.joinpath(f\"{validated_name}.safetensors\")\n",
        "      else:\n",
        "        vae_file = vae_dir.joinpath(validated_name)\n",
        "    elif stripped_model_vae.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "      vae_file = vae_dir.joinpath(stripped_model_vae[stripped_model_vae.rfind('/'):].replace(\"/\", \"\"))\n",
        "    else:\n",
        "      vae_file = vae_dir.joinpath(\"downloaded_vae.safetensors\")\n",
        "      if Path(vae_file).exists() and not download_in_drive:\n",
        "        !rm \"{vae_file}\"\n",
        "\n",
        "    print(f\"Downloading vae from {vae_url}...\")\n",
        "    !aria2c \"{vae_url}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "  else:\n",
        "    vae_file = \"\"\n",
        "\n",
        "def main():\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You have to run the 2nd step first!\")\n",
        "    return\n",
        "\n",
        "  if download_in_drive and not use_drive:\n",
        "    print(\"You are trying to download the model and/or VAE in your drive but you didn't mount it. Please select the 'use_drive' option in 2nd step.\")\n",
        "    return\n",
        "\n",
        "  download_model()\n",
        "  download_vae()\n",
        "\n",
        "try:\n",
        "  main()\n",
        "  thrid_step_done = True\n",
        "except Exception as e:\n",
        "  print(f\"Failed to download the models\\n{e}\")\n",
        "  thrid_step_done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "66XBK6B_iSYj",
        "outputId": "70e87053-a351-46d3-d490-bad7cd6bda59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The path of the zip doesn't exists!\n"
          ]
        }
      ],
      "source": [
        "# @title ## 4. Upload your dataset ![doro shifty](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_shifty.png)\n",
        "import re\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown ### Unzip the dataset\n",
        "# @markdown If you have a dataset in a zip file, you can specify the path to it below. This will extract the dataset into the dataset directory specified in step 2. It supports downloading the zip from **HuggingFace**. To get the correct link you only need to follow the steps [for models/VAEs](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#from-huggingface) but applying them to the zip file.\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/Leonard/LoRa/dataset.zip\" # @param {type: \"string\"}\n",
        "# @markdown Specify the name of your dataset directory. If it doesn't exist, it will be created. If you have multiple dataset directories, extract each zip file into its respective dataset directory.\n",
        "extract_to_dataset_dir = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown Provide a [HuggingFace Access Token](https://huggingface.co/settings/tokens) if your dataset is in a private repository.\n",
        "hf_token = \"\" # @param {type: \"string\"}\n",
        "\n",
        "if not \"installed_dependencies\" in globals():\n",
        "  print(\"Installing missing dependency...\")\n",
        "  !apt -y update -qq\n",
        "  !apt install -y aria2 -qq\n",
        "  globals().setdefault(\"installed_dependencies\", True)\n",
        "\n",
        "def extract_dataset():\n",
        "  global zip_path\n",
        "  is_from_hf = False\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  if zip_path.startswith(\"https://huggingface.co/\"):\n",
        "    is_from_hf = True\n",
        "\n",
        "  if not Path(zip_path).exists() and not is_from_hf:\n",
        "    print(\"The path of the zip doesn't exists!\")\n",
        "    return\n",
        "\n",
        "  if \"drive/MyDrive\" in zip_path and not Path(drive_dir).exists():\n",
        "    print(\"Your trying to access drive but you didn't mount it!\")\n",
        "    return\n",
        "\n",
        "  dataset_dir = root_path.joinpath(project_path, extract_to_dataset_dir)\n",
        "  if Path(drive_dir).exists():\n",
        "    dataset_dir = drive_dir.joinpath(project_path, extract_to_dataset_dir)\n",
        "\n",
        "  if not Path(dataset_dir).exists():\n",
        "    Path(dataset_dir).mkdir(exist_ok=True)\n",
        "    print(f\"Created dataset directory on new location because it didn't exist before: {dataset_dir}\")\n",
        "\n",
        "  if is_from_hf and re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n",
        "    print(\"Zip file from HuggingFace detected, attempting to download...\")\n",
        "\n",
        "    if \"blob\" in zip_path:\n",
        "      zip_path = zip_path.replace(\"blob\", \"resolve\")\n",
        "    header = f\"Authorization: Bearer {hf_token}\" if hf_token else \"\"\n",
        "\n",
        "    !aria2c \"{zip_path}\" --console-log-level=warn --header=\"{header}\" -c -s 16 -x 16 -k 10M -d / -o \"/content/dataset.zip\"\n",
        "    zip_path = \"/content/dataset.zip\"\n",
        "  elif is_from_hf and not re.search(r\"https:\\/\\/huggingface\\.co\\/.*(?:resolve|blob).*\\.zip\", zip_path):\n",
        "    print(\"Invalid URL provided for downloading the zip file.\")\n",
        "    return\n",
        "\n",
        "  print(\"Extracting dataset...\")\n",
        "\n",
        "  with zipfile.ZipFile(zip_path, 'r') as f:\n",
        "    f.extractall(dataset_dir)\n",
        "\n",
        "  print(f\"Dataset extracted in {dataset_dir}\")\n",
        "\n",
        "  if is_from_hf:\n",
        "    print(\"Removing temporary zip file...\")\n",
        "    !rm \"{zip_path}\"\n",
        "    print(\"Done!\")\n",
        "\n",
        "extract_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J86M4s3ohUYv"
      },
      "outputs": [],
      "source": [
        "# @markdown ### Tag your images ![doro syuen](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_syuen.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown As the name suggests, this is the type of tagging you want for your dataset.\n",
        "method = \"Anime\" # @param [\"Anime\", \"Photorealistic\"]\n",
        "# @markdown `(Only applies to Anime method)` The default model used for tagging is `SmilingWolf/wd-eva02-large-tagger-v3`. I find it more accurate than other taggers, but if you have experience, you can use another one and tweak the parameters. If you don't, the default configuration should be fine.\n",
        "model = \"SmilingWolf/wd-eva02-large-tagger-v3\" # @param [\"SmilingWolf/wd-eva02-large-tagger-v3\", \"SmilingWolf/wd-vit-large-tagger-v3\", \"SmilingWolf/wd-swinv2-tagger-v3\", \"SmilingWolf/wd-vit-tagger-v3\", \"SmilingWolf/wd-convnext-tagger-v3\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n",
        "# @markdown The directory name of the dataset you want to tag. You can specify another directory when the previous one is fully tagged, in case you have more than one dataset.\n",
        "dataset_dir_name = \"dataset\" # @param {type: \"string\"}\n",
        "# @markdown The type of file to save your captions.\n",
        "file_extension = \".txt\" # @param [\".txt\", \".caption\"]\n",
        "# @markdown `(Only applies to Anime method)` Specify the tags that you don't want the autotagger to use. Separate each one with a comma `(,)` like this: **1girl, solo, standing, ...**\n",
        "blacklisted_tags = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Only applies to Anime method)` Specify the minimum confidence level required for assigning a tag to the image. A lower threshold results in more tags being assigned. The recommended default value for v2 taggers is 0.35 and for v3 is 0.25.\n",
        "threshold = 0.25 # @param {type: \"slider\", min:0.0, max: 1.0, step:0.01}\n",
        "# @markdown `(Only applies to Photorealistic method)` Specify the minimum number of words (also known as tokens) to include in the captions.\n",
        "caption_min = 10 # @param {type: \"number\"}\n",
        "# @markdown `(Only applies to Photorealistic method)` Specify the maximum number of words (also known as tokens) to include in the captions.\n",
        "caption_max = 75 # @param {type: \"number\"}\n",
        "\n",
        "blacklisted_tags = blacklisted_tags.replace(\" \", \"\")\n",
        "\n",
        "def caption_images():\n",
        "  global use_onnx_runtime\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  dataset_dir = root_path.joinpath(project_path, dataset_dir_name)\n",
        "  if Path(drive_dir).exists():\n",
        "    dataset_dir = drive_dir.joinpath(project_path, dataset_dir_name)\n",
        "\n",
        "  sd_scripts = trainer_dir.joinpath(\"sd_scripts\")\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the step 1 first.\")\n",
        "    return\n",
        "\n",
        "  if not globals().get(\"tagger_dependencies\"):\n",
        "    print(\"Installing missing dependencies...\")\n",
        "    !{venv_pip} install fairscale==0.4.13 timm==0.6.12\n",
        "    !{venv_pip} install onnxruntime-gpu==1.20.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
        "    globals().setdefault(\"tagger_dependencies\", True)\n",
        "\n",
        "  batch_size = 8 if \"v3\" in model or \"swinv2\" in model else 1\n",
        "\n",
        "  model_dir = tagger_models_dir.joinpath(model.split(\"/\")[-1])\n",
        "\n",
        "  print(\"Tagging images\")\n",
        "\n",
        "  if method == \"Anime\":\n",
        "    !{venv_python} {wd_path} \\\n",
        "      {dataset_dir} \\\n",
        "      --repo_id={model} \\\n",
        "      --model_dir={model_dir} \\\n",
        "      --thresh={threshold} \\\n",
        "      --batch_size={batch_size} \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --caption_extension={file_extension} \\\n",
        "      --undesired_tags={blacklisted_tags} \\\n",
        "      --remove_underscore \\\n",
        "      --onnx\n",
        "  else:\n",
        "    os.chdir(sd_scripts)\n",
        "    !{venv_python} finetune/make_captions.py \\\n",
        "      {dataset_dir} \\\n",
        "      --beam_search \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --batch_size=8 \\\n",
        "      --min_length={caption_min} \\\n",
        "      --max_length={caption_max} \\\n",
        "      --caption_extension=.txt\n",
        "    os.chdir(root_path)\n",
        "\n",
        "  print(\"Tagging complete!\")\n",
        "\n",
        "caption_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "PC5JsouHTr26",
        "outputId": "d8a946ec-701c-4338-8970-7e839bf44c20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset paths:\n",
            "  Dataset directory 1: /content/drive/MyDrive/Leonard/LoRa/dataset\n",
            "Model path: /content/pretrained_model/v6.safetensors\n",
            "VAE path: /content/vae/sdxl_vae.safetensors\n",
            "Output path: /content/drive/MyDrive/Leonard/LoRa/output\n",
            "Config file path: It's saved locally on your machine\n",
            "Tags file path: It's saved locally on your machine\n"
          ]
        }
      ],
      "source": [
        "# @title ## 5. Start the training ![doro cinderella](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_cinderella.png)\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Execute this cell to obtain the paths. Input these paths into the UI to start the training.\n",
        "\n",
        "def print_paths():\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  dataset_dirs = []\n",
        "  project_base_dir = root_path.joinpath(project_path)\n",
        "  if globals().get(\"use_drive\"):\n",
        "    project_base_dir = drive_dir.joinpath(project_path)\n",
        "\n",
        "  for id, p_dataset_m_dir in enumerate(dataset_dir_name.replace(\" \", \"\").split(',')):\n",
        "    dataset_dirs.append(f\"Dataset directory {id + 1}: {project_base_dir.joinpath(p_dataset_m_dir)}\")\n",
        "\n",
        "  model_path = model_file or \"None or you didn't run the cell to download it either because you forgot or because you have the model in drive\"\n",
        "  vae_path = vae_file or \"None or you didn't run the cell to download it either because you forgot or because you have the VAE in drive\"\n",
        "  output_path = project_base_dir.joinpath(output_dir_name)\n",
        "\n",
        "  print(\"Dataset paths:\\n  {0}\\nModel path: {1}\\nVAE path: {2}\\nOutput path: {3}\\nConfig file path: {4}\\nTags file path: {4}\".format('\\n  '.join(dataset_dirs), model_path.as_posix().replace(\" \", \"\"), vae_path, output_path, \"It's saved locally on your machine\"))\n",
        "\n",
        "print_paths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "80gDArpjBLk-",
        "outputId": "cf7261b1-f97d-49bd-d783-59fbda34fa67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/trainer/sd_scripts/venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "rich is not installed, using basic logging\n",
            "/content/trainer/sd_scripts/venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "tokenizer_config.json: 100% 592/592 [00:00<00:00, 3.29MB/s]\n",
            "vocab.json: 100% 862k/862k [00:00<00:00, 4.59MB/s]\n",
            "merges.txt: 100% 525k/525k [00:00<00:00, 2.88MB/s]\n",
            "special_tokens_map.json: 100% 389/389 [00:00<00:00, 3.19MB/s]\n",
            "tokenizer.json: 100% 2.22M/2.22M [00:00<00:00, 16.4MB/s]\n",
            "config.json: 100% 4.19k/4.19k [00:00<00:00, 16.0MB/s]\n",
            "/content/trainer/sd_scripts/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Download cloudflared...: 100% 36.1M/36.1M [00:00<00:00, 265MB/s]\n",
            " * Running on https://allocation-motivated-advocacy-assembly.trycloudflare.com\n",
            " * Traffic stats available on http://127.0.0.1:20241/metrics\n",
            "Duplicate file found, changing file name to test_1\n",
            "cloudflared process killed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Running this cell will create a tunnel that allows you to connect from your local UI so you can send the training settings to colab. If you don't have it installed, please install it [from here](https://github.com/derrian-distro/LoRA_Easy_Training_Scripts). Read the [instructions for installation](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-install-the-ui). Once you launch the UI, set up your training parameters, copy the given URL into your interface, and click \"Start training\".\n",
        "\n",
        "# @markdown `(Optional)` Ngrok is an alternative method, and you need a token that you can obtain from [Ngrok's dashboard](https://dashboard.ngrok.com/get-started/your-authtoken). I recommend using it only if you want, have experience, or if the default tunnel provider is down. [How to obtain Ngrok token](https://github.com/Jelosus2/LoRA_Easy_Training_Colab?tab=readme-ov-file#how-to-obtain-the-ngrok-token)\n",
        "\n",
        "use_ngrok = False # @param {type: \"boolean\"}\n",
        "ngrok_token = \"\" # @param {type: \"string\"}\n",
        "\n",
        "fifth_step_done = False\n",
        "\n",
        "def init_tunnel():\n",
        "  global fifth_step_done\n",
        "\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the 1st step first.\")\n",
        "    return\n",
        "\n",
        "  if not globals().get(\"second_step_done\"):\n",
        "    print(\"You didn't complete the second step!\")\n",
        "    return\n",
        "\n",
        "  config_file = trainer_dir.joinpath(\"config.json\")\n",
        "\n",
        "  if use_ngrok:\n",
        "    if not ngrok_token:\n",
        "      print(\"The ngrok token must not be empty!\")\n",
        "      return\n",
        "\n",
        "    with open(config_file, 'r') as config:\n",
        "      data = json.load(config)\n",
        "\n",
        "    data[\"remote_mode\"] = \"ngrok\"\n",
        "    data[\"ngrok_token\"] = ngrok_token\n",
        "\n",
        "    with open(config_file, 'w') as config:\n",
        "      json.dump(data, config, indent=2)\n",
        "  else:\n",
        "    with open(config_file, 'r') as config:\n",
        "      data = json.load(config)\n",
        "\n",
        "    if data[\"remote_mode\"] == \"ngrok\":\n",
        "      data[\"remote_mode\"] = \"cloudflared\"\n",
        "      data[\"ngrok_token\"] = \"\"\n",
        "\n",
        "      with open(config_file, 'w') as config:\n",
        "        json.dump(data, config, indent=2)\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "  !chmod 755 run.sh\n",
        "  !./run.sh\n",
        "  os.chdir(root_path)\n",
        "\n",
        "  fifth_step_done = True\n",
        "\n",
        "init_tunnel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "ufU4_DUl2Rzv",
        "outputId": "6a6f9776-c144-4412-cbca-d583e4ce4755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/trainer/sd_scripts/venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/content/trainer/sd_scripts/venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "rich is not installed, using basic logging\n",
            "Loading settings from /content/trainer/runtime_store/config.toml...\n",
            "/content/trainer/runtime_store/config\n",
            "tokenizer_config.json: 100% 905/905 [00:00<00:00, 5.74MB/s]\n",
            "vocab.json: 100% 961k/961k [00:00<00:00, 4.49MB/s]\n",
            "merges.txt: 100% 525k/525k [00:00<00:00, 3.62MB/s]\n",
            "special_tokens_map.json: 100% 389/389 [00:00<00:00, 2.34MB/s]\n",
            "tokenizer.json: 100% 2.22M/2.22M [00:00<00:00, 5.99MB/s]\n",
            "/content/trainer/sd_scripts/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 904/904 [00:00<00:00, 6.77MB/s]\n",
            "vocab.json: 100% 862k/862k [00:00<00:00, 113MB/s]\n",
            "merges.txt: 100% 525k/525k [00:00<00:00, 25.3MB/s]\n",
            "special_tokens_map.json: 100% 389/389 [00:00<00:00, 2.64MB/s]\n",
            "tokenizer.json: 100% 2.22M/2.22M [00:00<00:00, 22.1MB/s]\n",
            "Loading dataset config from /content/trainer/runtime_store/dataset.toml\n",
            "prepare images.\n",
            "get image size from name of cache files\n",
            "100% 6/6 [00:00<00:00, 87078.98it/s]\n",
            "set image size from cache files: 0/6\n",
            "found directory /content/drive/MyDrive/Leonard/LoRa/dataset contains 6 image files\n",
            "read caption: 100% 6/6 [00:00<00:00, 436.00it/s]\n",
            "6 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "[Dataset 0]\n",
            "  batch_size: 1\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  network_multiplier: 1.0\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/Leonard/LoRa/dataset\"\n",
            "    image_count: 6\n",
            "    num_repeats: 1\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 1\n",
            "    keep_tokens_separator: \n",
            "    caption_separator: ,\n",
            "    secondary_separator: None\n",
            "    enable_wildcard: False\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epochs: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    caption_prefix: None\n",
            "    caption_suffix: None\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1\n",
            "    token_warmup_step: 0\n",
            "    alpha_mask: False\n",
            "    custom_attributes: {}\n",
            "    is_reg: False\n",
            "    class_tokens: None\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 6/6 [00:04<00:00,  1.49it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (448, 576), count: 3\n",
            "bucket 1: resolution (512, 512), count: 2\n",
            "bucket 2: resolution (640, 384), count: 1\n",
            "mean ar error (without repeats): 0.06178817546382481\n",
            "preparing accelerator\n",
            "/content/trainer/sd_scripts/venv/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "accelerator device: cuda\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint: /content/pretrained_model/v6.safetensors\n",
            "building U-Net\n",
            "loading U-Net from checkpoint\n",
            "U-Net: <All keys matched successfully>\n",
            "building text encoders\n",
            "loading text encoders from checkpoint\n",
            "text encoder 1: <All keys matched successfully>\n",
            "text encoder 2: <All keys matched successfully>\n",
            "building VAE\n",
            "loading VAE from checkpoint\n",
            "VAE: <All keys matched successfully>\n",
            "load VAE: /content/vae/sdxl_vae.safetensors\n",
            "additional VAE loaded\n",
            "Enable SDPA for U-Net\n",
            "import network module: networks.lora\n",
            "[Dataset 0]\n",
            "caching latents with caching strategy.\n",
            "caching latents...\n",
            "100% 6/6 [00:03<00:00,  1.55it/s]\n",
            "create LoRA network. base dim (rank): 32, alpha: 16.0\n",
            "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
            "create LoRA for Text Encoder 1:\n",
            "create LoRA for Text Encoder 2:\n",
            "create LoRA for Text Encoder: 88 modules.\n",
            "create LoRA for U-Net: 722 modules.\n",
            "enable LoRA for text encoder: 88 modules\n",
            "enable LoRA for U-Net: 722 modules\n",
            "prepare optimizer, data loader etc.\n",
            "use AdamW optimizer | {'weight_decay': 0.1}\n",
            "override steps. steps for 7 epochs is / 指定エポックまでのステップ数: 42\n",
            "enable full fp16 training.\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 6\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 6\n",
            "  num epochs / epoch数: 7\n",
            "  batch size per device / バッチサイズ: 1\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 42\n",
            "steps:   0% 0/42 [00:00<?, ?it/s]unet dtype: torch.float16, device: cuda:0\n",
            "text_encoder [0] dtype: torch.float16, device: cuda:0\n",
            "text_encoder [1] dtype: torch.float16, device: cuda:0\n",
            "\n",
            "epoch 1/7\n",
            "epoch is incremented. current_epoch: 0, epoch: 1\n",
            "steps:  14% 6/42 [00:10<01:01,  1.70s/it, avr_loss=nan]\n",
            "saving checkpoint: /content/drive/MyDrive/Leonard/LoRa/output/test_1-000001.safetensors\n",
            "\n",
            "epoch 2/7\n",
            "epoch is incremented. current_epoch: 1, epoch: 2\n",
            "steps:  29% 12/42 [00:20<00:50,  1.69s/it, avr_loss=nan]\n",
            "saving checkpoint: /content/drive/MyDrive/Leonard/LoRa/output/test_1-000002.safetensors\n",
            "\n",
            "epoch 3/7\n",
            "epoch is incremented. current_epoch: 2, epoch: 3\n",
            "steps:  43% 18/42 [00:32<00:42,  1.78s/it, avr_loss=nan]\n",
            "saving checkpoint: /content/drive/MyDrive/Leonard/LoRa/output/test_1-000003.safetensors\n",
            "\n",
            "epoch 4/7\n",
            "epoch is incremented. current_epoch: 3, epoch: 4\n",
            "steps:  57% 24/42 [00:40<00:30,  1.71s/it, avr_loss=nan]\n",
            "saving checkpoint: /content/drive/MyDrive/Leonard/LoRa/output/test_1-000004.safetensors\n",
            "\n",
            "epoch 5/7\n",
            "epoch is incremented. current_epoch: 4, epoch: 5\n",
            "steps:  71% 30/42 [00:51<00:20,  1.72s/it, avr_loss=nan]\n",
            "saving checkpoint: /content/drive/MyDrive/Leonard/LoRa/output/test_1-000005.safetensors\n",
            "\n",
            "epoch 6/7\n",
            "epoch is incremented. current_epoch: 5, epoch: 6\n",
            "steps:  86% 36/42 [01:02<00:10,  1.73s/it, avr_loss=nan]\n",
            "saving checkpoint: /content/drive/MyDrive/Leonard/LoRa/output/test_1-000006.safetensors\n",
            "\n",
            "epoch 7/7\n",
            "epoch is incremented. current_epoch: 6, epoch: 7\n",
            "steps: 100% 42/42 [01:12<00:00,  1.73s/it, avr_loss=nan]\n",
            "saving checkpoint: /content/drive/MyDrive/Leonard/LoRa/output/test_1.safetensors\n",
            "model saved.\n",
            "steps: 100% 42/42 [01:14<00:00,  1.78s/it, avr_loss=nan]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Run this cell to start the training\n",
        "\n",
        "# @markdown Are you training on sdxl?\n",
        "sdxl = True # @param {type: \"boolean\"}\n",
        "\n",
        "def start_training(is_sdxl: bool):\n",
        "  if not globals().get(\"fifth_step_done\"):\n",
        "    print(\"Run the cell above this one first!\")\n",
        "    return\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "\n",
        "  config = Path(\"runtime_store/config.toml\").resolve()\n",
        "  dataset = Path(\"runtime_store/dataset.toml\").resolve()\n",
        "\n",
        "  if not Path(config).exists() and not Path(dataset).exists():\n",
        "    print(\"The required files were not generated while running the above cell, please check again!\")\n",
        "    return\n",
        "\n",
        "  sd_scripts = Path(\"sd_scripts\").resolve()\n",
        "  training_network = \"sdxl_train_network.py\" if is_sdxl else \"train_network.py\"\n",
        "\n",
        "  !{venv_python} {sd_scripts.joinpath(training_network)} \\\n",
        "    --config_file={config} \\\n",
        "    --dataset_config={dataset}\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "start_training(sdxl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Utils ![doro anachiro](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_anachiro.png)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown ### LoRA Resizer ![doro grave](https://raw.githubusercontent.com/Jelosus2/Lora_Easy_Training_Colab/refs/heads/main/assets/doro_grave.png)\n",
        "\n",
        "# @markdown The path pointing to the LoRA file you want to resize.\n",
        "lora = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Optional)` The path of the directory where the resized LoRA will be saved. If not specified the parent directory of the loaded LoRA will be used.\n",
        "output_dir = \"\" # @param {type: \"string\"}\n",
        "# @markdown `(Optional)` The name for the resized LoRA file. If not specified the name of the loaded LoRA will be used appending **_resized** to it.\n",
        "output_name = \"\" # @param {type: \"string\"}\n",
        "# @markdown The precision for saving the resized LoRA. `fp16` is the usual precision to use. **Don't touch unless you know what you are doing!**\n",
        "save_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"float\"]\n",
        "# @markdown The new dimensions, aka dim, for the LoRA.\n",
        "new_dim = 4 # @param {type: \"number\"}\n",
        "# @markdown `(LoCon-like networks only)` The new conv dimensions, aka conv dim, for the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc**. Keep the value less than 1 to omit it's usage.\n",
        "new_conv_dim = 0 # @param {type: \"number\"}\n",
        "# @markdown Enables/disables the usage of `dynamic_method` and `dynamic_param`. **Don't touch unless you know what you are doing!**\n",
        "use_dynamic = False # @param {type: \"boolean\"}\n",
        "# @markdown Method used to calculate the resize. `sv_fro` is the usual method to use.\n",
        "dynamic_method = \"sv_fro\" # @param [\"sv_fro\", \"sv_ratio\", \"sv_cumulative\"]\n",
        "# @markdown Value used by the `dynamic_method` to calculate the resize.\n",
        "dynamic_param = 0.9700 # @param {type: \"number\"}\n",
        "# @markdown Use the GPU resources to resize the LoRA. If disabled it will use the CPU which is **not recommended!**\n",
        "use_gpu = True # @param {type: \"boolean\"}\n",
        "# @markdown Prints in the console the information about the resizing when the process finishes.\n",
        "verbose_printing = False # @param {type: \"boolean\"}\n",
        "# @markdown `(LoCon-like networks only)` Removes the conv dim layers from the LoRA. Only use on networks that are trained with conv. For example: **LoCon, LyCORIS, LoHa, Lokr, etc. Don't touch unless you know what you are doing!**\n",
        "remove_conv_dims = False # @param {type: \"boolean\"}\n",
        "# @markdown Removes the linear dim layers (which is what is trained usually in a LoRA) from the LoRA. **Don't touch unless you know what you are doing!**\n",
        "remove_linear_dims = False # @param {type: \"boolean\"}\n",
        "\n",
        "def validate() -> tuple[bool, bool]:\n",
        "  global output_dir, output_name\n",
        "\n",
        "  failed = False\n",
        "  use_conv = True\n",
        "  if not globals().get(\"first_step_done\"):\n",
        "    print(\"Please run the 1st step first.\")\n",
        "    failed = True\n",
        "\n",
        "  if not Path(lora).is_file() or Path(lora).suffix not in [\".ckpt\", \".safetensors\"]:\n",
        "    print(\"The path to the LoRA file is invalid.\")\n",
        "    failed = True\n",
        "\n",
        "  if not Path(output_dir).is_dir() or not output_dir:\n",
        "    output_dir = Path(output_dir).parent if output_dir else Path(lora).parent\n",
        "    if not output_dir.is_dir():\n",
        "      print(\"The path to the output folder is invalid, or not a folder\")\n",
        "      failed = True\n",
        "    output_dir = output_dir.as_posix()\n",
        "\n",
        "  if not output_name:\n",
        "    output_name = f\"{Path(lora).name.split('.')[0]}_resized\"\n",
        "  else:\n",
        "    output_name = output_name.split(\".\")[0]\n",
        "\n",
        "  if Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n",
        "    idx = 1\n",
        "    temp_name = output_name\n",
        "    while Path(output_dir).joinpath(f\"{output_name}.safetensors\").exists():\n",
        "      output_name = f\"{temp_name}_{idx}\"\n",
        "      idx += 1\n",
        "\n",
        "    print(f\"Duplicated file in the output directory, file name changed to {output_name}\")\n",
        "\n",
        "  if new_dim < 1:\n",
        "    print(\"The new dim must be 1 or greater\")\n",
        "    failed = True\n",
        "\n",
        "  if new_conv_dim < 1:\n",
        "    print(\"Skipping setting new conv dim, using new dim only\")\n",
        "    use_conv = False\n",
        "\n",
        "  if use_dynamic and dynamic_param <= 0:\n",
        "    print(\"The dynamic param must be greater than 0\")\n",
        "    failed = True\n",
        "\n",
        "  return failed, use_conv\n",
        "\n",
        "def resize_lora(use_conv: bool):\n",
        "  output_file = Path(output_dir).joinpath(f\"{output_name}.safetensors\").resolve()\n",
        "\n",
        "  new_conv_arg = f\"--new_conv_rank={new_conv_dim}\" if use_conv else \"\"\n",
        "  dynamic_method_arg = f\"--dynamic_method={dynamic_method}\" if use_dynamic else \"\"\n",
        "  dynamic_param_arg = \"--dynamic_param={0:.4f}\".format(dynamic_param) if use_dynamic else \"\"\n",
        "\n",
        "  os.chdir(trainer_dir)\n",
        "\n",
        "  !{venv_python} {Path(\"utils/resize_lora.py\").resolve()} \\\n",
        "    --model={lora} \\\n",
        "    --save_precision={save_precision} \\\n",
        "    --new_rank={new_dim} \\\n",
        "    --save_to={output_file} \\\n",
        "    {new_conv_arg} \\\n",
        "    {dynamic_method_arg} \\\n",
        "    {dynamic_param_arg} \\\n",
        "    {\"--verbose\" if verbose_printing else \"\"} \\\n",
        "    {\"--device=cuda\" if use_gpu else \"\"} \\\n",
        "    {\"--del_conv\" if remove_conv_dims else \"\"} \\\n",
        "    {\"--del_linear\" if remove_linear_dims else \"\"} \\\n",
        "\n",
        "  os.chdir(root_path)\n",
        "\n",
        "def main():\n",
        "  failed, use_conv = validate()\n",
        "  if failed:\n",
        "    return\n",
        "\n",
        "  resize_lora(use_conv)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "pEf-buIXyDLg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "id": "xmWmlxtT8ZcE",
        "outputId": "f7c82a58-66be-4499-d6d5-70f0558b84e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "7fd5ac20248845688202bcf7f079e6d1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Chemin vers ton modèle dans Google Drive\n",
        "model_path = \"/content/drive/MyDrive/Leonard/LoRa/output/test-state/model.safetensors\"\n",
        "\n",
        "# Charger le pipeline de Stable Diffusion\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",  # Change selon la base de ton modèle\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None  # Désactive le safety checker si besoin\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Charger les poids entraînés\n",
        "pipe.unet.load_lora_adapter(model_path)\n",
        "\n",
        "\n",
        "# Tester une génération\n",
        "prompt = \"Un chat cybernétique dans un futur dystopique\"\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "# Afficher l'image\n",
        "image.show()\n"
      ],
      "metadata": {
        "id": "2V71D6nP7DYw",
        "outputId": "35a25bd7-fd75-41ec-c97c-1f9912393d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "227865e2de634dba9480fdba072b88ef",
            "be344c1b64a24603b089ab324d1d6f08",
            "ac1d66e732b04b869ef23e99933599c1",
            "37cd0c3d610c47448fe9126353e324b8",
            "3ff09ac318714a95a14e99c9906c5d9b",
            "a9f4f0c9fd964c2fafa9701b32174e31",
            "38d48603b3544d7eb924b7ee04058575",
            "2c10279452f240828cb9be69dd577b98",
            "749ef4abf2394697a2da0e16b022d0ec",
            "8feb177b61c24b94b5940a7185b1ac20",
            "d5853d4bed664a84911a1c912db08cfc"
          ]
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "227865e2de634dba9480fdba072b88ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a94a1a2f0a4a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Charger les poids entraînés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lora_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/loaders/peft.py\u001b[0m in \u001b[0;36mload_lora_adapter\u001b[0;34m(self, pretrained_model_name_or_path_or_dict, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mnetwork_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prefix}.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetwork_alphas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malpha_keys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mlora_config_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_alpha_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_alphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_state_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mlora_config_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_adjust_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_config_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/peft_utils.py\u001b[0m in \u001b[0;36mget_peft_kwargs\u001b[0;34m(rank_dict, network_alpha_dict, peft_state_dict, is_unet)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mrank_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0malpha_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlora_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Leonard/LoRa/output/test-state/model.safetensors\"\n",
        "model_weights = load_file(model_path)\n",
        "print(model_weights.items())  # Vérifie les poids disponibles\n",
        "\n"
      ],
      "metadata": {
        "id": "KqQt4cZk-iR7",
        "outputId": "eafc4b1f-0ce7-4fdb-9c03-444b1836a860",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "        [-0.0049, -0.0162, -0.0120,  ...,  0.0236,  0.0199, -0.0139]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight', tensor([[ 0.0209,  0.0122, -0.0086,  ..., -0.0022,  0.0128,  0.0204],\n",
            "        [-0.0263,  0.0043,  0.0161,  ..., -0.0264,  0.0119, -0.0024],\n",
            "        [ 0.0122, -0.0226, -0.0129,  ..., -0.0190, -0.0106, -0.0065],\n",
            "        ...,\n",
            "        [ 0.0211, -0.0099,  0.0248,  ...,  0.0141, -0.0200, -0.0139],\n",
            "        [ 0.0103,  0.0120,  0.0075,  ..., -0.0063,  0.0090,  0.0156],\n",
            "        [-0.0069, -0.0086, -0.0104,  ...,  0.0053, -0.0225, -0.0187]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.lora_down.weight', tensor([[-0.0232,  0.0139, -0.0209,  ..., -0.0004, -0.0062,  0.0255],\n",
            "        [-0.0169,  0.0255,  0.0064,  ...,  0.0262,  0.0273, -0.0096],\n",
            "        [-0.0231,  0.0213, -0.0242,  ...,  0.0077, -0.0126,  0.0150],\n",
            "        ...,\n",
            "        [-0.0132, -0.0160, -0.0023,  ...,  0.0135, -0.0104,  0.0031],\n",
            "        [-0.0254,  0.0019, -0.0108,  ...,  0.0136,  0.0198, -0.0170],\n",
            "        [ 0.0069, -0.0010, -0.0202,  ..., -0.0015, -0.0024,  0.0195]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.lora_down.weight', tensor([[ 0.0070,  0.0173,  0.0017,  ...,  0.0203,  0.0158, -0.0169],\n",
            "        [-0.0119,  0.0250, -0.0146,  ..., -0.0141, -0.0221, -0.0275],\n",
            "        [-0.0043,  0.0053,  0.0152,  ..., -0.0015, -0.0205, -0.0166],\n",
            "        ...,\n",
            "        [ 0.0117,  0.0217, -0.0218,  ..., -0.0009, -0.0170, -0.0254],\n",
            "        [ 0.0132, -0.0190, -0.0015,  ...,  0.0007,  0.0097,  0.0234],\n",
            "        [-0.0071,  0.0145,  0.0054,  ..., -0.0222,  0.0128,  0.0017]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.lora_down.weight', tensor([[ 0.0168,  0.0104, -0.0046,  ..., -0.0081, -0.0059, -0.0119],\n",
            "        [-0.0144, -0.0176, -0.0074,  ...,  0.0058,  0.0216,  0.0038],\n",
            "        [ 0.0174, -0.0064,  0.0067,  ..., -0.0117,  0.0091, -0.0201],\n",
            "        ...,\n",
            "        [ 0.0136,  0.0011,  0.0206,  ...,  0.0027,  0.0001,  0.0156],\n",
            "        [-0.0096, -0.0035,  0.0135,  ..., -0.0050,  0.0117,  0.0058],\n",
            "        [ 0.0175, -0.0157, -0.0054,  ...,  0.0026, -0.0020, -0.0119]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight', tensor([[ 0.0182,  0.0106,  0.0050,  ...,  0.0084,  0.0044,  0.0140],\n",
            "        [ 0.0171, -0.0210, -0.0044,  ...,  0.0203,  0.0040, -0.0056],\n",
            "        [ 0.0264, -0.0088, -0.0207,  ..., -0.0186,  0.0256,  0.0210],\n",
            "        ...,\n",
            "        [-0.0219, -0.0030,  0.0247,  ...,  0.0033, -0.0119,  0.0267],\n",
            "        [-0.0157,  0.0062,  0.0213,  ..., -0.0268,  0.0048,  0.0020],\n",
            "        [-0.0189,  0.0086, -0.0191,  ..., -0.0024,  0.0006, -0.0129]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.lora_down.weight', tensor([[-0.0107,  0.0136,  0.0132,  ...,  0.0027, -0.0105,  0.0248],\n",
            "        [-0.0179,  0.0145,  0.0201,  ..., -0.0230,  0.0027,  0.0186],\n",
            "        [ 0.0102, -0.0207, -0.0088,  ...,  0.0140,  0.0039,  0.0176],\n",
            "        ...,\n",
            "        [-0.0203, -0.0067,  0.0087,  ...,  0.0234,  0.0018,  0.0095],\n",
            "        [ 0.0229,  0.0253,  0.0198,  ...,  0.0020,  0.0173,  0.0240],\n",
            "        [-0.0083,  0.0092, -0.0259,  ..., -0.0119, -0.0211, -0.0115]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.lora_down.weight', tensor([[-0.0008,  0.0080,  0.0099,  ...,  0.0093,  0.0077,  0.0213],\n",
            "        [-0.0144, -0.0165, -0.0095,  ..., -0.0173, -0.0023, -0.0014],\n",
            "        [ 0.0107, -0.0021, -0.0083,  ..., -0.0123,  0.0036,  0.0132],\n",
            "        ...,\n",
            "        [ 0.0138, -0.0160,  0.0172,  ...,  0.0008,  0.0056, -0.0003],\n",
            "        [ 0.0036,  0.0037,  0.0040,  ...,  0.0161,  0.0042,  0.0025],\n",
            "        [ 0.0212, -0.0101,  0.0094,  ..., -0.0096,  0.0041,  0.0207]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight', tensor([[-0.0140, -0.0027, -0.0135,  ...,  0.0247, -0.0160, -0.0129],\n",
            "        [-0.0035, -0.0043, -0.0256,  ...,  0.0157,  0.0266,  0.0264],\n",
            "        [ 0.0147, -0.0270, -0.0117,  ...,  0.0088,  0.0018, -0.0190],\n",
            "        ...,\n",
            "        [ 0.0278,  0.0240, -0.0180,  ..., -0.0200,  0.0022, -0.0134],\n",
            "        [-0.0153, -0.0046, -0.0242,  ...,  0.0071, -0.0040, -0.0107],\n",
            "        [-0.0059, -0.0081, -0.0246,  ..., -0.0199,  0.0264,  0.0113]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.lora_down.weight', tensor([[ 0.0107, -0.0007,  0.0114,  ..., -0.0005,  0.0011,  0.0119],\n",
            "        [-0.0053,  0.0043, -0.0080,  ...,  0.0017,  0.0070, -0.0103],\n",
            "        [-0.0061,  0.0014, -0.0121,  ...,  0.0025,  0.0084, -0.0103],\n",
            "        ...,\n",
            "        [ 0.0110, -0.0026,  0.0133,  ...,  0.0016,  0.0107,  0.0040],\n",
            "        [ 0.0112,  0.0067,  0.0050,  ..., -0.0053,  0.0050, -0.0113],\n",
            "        [ 0.0043, -0.0037,  0.0093,  ..., -0.0071,  0.0127,  0.0057]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.lora_down.weight', tensor([[ 0.0164,  0.0151,  0.0111,  ...,  0.0237, -0.0082, -0.0120],\n",
            "        [-0.0208, -0.0181,  0.0269,  ...,  0.0255, -0.0111, -0.0247],\n",
            "        [-0.0056,  0.0200,  0.0053,  ..., -0.0154, -0.0254, -0.0007],\n",
            "        ...,\n",
            "        [-0.0098, -0.0219,  0.0115,  ...,  0.0053, -0.0237,  0.0144],\n",
            "        [-0.0205,  0.0200,  0.0068,  ...,  0.0108, -0.0155, -0.0181],\n",
            "        [-0.0112, -0.0102, -0.0061,  ...,  0.0235, -0.0145, -0.0278]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight', tensor([[-0.0199, -0.0268,  0.0202,  ...,  0.0093, -0.0086,  0.0231],\n",
            "        [-0.0055,  0.0049, -0.0206,  ..., -0.0006, -0.0135, -0.0270],\n",
            "        [-0.0032, -0.0098, -0.0044,  ...,  0.0168, -0.0271, -0.0271],\n",
            "        ...,\n",
            "        [ 0.0131,  0.0018,  0.0040,  ...,  0.0142,  0.0119,  0.0158],\n",
            "        [-0.0079,  0.0101,  0.0210,  ...,  0.0190,  0.0267,  0.0148],\n",
            "        [-0.0126, -0.0247,  0.0270,  ..., -0.0111,  0.0002,  0.0075]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.lora_down.weight', tensor([[-0.0211,  0.0148, -0.0019,  ..., -0.0195,  0.0041,  0.0065],\n",
            "        [-0.0195, -0.0170, -0.0157,  ..., -0.0002,  0.0083, -0.0166],\n",
            "        [ 0.0071,  0.0010, -0.0144,  ..., -0.0127,  0.0067,  0.0149],\n",
            "        ...,\n",
            "        [-0.0044,  0.0095, -0.0145,  ...,  0.0123,  0.0155,  0.0152],\n",
            "        [ 0.0005, -0.0159,  0.0115,  ...,  0.0100, -0.0272,  0.0125],\n",
            "        [ 0.0009, -0.0125,  0.0212,  ..., -0.0018, -0.0240,  0.0228]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.lora_down.weight', tensor([[ 0.0021, -0.0223, -0.0034,  ...,  0.0181, -0.0202,  0.0132],\n",
            "        [ 0.0269,  0.0180, -0.0237,  ...,  0.0151, -0.0250,  0.0137],\n",
            "        [ 0.0216, -0.0232,  0.0054,  ..., -0.0178, -0.0179, -0.0202],\n",
            "        ...,\n",
            "        [-0.0094,  0.0194,  0.0065,  ..., -0.0060, -0.0268, -0.0145],\n",
            "        [ 0.0100,  0.0216,  0.0003,  ..., -0.0218,  0.0018, -0.0015],\n",
            "        [-0.0052, -0.0015,  0.0266,  ..., -0.0176, -0.0166,  0.0031]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.lora_down.weight', tensor([[ 0.0194,  0.0055,  0.0012,  ...,  0.0113, -0.0047,  0.0148],\n",
            "        [ 0.0060, -0.0141, -0.0011,  ...,  0.0068,  0.0157,  0.0197],\n",
            "        [ 0.0025,  0.0005,  0.0032,  ..., -0.0101,  0.0197, -0.0191],\n",
            "        ...,\n",
            "        [ 0.0211, -0.0165, -0.0032,  ...,  0.0199, -0.0182, -0.0176],\n",
            "        [ 0.0046,  0.0158,  0.0143,  ..., -0.0204,  0.0034, -0.0014],\n",
            "        [-0.0146,  0.0142,  0.0146,  ..., -0.0047, -0.0141, -0.0180]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight', tensor([[ 0.0190,  0.0012,  0.0020,  ..., -0.0140,  0.0027,  0.0274],\n",
            "        [ 0.0232,  0.0153,  0.0031,  ..., -0.0061,  0.0274,  0.0097],\n",
            "        [ 0.0180, -0.0073,  0.0140,  ...,  0.0091,  0.0146,  0.0115],\n",
            "        ...,\n",
            "        [-0.0123, -0.0193,  0.0097,  ...,  0.0253, -0.0174, -0.0032],\n",
            "        [ 0.0001, -0.0049,  0.0231,  ...,  0.0266,  0.0253, -0.0123],\n",
            "        [-0.0131, -0.0031,  0.0025,  ..., -0.0205, -0.0202, -0.0194]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.lora_down.weight', tensor([[-0.0202,  0.0236, -0.0116,  ...,  0.0123, -0.0160,  0.0268],\n",
            "        [-0.0274,  0.0269, -0.0196,  ...,  0.0187,  0.0123, -0.0146],\n",
            "        [-0.0229, -0.0088, -0.0053,  ..., -0.0056,  0.0198,  0.0145],\n",
            "        ...,\n",
            "        [ 0.0226, -0.0120, -0.0048,  ...,  0.0267,  0.0032, -0.0277],\n",
            "        [-0.0167,  0.0029,  0.0205,  ...,  0.0206,  0.0066, -0.0064],\n",
            "        [-0.0212, -0.0096, -0.0079,  ..., -0.0259, -0.0134,  0.0108]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.lora_down.weight', tensor([[-0.0049,  0.0056,  0.0002,  ..., -0.0154,  0.0151,  0.0097],\n",
            "        [-0.0073, -0.0077, -0.0018,  ...,  0.0023, -0.0099, -0.0105],\n",
            "        [-0.0045,  0.0024, -0.0024,  ..., -0.0060, -0.0085,  0.0134],\n",
            "        ...,\n",
            "        [-0.0020,  0.0151,  0.0177,  ..., -0.0115, -0.0097, -0.0069],\n",
            "        [-0.0170, -0.0129,  0.0009,  ..., -0.0170,  0.0210,  0.0104],\n",
            "        [ 0.0046,  0.0169, -0.0083,  ...,  0.0025,  0.0183, -0.0172]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight', tensor([[ 0.0235, -0.0191,  0.0219,  ..., -0.0201,  0.0126,  0.0028],\n",
            "        [ 0.0136, -0.0225, -0.0260,  ...,  0.0044, -0.0200, -0.0091],\n",
            "        [ 0.0190,  0.0158,  0.0197,  ..., -0.0102,  0.0246, -0.0075],\n",
            "        ...,\n",
            "        [-0.0268, -0.0007, -0.0057,  ..., -0.0131, -0.0007,  0.0038],\n",
            "        [ 0.0270,  0.0073, -0.0229,  ..., -0.0127,  0.0089,  0.0268],\n",
            "        [-0.0042,  0.0213, -0.0192,  ...,  0.0013, -0.0080,  0.0032]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.lora_down.weight', tensor([[ 0.0102, -0.0078,  0.0012,  ..., -0.0030, -0.0067,  0.0027],\n",
            "        [-0.0107, -0.0035, -0.0083,  ...,  0.0059, -0.0081, -0.0129],\n",
            "        [ 0.0093,  0.0065,  0.0011,  ..., -0.0114,  0.0065,  0.0138],\n",
            "        ...,\n",
            "        [-0.0064,  0.0003,  0.0045,  ..., -0.0121, -0.0104, -0.0066],\n",
            "        [ 0.0031, -0.0003,  0.0133,  ..., -0.0064, -0.0126,  0.0024],\n",
            "        [ 0.0126,  0.0003, -0.0126,  ...,  0.0054, -0.0064, -0.0075]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.lora_down.weight', tensor([[ 0.0273, -0.0266,  0.0090,  ...,  0.0247, -0.0020, -0.0198],\n",
            "        [-0.0044, -0.0204,  0.0120,  ..., -0.0259,  0.0170,  0.0224],\n",
            "        [ 0.0027, -0.0080,  0.0085,  ..., -0.0131,  0.0177,  0.0181],\n",
            "        ...,\n",
            "        [-0.0230, -0.0019, -0.0159,  ..., -0.0093,  0.0010,  0.0274],\n",
            "        [-0.0020, -0.0225, -0.0026,  ..., -0.0052, -0.0037,  0.0222],\n",
            "        [-0.0044, -0.0119,  0.0025,  ..., -0.0113, -0.0275, -0.0028]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight', tensor([[ 0.0240,  0.0115,  0.0116,  ..., -0.0207, -0.0258,  0.0135],\n",
            "        [-0.0139, -0.0049,  0.0222,  ..., -0.0245,  0.0206, -0.0244],\n",
            "        [-0.0017, -0.0217, -0.0011,  ..., -0.0022, -0.0269, -0.0121],\n",
            "        ...,\n",
            "        [ 0.0010,  0.0129, -0.0012,  ...,  0.0156, -0.0005,  0.0243],\n",
            "        [-0.0199, -0.0227,  0.0041,  ..., -0.0053,  0.0168,  0.0159],\n",
            "        [ 0.0259, -0.0064, -0.0188,  ...,  0.0240, -0.0023,  0.0256]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.lora_down.weight', tensor([[-0.0051, -0.0072,  0.0115,  ...,  0.0204,  0.0135,  0.0075],\n",
            "        [ 0.0083,  0.0121, -0.0098,  ..., -0.0100,  0.0161,  0.0175],\n",
            "        [-0.0218,  0.0199,  0.0137,  ...,  0.0035,  0.0151,  0.0051],\n",
            "        ...,\n",
            "        [ 0.0022,  0.0085,  0.0036,  ...,  0.0278,  0.0052, -0.0117],\n",
            "        [ 0.0091,  0.0173,  0.0012,  ...,  0.0031, -0.0260, -0.0167],\n",
            "        [-0.0235, -0.0150, -0.0132,  ...,  0.0068, -0.0254,  0.0195]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.lora_down.weight', tensor([[ 0.0266, -0.0056, -0.0069,  ..., -0.0251, -0.0235, -0.0017],\n",
            "        [-0.0154,  0.0113, -0.0183,  ...,  0.0107, -0.0059,  0.0073],\n",
            "        [ 0.0025,  0.0001, -0.0208,  ..., -0.0120, -0.0032,  0.0207],\n",
            "        ...,\n",
            "        [-0.0036, -0.0209, -0.0174,  ..., -0.0065, -0.0251,  0.0097],\n",
            "        [ 0.0060,  0.0267,  0.0167,  ...,  0.0115,  0.0206, -0.0082],\n",
            "        [-0.0055, -0.0065,  0.0271,  ...,  0.0200,  0.0199,  0.0155]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.lora_down.weight', tensor([[ 0.0037,  0.0117,  0.0039,  ..., -0.0040,  0.0172,  0.0208],\n",
            "        [-0.0200,  0.0083, -0.0170,  ...,  0.0132,  0.0126,  0.0046],\n",
            "        [-0.0124,  0.0100, -0.0060,  ...,  0.0018,  0.0151, -0.0202],\n",
            "        ...,\n",
            "        [ 0.0104, -0.0006, -0.0037,  ...,  0.0182, -0.0097, -0.0063],\n",
            "        [ 0.0139,  0.0135, -0.0018,  ..., -0.0052, -0.0065, -0.0163],\n",
            "        [-0.0154,  0.0006,  0.0045,  ..., -0.0006,  0.0013, -0.0138]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight', tensor([[ 0.0198, -0.0199, -0.0197,  ..., -0.0169, -0.0170, -0.0215],\n",
            "        [ 0.0055, -0.0209,  0.0015,  ..., -0.0170,  0.0125, -0.0252],\n",
            "        [-0.0218,  0.0089,  0.0092,  ..., -0.0089,  0.0096,  0.0250],\n",
            "        ...,\n",
            "        [ 0.0070, -0.0040,  0.0019,  ..., -0.0191, -0.0065,  0.0189],\n",
            "        [ 0.0070,  0.0179, -0.0157,  ..., -0.0188,  0.0070,  0.0109],\n",
            "        [-0.0171,  0.0164,  0.0187,  ...,  0.0194, -0.0031, -0.0122]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.lora_down.weight', tensor([[ 0.0033,  0.0006, -0.0139,  ...,  0.0151,  0.0133, -0.0039],\n",
            "        [ 0.0197, -0.0240, -0.0127,  ...,  0.0059, -0.0268, -0.0093],\n",
            "        [ 0.0038,  0.0242,  0.0014,  ..., -0.0238,  0.0268,  0.0091],\n",
            "        ...,\n",
            "        [-0.0086, -0.0060,  0.0061,  ..., -0.0125, -0.0226,  0.0139],\n",
            "        [-0.0103,  0.0094, -0.0264,  ..., -0.0121,  0.0132,  0.0266],\n",
            "        [ 0.0213,  0.0237, -0.0149,  ...,  0.0125,  0.0090,  0.0202]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.lora_down.weight', tensor([[ 0.0192, -0.0138,  0.0190,  ...,  0.0025, -0.0021,  0.0056],\n",
            "        [ 0.0002, -0.0140, -0.0099,  ...,  0.0180, -0.0135, -0.0183],\n",
            "        [-0.0167,  0.0106,  0.0171,  ...,  0.0096,  0.0093,  0.0204],\n",
            "        ...,\n",
            "        [ 0.0136, -0.0214,  0.0059,  ..., -0.0092,  0.0141,  0.0093],\n",
            "        [-0.0105, -0.0048, -0.0213,  ...,  0.0139, -0.0085,  0.0030],\n",
            "        [-0.0115,  0.0009, -0.0155,  ..., -0.0014,  0.0050,  0.0126]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight', tensor([[-0.0150,  0.0120,  0.0168,  ...,  0.0246,  0.0083, -0.0157],\n",
            "        [ 0.0122,  0.0156,  0.0109,  ...,  0.0255, -0.0096,  0.0210],\n",
            "        [-0.0185,  0.0097, -0.0156,  ...,  0.0122, -0.0233, -0.0138],\n",
            "        ...,\n",
            "        [-0.0175,  0.0060,  0.0083,  ..., -0.0137,  0.0102, -0.0232],\n",
            "        [ 0.0063, -0.0226,  0.0143,  ...,  0.0224,  0.0192, -0.0216],\n",
            "        [ 0.0098,  0.0089, -0.0162,  ...,  0.0194,  0.0256, -0.0127]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.lora_down.weight', tensor([[ 0.0082,  0.0068,  0.0046,  ..., -0.0134,  0.0067, -0.0088],\n",
            "        [ 0.0034,  0.0131,  0.0049,  ..., -0.0084, -0.0054,  0.0035],\n",
            "        [ 0.0082,  0.0098, -0.0091,  ..., -0.0003,  0.0011, -0.0002],\n",
            "        ...,\n",
            "        [-0.0116, -0.0055,  0.0082,  ...,  0.0112,  0.0053, -0.0090],\n",
            "        [-0.0041, -0.0126, -0.0125,  ..., -0.0047, -0.0111, -0.0033],\n",
            "        [ 0.0026, -0.0135,  0.0124,  ..., -0.0068, -0.0059, -0.0023]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.lora_down.weight', tensor([[ 0.0102, -0.0138, -0.0127,  ..., -0.0016, -0.0046, -0.0128],\n",
            "        [-0.0182,  0.0116, -0.0175,  ..., -0.0252,  0.0115,  0.0261],\n",
            "        [-0.0249, -0.0242, -0.0085,  ..., -0.0101, -0.0221, -0.0104],\n",
            "        ...,\n",
            "        [ 0.0141,  0.0103,  0.0134,  ...,  0.0207,  0.0139, -0.0068],\n",
            "        [-0.0012,  0.0228,  0.0205,  ..., -0.0245, -0.0058, -0.0259],\n",
            "        [-0.0272, -0.0138,  0.0178,  ...,  0.0124,  0.0271,  0.0104]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight', tensor([[ 0.0155,  0.0235, -0.0066,  ...,  0.0153,  0.0202, -0.0019],\n",
            "        [-0.0130, -0.0128, -0.0173,  ...,  0.0067,  0.0157, -0.0076],\n",
            "        [-0.0222, -0.0177, -0.0116,  ..., -0.0124, -0.0116,  0.0127],\n",
            "        ...,\n",
            "        [-0.0187, -0.0105,  0.0147,  ..., -0.0178, -0.0003, -0.0150],\n",
            "        [-0.0168,  0.0206,  0.0023,  ...,  0.0017,  0.0019, -0.0035],\n",
            "        [ 0.0077, -0.0037,  0.0034,  ..., -0.0059,  0.0138, -0.0265]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.lora_down.weight', tensor([[ 0.0075,  0.0198, -0.0236,  ..., -0.0092,  0.0225, -0.0276],\n",
            "        [-0.0105,  0.0149, -0.0171,  ..., -0.0268, -0.0089, -0.0174],\n",
            "        [-0.0160,  0.0163,  0.0004,  ..., -0.0215,  0.0009, -0.0264],\n",
            "        ...,\n",
            "        [ 0.0269, -0.0079,  0.0136,  ...,  0.0006, -0.0018, -0.0190],\n",
            "        [-0.0171, -0.0042, -0.0264,  ...,  0.0076, -0.0235, -0.0188],\n",
            "        [ 0.0112, -0.0265,  0.0170,  ..., -0.0256, -0.0026, -0.0042]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.lora_down.weight', tensor([[ 0.0128, -0.0193,  0.0203,  ...,  0.0253,  0.0269, -0.0020],\n",
            "        [ 0.0113,  0.0251, -0.0168,  ...,  0.0126,  0.0014,  0.0093],\n",
            "        [ 0.0132,  0.0043, -0.0266,  ...,  0.0020, -0.0164,  0.0277],\n",
            "        ...,\n",
            "        [-0.0190, -0.0156,  0.0008,  ...,  0.0228,  0.0103,  0.0164],\n",
            "        [-0.0121,  0.0137, -0.0048,  ..., -0.0012,  0.0075,  0.0219],\n",
            "        [ 0.0220, -0.0276,  0.0215,  ...,  0.0097, -0.0142, -0.0108]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.lora_down.weight', tensor([[-3.1204e-03, -1.1711e-02,  8.4991e-03,  ...,  1.5366e-02,\n",
            "         -2.0859e-02,  1.1551e-02],\n",
            "        [ 4.9515e-03, -1.9779e-03, -5.0187e-05,  ..., -1.7838e-02,\n",
            "          1.9226e-02,  1.1650e-02],\n",
            "        [ 1.7700e-02, -1.9501e-02,  1.7990e-02,  ..., -1.3847e-03,\n",
            "         -2.9888e-03, -1.7136e-02],\n",
            "        ...,\n",
            "        [ 1.8753e-02,  2.5673e-03, -1.6190e-02,  ..., -1.0078e-02,\n",
            "         -1.5930e-02,  1.7365e-02],\n",
            "        [-9.9599e-05,  1.7380e-02,  8.4763e-03,  ..., -2.1698e-02,\n",
            "          7.4120e-03, -2.5120e-03],\n",
            "        [ 1.4435e-02, -7.0534e-03, -3.4447e-03,  ...,  1.7914e-02,\n",
            "          1.8339e-03,  3.5572e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight', tensor([[-0.0189, -0.0275,  0.0234,  ...,  0.0145,  0.0002,  0.0129],\n",
            "        [-0.0185, -0.0059, -0.0116,  ...,  0.0150,  0.0101,  0.0060],\n",
            "        [-0.0208, -0.0077, -0.0007,  ...,  0.0201,  0.0221, -0.0032],\n",
            "        ...,\n",
            "        [-0.0132, -0.0112, -0.0228,  ..., -0.0191, -0.0259,  0.0233],\n",
            "        [ 0.0124,  0.0145, -0.0147,  ..., -0.0190, -0.0116, -0.0097],\n",
            "        [-0.0226,  0.0211,  0.0225,  ..., -0.0068,  0.0023, -0.0161]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.lora_down.weight', tensor([[-0.0120,  0.0184, -0.0178,  ..., -0.0062, -0.0108, -0.0175],\n",
            "        [-0.0043,  0.0025, -0.0015,  ...,  0.0245, -0.0206,  0.0227],\n",
            "        [ 0.0268,  0.0152, -0.0270,  ...,  0.0030, -0.0137, -0.0077],\n",
            "        ...,\n",
            "        [ 0.0120,  0.0128,  0.0175,  ...,  0.0143, -0.0173,  0.0054],\n",
            "        [-0.0219, -0.0049,  0.0157,  ..., -0.0007,  0.0235, -0.0274],\n",
            "        [ 0.0161,  0.0263, -0.0132,  ..., -0.0262,  0.0247, -0.0263]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.lora_down.weight', tensor([[-0.0046, -0.0109,  0.0022,  ...,  0.0019,  0.0144, -0.0088],\n",
            "        [-0.0011, -0.0056,  0.0076,  ...,  0.0002,  0.0216, -0.0121],\n",
            "        [-0.0073,  0.0156,  0.0042,  ...,  0.0104,  0.0148, -0.0143],\n",
            "        ...,\n",
            "        [-0.0170, -0.0023,  0.0196,  ..., -0.0098,  0.0035, -0.0181],\n",
            "        [ 0.0186,  0.0025, -0.0151,  ..., -0.0090, -0.0012,  0.0165],\n",
            "        [-0.0213,  0.0051, -0.0172,  ..., -0.0202,  0.0050,  0.0005]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight', tensor([[ 0.0040,  0.0192, -0.0114,  ..., -0.0106, -0.0093, -0.0279],\n",
            "        [-0.0003, -0.0043, -0.0230,  ..., -0.0136, -0.0033,  0.0028],\n",
            "        [ 0.0025, -0.0189,  0.0019,  ...,  0.0090,  0.0241,  0.0031],\n",
            "        ...,\n",
            "        [-0.0215,  0.0088,  0.0273,  ...,  0.0136, -0.0196,  0.0063],\n",
            "        [ 0.0003,  0.0060, -0.0013,  ..., -0.0009, -0.0186,  0.0240],\n",
            "        [ 0.0083,  0.0062,  0.0006,  ..., -0.0162,  0.0132,  0.0263]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.lora_down.weight', tensor([[ 0.0133, -0.0126,  0.0070,  ..., -0.0083, -0.0038, -0.0022],\n",
            "        [-0.0042,  0.0134,  0.0064,  ...,  0.0080, -0.0052, -0.0033],\n",
            "        [ 0.0055, -0.0082, -0.0087,  ..., -0.0131, -0.0058, -0.0027],\n",
            "        ...,\n",
            "        [ 0.0059, -0.0023, -0.0014,  ..., -0.0122, -0.0138, -0.0111],\n",
            "        [ 0.0133, -0.0087, -0.0071,  ..., -0.0036,  0.0026,  0.0121],\n",
            "        [-0.0030,  0.0062, -0.0026,  ...,  0.0070, -0.0012, -0.0082]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.lora_down.weight', tensor([[-0.0025,  0.0139, -0.0157,  ...,  0.0150, -0.0048, -0.0264],\n",
            "        [ 0.0263,  0.0157, -0.0049,  ...,  0.0080,  0.0244,  0.0173],\n",
            "        [-0.0090,  0.0053,  0.0124,  ...,  0.0223,  0.0196,  0.0206],\n",
            "        ...,\n",
            "        [ 0.0271,  0.0049, -0.0005,  ...,  0.0143, -0.0238, -0.0242],\n",
            "        [ 0.0084,  0.0143,  0.0065,  ...,  0.0216,  0.0036, -0.0014],\n",
            "        [ 0.0038,  0.0058,  0.0148,  ...,  0.0017,  0.0111, -0.0272]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight', tensor([[ 0.0013, -0.0138,  0.0170,  ..., -0.0149, -0.0256, -0.0094],\n",
            "        [ 0.0103,  0.0098,  0.0057,  ...,  0.0004,  0.0215, -0.0038],\n",
            "        [ 0.0020,  0.0126,  0.0034,  ...,  0.0096, -0.0037,  0.0179],\n",
            "        ...,\n",
            "        [-0.0211,  0.0139, -0.0218,  ..., -0.0091, -0.0152, -0.0066],\n",
            "        [-0.0038, -0.0106,  0.0085,  ..., -0.0025,  0.0187,  0.0263],\n",
            "        [ 0.0234,  0.0196,  0.0045,  ...,  0.0013, -0.0161, -0.0180]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.lora_down.weight', tensor([[-0.0193, -0.0193, -0.0201,  ..., -0.0165,  0.0072, -0.0211],\n",
            "        [-0.0141, -0.0166,  0.0231,  ..., -0.0143, -0.0231,  0.0276],\n",
            "        [ 0.0054, -0.0006, -0.0147,  ...,  0.0211, -0.0013, -0.0034],\n",
            "        ...,\n",
            "        [-0.0228,  0.0271, -0.0065,  ..., -0.0135,  0.0017,  0.0005],\n",
            "        [ 0.0150,  0.0162, -0.0053,  ..., -0.0036, -0.0066,  0.0041],\n",
            "        [-0.0215, -0.0101,  0.0195,  ..., -0.0236, -0.0080,  0.0144]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.lora_down.weight', tensor([[ 0.0040, -0.0216, -0.0183,  ...,  0.0197, -0.0112, -0.0126],\n",
            "        [ 0.0109, -0.0234, -0.0008,  ...,  0.0020,  0.0262, -0.0067],\n",
            "        [ 0.0095, -0.0208,  0.0174,  ..., -0.0072, -0.0214, -0.0052],\n",
            "        ...,\n",
            "        [ 0.0195,  0.0081,  0.0210,  ..., -0.0203, -0.0061,  0.0150],\n",
            "        [-0.0201,  0.0226,  0.0127,  ...,  0.0114, -0.0008, -0.0270],\n",
            "        [ 0.0111,  0.0208, -0.0022,  ...,  0.0224,  0.0112, -0.0267]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.lora_down.weight', tensor([[ 0.0200,  0.0080,  0.0078,  ..., -0.0088, -0.0164,  0.0150],\n",
            "        [ 0.0133, -0.0024,  0.0213,  ..., -0.0036, -0.0104,  0.0106],\n",
            "        [ 0.0183,  0.0115, -0.0167,  ..., -0.0173, -0.0054,  0.0137],\n",
            "        ...,\n",
            "        [ 0.0027,  0.0203, -0.0196,  ..., -0.0207, -0.0097, -0.0141],\n",
            "        [ 0.0068, -0.0204, -0.0128,  ..., -0.0149,  0.0026, -0.0163],\n",
            "        [-0.0092, -0.0083,  0.0147,  ...,  0.0027,  0.0130,  0.0221]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight', tensor([[ 0.0274,  0.0097, -0.0189,  ...,  0.0199,  0.0077, -0.0088],\n",
            "        [-0.0067, -0.0277,  0.0181,  ...,  0.0158, -0.0234, -0.0244],\n",
            "        [ 0.0105, -0.0077, -0.0168,  ..., -0.0189, -0.0126, -0.0165],\n",
            "        ...,\n",
            "        [-0.0023, -0.0051, -0.0229,  ..., -0.0253, -0.0112,  0.0252],\n",
            "        [-0.0078, -0.0135,  0.0044,  ...,  0.0147,  0.0219,  0.0044],\n",
            "        [-0.0082, -0.0133,  0.0177,  ..., -0.0107,  0.0024,  0.0199]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.lora_down.weight', tensor([[-0.0156,  0.0014,  0.0119,  ...,  0.0152, -0.0004, -0.0066],\n",
            "        [-0.0066, -0.0267, -0.0210,  ..., -0.0039, -0.0070,  0.0224],\n",
            "        [ 0.0254, -0.0104,  0.0139,  ...,  0.0160,  0.0092,  0.0107],\n",
            "        ...,\n",
            "        [ 0.0005, -0.0165,  0.0053,  ...,  0.0030, -0.0222, -0.0160],\n",
            "        [ 0.0104, -0.0222,  0.0261,  ..., -0.0148,  0.0268, -0.0173],\n",
            "        [ 0.0021, -0.0045,  0.0058,  ...,  0.0190,  0.0260, -0.0261]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.lora_down.weight', tensor([[ 1.1292e-02, -1.5129e-02,  1.1658e-02,  ..., -1.5915e-02,\n",
            "          2.1652e-02, -1.4130e-02],\n",
            "        [ 3.3426e-04,  4.4823e-03, -1.1101e-02,  ...,  1.7792e-02,\n",
            "         -9.3079e-04,  2.0645e-02],\n",
            "        [ 9.8114e-03, -3.6850e-03, -8.9228e-05,  ..., -6.9046e-03,\n",
            "          7.5150e-03, -2.1040e-04],\n",
            "        ...,\n",
            "        [ 1.5091e-02, -4.8027e-03, -1.7120e-02,  ..., -4.9925e-04,\n",
            "          1.9608e-02, -1.2054e-02],\n",
            "        [-2.0645e-02,  4.3793e-03,  2.2034e-02,  ..., -1.5007e-02,\n",
            "          2.1133e-02, -2.6379e-03],\n",
            "        [-1.0994e-02, -2.0279e-02, -2.6512e-03,  ...,  4.5128e-03,\n",
            "         -4.6234e-03, -1.7357e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight', tensor([[-0.0039, -0.0229, -0.0040,  ...,  0.0086,  0.0144, -0.0074],\n",
            "        [ 0.0206,  0.0016,  0.0228,  ...,  0.0197, -0.0252, -0.0172],\n",
            "        [ 0.0041,  0.0161, -0.0050,  ..., -0.0159,  0.0061, -0.0166],\n",
            "        ...,\n",
            "        [ 0.0268, -0.0254, -0.0096,  ...,  0.0016,  0.0155, -0.0054],\n",
            "        [-0.0232, -0.0085, -0.0245,  ...,  0.0101,  0.0172, -0.0215],\n",
            "        [ 0.0152,  0.0106,  0.0135,  ...,  0.0135, -0.0013,  0.0133]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.lora_down.weight', tensor([[-0.0077,  0.0105, -0.0075,  ...,  0.0102,  0.0048,  0.0029],\n",
            "        [ 0.0054,  0.0113, -0.0093,  ..., -0.0124,  0.0129, -0.0112],\n",
            "        [-0.0059, -0.0052,  0.0009,  ..., -0.0043,  0.0097, -0.0022],\n",
            "        ...,\n",
            "        [-0.0064,  0.0009, -0.0062,  ...,  0.0011,  0.0053,  0.0085],\n",
            "        [-0.0044,  0.0109, -0.0039,  ...,  0.0036, -0.0094, -0.0090],\n",
            "        [-0.0023,  0.0061,  0.0008,  ..., -0.0105,  0.0126, -0.0131]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.lora_down.weight', tensor([[ 0.0145, -0.0077,  0.0259,  ..., -0.0168,  0.0052, -0.0238],\n",
            "        [ 0.0059, -0.0094, -0.0184,  ..., -0.0223,  0.0112, -0.0027],\n",
            "        [ 0.0161,  0.0058,  0.0042,  ...,  0.0233,  0.0242, -0.0065],\n",
            "        ...,\n",
            "        [-0.0081,  0.0268,  0.0229,  ..., -0.0273,  0.0177, -0.0175],\n",
            "        [ 0.0097,  0.0075,  0.0163,  ...,  0.0135,  0.0088,  0.0152],\n",
            "        [ 0.0199,  0.0060,  0.0201,  ..., -0.0206, -0.0022, -0.0026]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight', tensor([[-0.0116, -0.0012, -0.0277,  ...,  0.0067, -0.0128, -0.0184],\n",
            "        [-0.0007,  0.0136,  0.0189,  ...,  0.0022, -0.0090, -0.0251],\n",
            "        [ 0.0008,  0.0148,  0.0271,  ..., -0.0078, -0.0235, -0.0272],\n",
            "        ...,\n",
            "        [ 0.0082,  0.0267,  0.0229,  ...,  0.0054, -0.0084,  0.0223],\n",
            "        [-0.0152, -0.0232,  0.0119,  ..., -0.0081,  0.0134,  0.0057],\n",
            "        [-0.0113, -0.0053,  0.0133,  ...,  0.0095, -0.0155, -0.0195]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.lora_down.weight', tensor([[ 1.2863e-02,  5.3062e-03,  4.1199e-03,  ...,  2.7206e-02,\n",
            "         -9.7961e-03, -2.9697e-03],\n",
            "        [ 9.6741e-03, -2.3071e-02,  2.4445e-02,  ..., -1.6800e-02,\n",
            "         -1.7349e-02, -9.1858e-03],\n",
            "        [-7.3776e-03, -2.1637e-02,  1.6083e-02,  ..., -7.9880e-03,\n",
            "         -1.2115e-02, -1.1963e-02],\n",
            "        ...,\n",
            "        [-7.6561e-03, -6.7253e-03, -1.8326e-02,  ..., -5.7487e-03,\n",
            "         -7.4120e-03,  1.5114e-02],\n",
            "        [-1.3008e-02, -8.4579e-05, -2.6352e-02,  ..., -8.9951e-03,\n",
            "          1.1086e-02,  2.5208e-02],\n",
            "        [ 1.5137e-02,  9.7961e-03, -2.3529e-02,  ..., -1.2566e-02,\n",
            "          2.2079e-02,  5.4979e-04]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.lora_down.weight', tensor([[ 0.0161, -0.0144,  0.0182,  ..., -0.0266, -0.0185, -0.0093],\n",
            "        [-0.0271,  0.0076, -0.0122,  ..., -0.0077,  0.0010, -0.0175],\n",
            "        [-0.0170,  0.0083,  0.0208,  ...,  0.0178,  0.0058,  0.0102],\n",
            "        ...,\n",
            "        [ 0.0108,  0.0260, -0.0237,  ..., -0.0136, -0.0022, -0.0184],\n",
            "        [-0.0011, -0.0116, -0.0136,  ..., -0.0209, -0.0161,  0.0192],\n",
            "        [-0.0082, -0.0243, -0.0172,  ...,  0.0118, -0.0004, -0.0145]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.lora_down.weight', tensor([[-0.0016, -0.0082, -0.0121,  ...,  0.0177, -0.0099, -0.0214],\n",
            "        [ 0.0011,  0.0125,  0.0117,  ...,  0.0219,  0.0208, -0.0053],\n",
            "        [ 0.0078, -0.0141,  0.0220,  ...,  0.0190,  0.0142, -0.0204],\n",
            "        ...,\n",
            "        [-0.0020,  0.0043, -0.0176,  ..., -0.0211, -0.0019,  0.0066],\n",
            "        [ 0.0166, -0.0184,  0.0133,  ..., -0.0138,  0.0154, -0.0217],\n",
            "        [ 0.0204,  0.0165,  0.0014,  ..., -0.0136, -0.0026,  0.0200]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight', tensor([[ 2.7466e-02,  7.1487e-03,  6.6223e-03,  ...,  2.2049e-03,\n",
            "         -1.7181e-02,  1.1391e-02],\n",
            "        [ 1.8631e-02, -2.0096e-02, -5.0259e-04,  ..., -1.4391e-03,\n",
            "          2.2068e-03,  1.4793e-02],\n",
            "        [ 2.5803e-02,  1.4847e-02,  2.7893e-02,  ..., -5.1804e-03,\n",
            "         -1.2985e-02,  8.9035e-03],\n",
            "        ...,\n",
            "        [-3.1013e-03, -2.2156e-02,  6.2346e-05,  ...,  1.3954e-02,\n",
            "          1.2611e-02, -2.7328e-02],\n",
            "        [-5.3711e-03, -2.3621e-02,  1.5671e-02,  ...,  1.0658e-02,\n",
            "         -1.8341e-02, -2.4338e-02],\n",
            "        [-4.6997e-03,  2.2720e-02, -1.2314e-02,  ..., -1.4008e-02,\n",
            "         -6.9923e-03, -9.2010e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.lora_down.weight', tensor([[-0.0251, -0.0197,  0.0059,  ...,  0.0167,  0.0229,  0.0152],\n",
            "        [-0.0055, -0.0015,  0.0248,  ..., -0.0119,  0.0206,  0.0117],\n",
            "        [-0.0042,  0.0135, -0.0255,  ...,  0.0089,  0.0175, -0.0153],\n",
            "        ...,\n",
            "        [-0.0216,  0.0055,  0.0238,  ...,  0.0279,  0.0219,  0.0025],\n",
            "        [-0.0063, -0.0239,  0.0050,  ..., -0.0245, -0.0238,  0.0186],\n",
            "        [ 0.0071,  0.0279, -0.0053,  ..., -0.0070, -0.0120,  0.0097]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.lora_down.weight', tensor([[ 0.0019,  0.0025,  0.0007,  ..., -0.0178,  0.0208, -0.0069],\n",
            "        [-0.0028, -0.0075,  0.0146,  ..., -0.0092, -0.0102,  0.0019],\n",
            "        [ 0.0148, -0.0038, -0.0073,  ..., -0.0033,  0.0130, -0.0043],\n",
            "        ...,\n",
            "        [ 0.0028, -0.0134,  0.0069,  ..., -0.0037,  0.0175, -0.0022],\n",
            "        [-0.0111,  0.0142, -0.0112,  ...,  0.0156, -0.0129, -0.0137],\n",
            "        [ 0.0127, -0.0053,  0.0070,  ...,  0.0129, -0.0045,  0.0077]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight', tensor([[-0.0055,  0.0208,  0.0165,  ...,  0.0031,  0.0224, -0.0199],\n",
            "        [-0.0276, -0.0066,  0.0109,  ...,  0.0083, -0.0267,  0.0242],\n",
            "        [-0.0219,  0.0071,  0.0277,  ..., -0.0192, -0.0112, -0.0082],\n",
            "        ...,\n",
            "        [ 0.0013,  0.0274, -0.0189,  ...,  0.0024,  0.0137,  0.0176],\n",
            "        [ 0.0033, -0.0186,  0.0139,  ...,  0.0002,  0.0051, -0.0189],\n",
            "        [ 0.0035,  0.0160, -0.0197,  ...,  0.0226, -0.0155, -0.0047]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.lora_down.weight', tensor([[ 0.0042,  0.0087, -0.0093,  ...,  0.0005, -0.0063,  0.0033],\n",
            "        [-0.0055, -0.0051, -0.0076,  ...,  0.0029, -0.0030, -0.0065],\n",
            "        [ 0.0008,  0.0003,  0.0102,  ...,  0.0125, -0.0019,  0.0054],\n",
            "        ...,\n",
            "        [ 0.0113,  0.0100,  0.0111,  ...,  0.0083,  0.0135,  0.0015],\n",
            "        [ 0.0054,  0.0077, -0.0073,  ...,  0.0085,  0.0113,  0.0088],\n",
            "        [-0.0025,  0.0065, -0.0003,  ..., -0.0079,  0.0029,  0.0124]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.lora_down.weight', tensor([[ 0.0232, -0.0087,  0.0141,  ...,  0.0087, -0.0162, -0.0073],\n",
            "        [ 0.0270,  0.0112, -0.0083,  ...,  0.0168,  0.0236, -0.0200],\n",
            "        [ 0.0228, -0.0094, -0.0157,  ...,  0.0155, -0.0070,  0.0114],\n",
            "        ...,\n",
            "        [ 0.0068,  0.0221,  0.0047,  ...,  0.0147,  0.0237, -0.0160],\n",
            "        [-0.0260,  0.0229,  0.0261,  ...,  0.0109,  0.0063, -0.0195],\n",
            "        [-0.0057,  0.0183,  0.0170,  ..., -0.0233, -0.0271, -0.0156]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight', tensor([[-0.0230,  0.0225,  0.0263,  ..., -0.0099,  0.0234, -0.0059],\n",
            "        [ 0.0045,  0.0229,  0.0263,  ...,  0.0025, -0.0190, -0.0023],\n",
            "        [-0.0081, -0.0138, -0.0234,  ..., -0.0027, -0.0150, -0.0128],\n",
            "        ...,\n",
            "        [-0.0230,  0.0210, -0.0243,  ..., -0.0028, -0.0276, -0.0250],\n",
            "        [ 0.0001,  0.0130,  0.0124,  ...,  0.0033,  0.0180, -0.0096],\n",
            "        [ 0.0003,  0.0143, -0.0251,  ...,  0.0054,  0.0085,  0.0242]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.lora_down.weight', tensor([[-0.0149, -0.0089,  0.0081,  ..., -0.0024,  0.0047,  0.0219],\n",
            "        [-0.0174,  0.0155, -0.0078,  ..., -0.0056, -0.0185,  0.0073],\n",
            "        [ 0.0242,  0.0090,  0.0071,  ..., -0.0232, -0.0186, -0.0060],\n",
            "        ...,\n",
            "        [-0.0117,  0.0069, -0.0130,  ..., -0.0230,  0.0116,  0.0254],\n",
            "        [-0.0157,  0.0142, -0.0166,  ...,  0.0176,  0.0061, -0.0242],\n",
            "        [ 0.0040,  0.0232, -0.0104,  ...,  0.0031, -0.0114,  0.0163]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.lora_down.weight', tensor([[-0.0008, -0.0164,  0.0269,  ...,  0.0232, -0.0217,  0.0026],\n",
            "        [ 0.0169, -0.0242,  0.0230,  ..., -0.0199,  0.0155, -0.0179],\n",
            "        [-0.0040,  0.0021, -0.0027,  ...,  0.0275, -0.0004, -0.0230],\n",
            "        ...,\n",
            "        [-0.0277, -0.0209,  0.0223,  ..., -0.0210, -0.0050,  0.0114],\n",
            "        [-0.0227, -0.0085,  0.0043,  ..., -0.0215,  0.0058,  0.0118],\n",
            "        [-0.0158,  0.0204,  0.0009,  ..., -0.0260,  0.0044,  0.0183]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.lora_down.weight', tensor([[ 1.2367e-02,  4.8370e-03, -1.8433e-02,  ..., -1.1223e-02,\n",
            "         -7.4692e-03,  1.5289e-02],\n",
            "        [-6.1035e-05,  6.0616e-03, -4.5090e-03,  ..., -1.4122e-02,\n",
            "         -1.6113e-02, -1.0178e-02],\n",
            "        [ 6.6376e-03, -2.1088e-02,  1.3817e-02,  ..., -1.6525e-02,\n",
            "         -1.5579e-02,  4.1962e-03],\n",
            "        ...,\n",
            "        [-2.9964e-03,  1.1696e-02, -2.0401e-02,  ..., -2.0523e-02,\n",
            "         -1.2451e-02, -7.8583e-03],\n",
            "        [-2.0004e-02, -1.6815e-02,  9.7418e-04,  ...,  1.6113e-02,\n",
            "          1.6006e-02, -1.7796e-03],\n",
            "        [ 2.1072e-02,  1.1322e-02,  1.6785e-02,  ..., -1.7014e-02,\n",
            "          1.2993e-02,  1.2085e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight', tensor([[ 0.0146, -0.0189,  0.0006,  ...,  0.0040, -0.0151,  0.0162],\n",
            "        [-0.0210, -0.0201, -0.0082,  ..., -0.0196, -0.0132,  0.0227],\n",
            "        [-0.0009,  0.0229,  0.0140,  ..., -0.0242, -0.0096, -0.0249],\n",
            "        ...,\n",
            "        [-0.0209,  0.0105,  0.0112,  ...,  0.0187,  0.0275, -0.0090],\n",
            "        [ 0.0253, -0.0007,  0.0087,  ..., -0.0149,  0.0066, -0.0229],\n",
            "        [-0.0269,  0.0131, -0.0216,  ...,  0.0073,  0.0089,  0.0239]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.lora_down.weight', tensor([[-0.0194, -0.0210, -0.0276,  ..., -0.0095,  0.0056, -0.0090],\n",
            "        [ 0.0229,  0.0168, -0.0249,  ..., -0.0248, -0.0113, -0.0179],\n",
            "        [-0.0062,  0.0006, -0.0046,  ..., -0.0059,  0.0048, -0.0022],\n",
            "        ...,\n",
            "        [-0.0139,  0.0139,  0.0258,  ..., -0.0192,  0.0204, -0.0130],\n",
            "        [ 0.0205, -0.0183, -0.0230,  ...,  0.0231, -0.0186,  0.0255],\n",
            "        [-0.0143, -0.0168, -0.0071,  ..., -0.0135,  0.0038, -0.0090]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.lora_down.weight', tensor([[ 0.0064,  0.0014,  0.0190,  ...,  0.0022, -0.0144,  0.0098],\n",
            "        [ 0.0123, -0.0198, -0.0212,  ..., -0.0121,  0.0110, -0.0189],\n",
            "        [ 0.0184, -0.0029,  0.0197,  ...,  0.0202,  0.0043,  0.0079],\n",
            "        ...,\n",
            "        [ 0.0150, -0.0044, -0.0153,  ..., -0.0033,  0.0192, -0.0217],\n",
            "        [-0.0045, -0.0164,  0.0163,  ...,  0.0070,  0.0133, -0.0134],\n",
            "        [-0.0078, -0.0003, -0.0029,  ...,  0.0163,  0.0212,  0.0170]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight', tensor([[-0.0132,  0.0086, -0.0049,  ...,  0.0222, -0.0275,  0.0028],\n",
            "        [-0.0003, -0.0156, -0.0016,  ..., -0.0087, -0.0041,  0.0157],\n",
            "        [ 0.0233,  0.0171, -0.0184,  ..., -0.0036,  0.0155, -0.0180],\n",
            "        ...,\n",
            "        [-0.0172, -0.0154, -0.0080,  ..., -0.0139, -0.0090, -0.0181],\n",
            "        [ 0.0054,  0.0248, -0.0182,  ..., -0.0247,  0.0242, -0.0117],\n",
            "        [ 0.0227, -0.0188, -0.0269,  ...,  0.0208,  0.0126,  0.0268]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.lora_down.weight', tensor([[ 0.0069, -0.0059,  0.0076,  ...,  0.0104,  0.0115, -0.0044],\n",
            "        [-0.0052,  0.0070, -0.0027,  ..., -0.0049, -0.0015,  0.0072],\n",
            "        [-0.0114, -0.0107,  0.0111,  ..., -0.0113, -0.0023, -0.0109],\n",
            "        ...,\n",
            "        [-0.0057,  0.0086,  0.0099,  ..., -0.0038, -0.0099, -0.0130],\n",
            "        [-0.0034,  0.0100, -0.0115,  ...,  0.0070, -0.0039,  0.0079],\n",
            "        [ 0.0055,  0.0029,  0.0118,  ...,  0.0058, -0.0071,  0.0135]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.lora_down.weight', tensor([[ 0.0206, -0.0123,  0.0202,  ..., -0.0217, -0.0094, -0.0217],\n",
            "        [ 0.0044, -0.0084, -0.0142,  ..., -0.0036,  0.0083, -0.0128],\n",
            "        [-0.0159, -0.0229, -0.0254,  ...,  0.0011,  0.0187,  0.0057],\n",
            "        ...,\n",
            "        [-0.0123, -0.0051,  0.0211,  ..., -0.0262, -0.0028,  0.0195],\n",
            "        [ 0.0013,  0.0095,  0.0056,  ...,  0.0019,  0.0037, -0.0011],\n",
            "        [ 0.0051,  0.0255,  0.0172,  ...,  0.0046,  0.0271,  0.0063]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight', tensor([[ 0.0182,  0.0210,  0.0222,  ..., -0.0020, -0.0082,  0.0076],\n",
            "        [-0.0111,  0.0017, -0.0018,  ..., -0.0130, -0.0106, -0.0081],\n",
            "        [ 0.0133,  0.0086, -0.0121,  ...,  0.0109, -0.0099,  0.0071],\n",
            "        ...,\n",
            "        [ 0.0025,  0.0209,  0.0052,  ...,  0.0141,  0.0241, -0.0190],\n",
            "        [ 0.0222,  0.0053, -0.0224,  ..., -0.0235,  0.0083,  0.0224],\n",
            "        [ 0.0205, -0.0129, -0.0234,  ...,  0.0124, -0.0230, -0.0145]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.lora_down.weight', tensor([[-0.0270,  0.0130,  0.0173,  ..., -0.0069,  0.0279,  0.0160],\n",
            "        [ 0.0216, -0.0136,  0.0043,  ..., -0.0137, -0.0101,  0.0111],\n",
            "        [ 0.0199,  0.0141, -0.0222,  ...,  0.0073, -0.0024,  0.0141],\n",
            "        ...,\n",
            "        [-0.0084, -0.0002,  0.0194,  ...,  0.0255,  0.0075, -0.0051],\n",
            "        [-0.0257, -0.0233,  0.0252,  ..., -0.0053, -0.0166, -0.0008],\n",
            "        [ 0.0220, -0.0082,  0.0028,  ...,  0.0182, -0.0084, -0.0078]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.lora_down.weight', tensor([[-0.0156, -0.0115,  0.0108,  ..., -0.0262, -0.0155,  0.0195],\n",
            "        [-0.0150,  0.0062,  0.0116,  ..., -0.0073, -0.0170, -0.0132],\n",
            "        [ 0.0127,  0.0151, -0.0156,  ...,  0.0243,  0.0147,  0.0195],\n",
            "        ...,\n",
            "        [ 0.0240, -0.0031, -0.0154,  ..., -0.0268, -0.0120, -0.0102],\n",
            "        [-0.0238, -0.0070,  0.0204,  ..., -0.0260, -0.0200, -0.0076],\n",
            "        [-0.0278, -0.0071,  0.0209,  ...,  0.0258, -0.0115,  0.0212]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.lora_down.weight', tensor([[ 0.0055,  0.0190, -0.0042,  ..., -0.0057, -0.0124, -0.0175],\n",
            "        [-0.0016, -0.0151,  0.0055,  ..., -0.0071,  0.0049, -0.0071],\n",
            "        [-0.0178,  0.0032, -0.0191,  ...,  0.0051,  0.0067, -0.0003],\n",
            "        ...,\n",
            "        [-0.0149,  0.0143,  0.0190,  ..., -0.0051, -0.0168,  0.0066],\n",
            "        [-0.0071,  0.0132, -0.0190,  ..., -0.0178,  0.0081,  0.0209],\n",
            "        [ 0.0115,  0.0097,  0.0099,  ..., -0.0055, -0.0138, -0.0198]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight', tensor([[ 0.0205,  0.0210,  0.0254,  ...,  0.0137,  0.0030, -0.0078],\n",
            "        [ 0.0277, -0.0147, -0.0187,  ..., -0.0163, -0.0005,  0.0013],\n",
            "        [ 0.0183, -0.0100,  0.0263,  ..., -0.0272,  0.0111,  0.0208],\n",
            "        ...,\n",
            "        [ 0.0064, -0.0043,  0.0053,  ...,  0.0273, -0.0012, -0.0022],\n",
            "        [ 0.0246,  0.0232,  0.0185,  ..., -0.0133, -0.0162,  0.0063],\n",
            "        [ 0.0279,  0.0208,  0.0229,  ..., -0.0140, -0.0135, -0.0197]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.lora_down.weight', tensor([[ 0.0003, -0.0067,  0.0193,  ...,  0.0074, -0.0193, -0.0159],\n",
            "        [-0.0159,  0.0147,  0.0155,  ...,  0.0008,  0.0145, -0.0098],\n",
            "        [-0.0120,  0.0051, -0.0056,  ..., -0.0131,  0.0039,  0.0125],\n",
            "        ...,\n",
            "        [ 0.0222,  0.0129,  0.0211,  ...,  0.0034, -0.0179,  0.0202],\n",
            "        [ 0.0205,  0.0140, -0.0040,  ..., -0.0047,  0.0139, -0.0131],\n",
            "        [-0.0223,  0.0013,  0.0263,  ...,  0.0046, -0.0038, -0.0250]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.lora_down.weight', tensor([[-0.0178,  0.0028,  0.0113,  ...,  0.0047, -0.0085, -0.0219],\n",
            "        [-0.0166,  0.0160,  0.0206,  ..., -0.0021,  0.0160,  0.0071],\n",
            "        [ 0.0177,  0.0106, -0.0211,  ..., -0.0092, -0.0221, -0.0052],\n",
            "        ...,\n",
            "        [-0.0134, -0.0144, -0.0115,  ...,  0.0153,  0.0027, -0.0123],\n",
            "        [ 0.0086, -0.0204, -0.0133,  ...,  0.0062, -0.0040, -0.0001],\n",
            "        [ 0.0149,  0.0131, -0.0127,  ..., -0.0160,  0.0173, -0.0185]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight', tensor([[ 0.0032,  0.0127,  0.0253,  ...,  0.0170, -0.0141, -0.0182],\n",
            "        [ 0.0197, -0.0082, -0.0095,  ..., -0.0080, -0.0211,  0.0134],\n",
            "        [-0.0140,  0.0009,  0.0053,  ...,  0.0088, -0.0053,  0.0137],\n",
            "        ...,\n",
            "        [-0.0213, -0.0211, -0.0050,  ...,  0.0191, -0.0189, -0.0083],\n",
            "        [ 0.0114, -0.0086,  0.0055,  ..., -0.0168, -0.0183,  0.0257],\n",
            "        [-0.0249,  0.0115, -0.0159,  ...,  0.0142, -0.0198,  0.0045]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.lora_down.weight', tensor([[-1.2884e-03,  1.4791e-03,  3.9749e-03,  ..., -1.8466e-04,\n",
            "         -5.5466e-03, -2.3270e-03],\n",
            "        [ 1.2238e-02, -1.3390e-02,  1.2253e-02,  ...,  1.3332e-03,\n",
            "          3.6583e-03, -2.8038e-03],\n",
            "        [ 1.1887e-02,  1.2955e-02,  4.6043e-03,  ..., -6.6948e-03,\n",
            "          2.1725e-03,  1.1421e-02],\n",
            "        ...,\n",
            "        [-4.6806e-03,  1.1620e-02, -2.5215e-03,  ...,  4.4632e-04,\n",
            "         -1.2474e-02, -1.1581e-02],\n",
            "        [-7.7019e-03, -1.1398e-02,  4.3755e-03,  ...,  8.2932e-03,\n",
            "          4.9400e-03, -3.0041e-03],\n",
            "        [ 1.7986e-03, -6.6719e-03,  6.9976e-05,  ...,  3.5057e-03,\n",
            "          1.1665e-02, -8.6975e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.lora_down.weight', tensor([[-0.0157,  0.0239,  0.0108,  ...,  0.0247, -0.0165,  0.0216],\n",
            "        [-0.0148, -0.0221,  0.0231,  ...,  0.0159, -0.0125, -0.0228],\n",
            "        [-0.0202, -0.0191,  0.0155,  ..., -0.0113,  0.0248,  0.0251],\n",
            "        ...,\n",
            "        [ 0.0157,  0.0184, -0.0162,  ..., -0.0019, -0.0049,  0.0181],\n",
            "        [-0.0211, -0.0260, -0.0129,  ..., -0.0119,  0.0166,  0.0157],\n",
            "        [ 0.0071, -0.0140, -0.0036,  ..., -0.0066,  0.0219,  0.0066]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight', tensor([[ 0.0200, -0.0152, -0.0123,  ..., -0.0004, -0.0157, -0.0258],\n",
            "        [ 0.0208,  0.0269,  0.0217,  ...,  0.0150,  0.0113,  0.0173],\n",
            "        [ 0.0045, -0.0099,  0.0249,  ..., -0.0053,  0.0227,  0.0269],\n",
            "        ...,\n",
            "        [-0.0030, -0.0131, -0.0010,  ...,  0.0157,  0.0003,  0.0182],\n",
            "        [-0.0220, -0.0197, -0.0007,  ...,  0.0270, -0.0220,  0.0045],\n",
            "        [-0.0069, -0.0066, -0.0195,  ..., -0.0096,  0.0089, -0.0249]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.lora_down.weight', tensor([[-2.2186e-02,  1.0391e-02,  1.1032e-02,  ...,  1.5442e-02,\n",
            "          1.0880e-02, -2.6794e-02],\n",
            "        [-8.6927e-04, -1.6571e-02, -7.9422e-03,  ..., -1.9455e-02,\n",
            "         -2.2888e-02, -2.1103e-02],\n",
            "        [ 1.4046e-02,  1.5778e-02, -1.6266e-02,  ..., -1.8906e-02,\n",
            "         -1.4175e-02,  1.5358e-02],\n",
            "        ...,\n",
            "        [ 3.6697e-03,  1.1703e-02,  1.2840e-02,  ...,  1.9409e-02,\n",
            "         -6.3553e-03, -1.7929e-02],\n",
            "        [ 1.1406e-02, -1.3018e-03,  1.5266e-02,  ...,  1.5579e-02,\n",
            "         -9.9258e-03, -1.3977e-02],\n",
            "        [ 2.1103e-02,  2.3346e-02, -2.6947e-02,  ...,  2.6894e-03,\n",
            "          1.9806e-02,  7.5758e-05]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.lora_down.weight', tensor([[ 0.0150,  0.0231, -0.0241,  ..., -0.0030, -0.0216,  0.0133],\n",
            "        [-0.0242,  0.0070, -0.0236,  ...,  0.0020, -0.0053,  0.0260],\n",
            "        [ 0.0072, -0.0262,  0.0256,  ...,  0.0132, -0.0100, -0.0016],\n",
            "        ...,\n",
            "        [-0.0111,  0.0042, -0.0132,  ...,  0.0083, -0.0149,  0.0020],\n",
            "        [ 0.0047,  0.0193, -0.0138,  ..., -0.0018, -0.0267, -0.0135],\n",
            "        [ 0.0038, -0.0270,  0.0191,  ..., -0.0151,  0.0098, -0.0224]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.lora_down.weight', tensor([[-0.0145, -0.0065, -0.0034,  ..., -0.0170,  0.0213, -0.0108],\n",
            "        [-0.0204,  0.0014, -0.0088,  ...,  0.0099,  0.0190, -0.0039],\n",
            "        [ 0.0077,  0.0019, -0.0054,  ..., -0.0098, -0.0117, -0.0163],\n",
            "        ...,\n",
            "        [ 0.0078, -0.0173,  0.0010,  ...,  0.0085,  0.0110,  0.0199],\n",
            "        [ 0.0126,  0.0204,  0.0200,  ..., -0.0092, -0.0127, -0.0173],\n",
            "        [-0.0039,  0.0084,  0.0076,  ..., -0.0162, -0.0203,  0.0220]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight', tensor([[-1.3870e-02,  1.9394e-02,  1.2390e-02,  ..., -2.5391e-02,\n",
            "          2.5620e-02, -2.6131e-03],\n",
            "        [ 3.7980e-04, -2.6718e-02,  2.6703e-02,  ...,  3.3522e-04,\n",
            "          9.0485e-03,  5.6624e-06],\n",
            "        [ 5.2261e-03,  2.4673e-02, -1.3077e-02,  ..., -2.1683e-02,\n",
            "         -5.6505e-04,  7.2708e-03],\n",
            "        ...,\n",
            "        [-9.7046e-03, -2.3010e-02, -1.4046e-02,  ...,  1.9272e-02,\n",
            "         -1.5511e-02,  5.1651e-03],\n",
            "        [-5.1155e-03, -2.7771e-02,  2.5543e-02,  ..., -1.7900e-03,\n",
            "          2.7527e-02,  5.1460e-03],\n",
            "        [ 2.1729e-02, -2.6321e-02, -2.5345e-02,  ...,  1.1665e-02,\n",
            "          7.6866e-03,  2.1774e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.lora_down.weight', tensor([[-0.0163, -0.0167,  0.0072,  ...,  0.0196, -0.0202, -0.0119],\n",
            "        [ 0.0042, -0.0231, -0.0251,  ...,  0.0068,  0.0233, -0.0032],\n",
            "        [ 0.0129, -0.0181,  0.0201,  ...,  0.0057, -0.0254,  0.0161],\n",
            "        ...,\n",
            "        [ 0.0127, -0.0009, -0.0041,  ..., -0.0227,  0.0004, -0.0247],\n",
            "        [ 0.0156,  0.0227,  0.0224,  ...,  0.0246, -0.0012,  0.0112],\n",
            "        [ 0.0047,  0.0055,  0.0047,  ...,  0.0077,  0.0077, -0.0225]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.lora_down.weight', tensor([[-1.3046e-03, -2.3842e-03,  1.0849e-02,  ...,  1.6205e-02,\n",
            "         -1.1345e-02, -1.9741e-03],\n",
            "        [-4.1962e-03,  1.8295e-02,  1.2177e-02,  ...,  2.5153e-05,\n",
            "         -4.1733e-03, -4.6692e-03],\n",
            "        [ 1.3138e-02,  5.0468e-03, -1.6617e-02,  ..., -2.1637e-02,\n",
            "          1.3313e-02, -8.2703e-03],\n",
            "        ...,\n",
            "        [-9.8801e-03, -1.9180e-02, -5.0545e-03,  ..., -1.5991e-02,\n",
            "          3.8261e-03, -2.1606e-02],\n",
            "        [-1.2009e-02,  8.5907e-03,  2.1027e-02,  ..., -5.1117e-03,\n",
            "         -6.0043e-03,  4.2458e-03],\n",
            "        [ 4.5419e-05, -2.9449e-03, -1.9958e-02,  ...,  1.5411e-02,\n",
            "          1.1513e-02,  1.4114e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight', tensor([[-0.0114,  0.0195, -0.0188,  ...,  0.0246,  0.0068, -0.0099],\n",
            "        [-0.0207,  0.0032,  0.0242,  ..., -0.0271,  0.0130, -0.0277],\n",
            "        [-0.0235, -0.0181,  0.0008,  ...,  0.0007, -0.0260,  0.0038],\n",
            "        ...,\n",
            "        [-0.0038,  0.0145, -0.0209,  ...,  0.0186,  0.0062, -0.0149],\n",
            "        [ 0.0125, -0.0107, -0.0216,  ...,  0.0206,  0.0197,  0.0259],\n",
            "        [-0.0135, -0.0070,  0.0120,  ...,  0.0175, -0.0149,  0.0048]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.lora_down.weight', tensor([[-0.0051, -0.0137, -0.0068,  ...,  0.0093, -0.0014,  0.0064],\n",
            "        [ 0.0100, -0.0101, -0.0072,  ..., -0.0036, -0.0045, -0.0116],\n",
            "        [ 0.0008,  0.0097, -0.0032,  ..., -0.0058,  0.0110,  0.0052],\n",
            "        ...,\n",
            "        [-0.0011, -0.0108, -0.0053,  ...,  0.0064, -0.0138, -0.0138],\n",
            "        [ 0.0043, -0.0035,  0.0030,  ...,  0.0074,  0.0002, -0.0067],\n",
            "        [-0.0034,  0.0058, -0.0020,  ...,  0.0006,  0.0021, -0.0079]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.lora_down.weight', tensor([[-0.0057, -0.0032,  0.0198,  ..., -0.0168,  0.0007,  0.0201],\n",
            "        [ 0.0260, -0.0066, -0.0131,  ..., -0.0033,  0.0185, -0.0171],\n",
            "        [-0.0235, -0.0161, -0.0060,  ...,  0.0058, -0.0072,  0.0082],\n",
            "        ...,\n",
            "        [-0.0109, -0.0265,  0.0179,  ..., -0.0067, -0.0168, -0.0100],\n",
            "        [-0.0117,  0.0115,  0.0076,  ..., -0.0062, -0.0198, -0.0275],\n",
            "        [ 0.0200,  0.0163, -0.0265,  ..., -0.0015,  0.0059,  0.0196]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight', tensor([[ 0.0259,  0.0080, -0.0095,  ...,  0.0187, -0.0102, -0.0082],\n",
            "        [-0.0058,  0.0137,  0.0117,  ..., -0.0156,  0.0231,  0.0253],\n",
            "        [-0.0224,  0.0027, -0.0274,  ..., -0.0040,  0.0228, -0.0122],\n",
            "        ...,\n",
            "        [-0.0209, -0.0184, -0.0200,  ..., -0.0212,  0.0259, -0.0001],\n",
            "        [-0.0278,  0.0264, -0.0131,  ..., -0.0002,  0.0173, -0.0150],\n",
            "        [ 0.0018, -0.0154,  0.0070,  ..., -0.0253,  0.0263,  0.0211]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.lora_down.weight', tensor([[-0.0086, -0.0124,  0.0005,  ..., -0.0067,  0.0145,  0.0001],\n",
            "        [-0.0266,  0.0201, -0.0140,  ...,  0.0234, -0.0212,  0.0093],\n",
            "        [-0.0109, -0.0210, -0.0114,  ...,  0.0143,  0.0244, -0.0201],\n",
            "        ...,\n",
            "        [ 0.0168, -0.0075,  0.0082,  ...,  0.0130,  0.0129,  0.0151],\n",
            "        [-0.0119,  0.0019, -0.0021,  ..., -0.0029,  0.0134, -0.0070],\n",
            "        [-0.0173, -0.0053, -0.0200,  ...,  0.0047,  0.0023, -0.0185]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.lora_down.weight', tensor([[-0.0183,  0.0068,  0.0209,  ..., -0.0235, -0.0143,  0.0237],\n",
            "        [ 0.0258, -0.0038,  0.0029,  ..., -0.0148,  0.0079, -0.0043],\n",
            "        [-0.0095, -0.0021,  0.0086,  ..., -0.0158,  0.0034,  0.0208],\n",
            "        ...,\n",
            "        [-0.0262,  0.0071,  0.0035,  ..., -0.0033, -0.0072, -0.0086],\n",
            "        [-0.0022,  0.0200,  0.0177,  ..., -0.0081, -0.0045, -0.0264],\n",
            "        [ 0.0104,  0.0152, -0.0076,  ..., -0.0251, -0.0187, -0.0005]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.lora_down.weight', tensor([[-0.0129, -0.0208, -0.0163,  ...,  0.0043, -0.0037,  0.0179],\n",
            "        [-0.0113,  0.0023,  0.0194,  ..., -0.0195, -0.0117,  0.0057],\n",
            "        [-0.0033, -0.0220, -0.0126,  ..., -0.0186,  0.0063, -0.0036],\n",
            "        ...,\n",
            "        [-0.0076, -0.0127, -0.0146,  ...,  0.0197, -0.0140,  0.0022],\n",
            "        [ 0.0198, -0.0176, -0.0037,  ..., -0.0024,  0.0053, -0.0086],\n",
            "        [-0.0111,  0.0050,  0.0122,  ...,  0.0183, -0.0099,  0.0040]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight', tensor([[ 0.0226, -0.0181, -0.0257,  ...,  0.0013, -0.0086, -0.0082],\n",
            "        [ 0.0147, -0.0128, -0.0163,  ...,  0.0174,  0.0178, -0.0120],\n",
            "        [ 0.0229,  0.0029,  0.0132,  ..., -0.0182,  0.0243, -0.0058],\n",
            "        ...,\n",
            "        [-0.0063, -0.0035, -0.0154,  ...,  0.0216, -0.0225, -0.0259],\n",
            "        [-0.0061,  0.0043, -0.0038,  ..., -0.0240, -0.0014,  0.0139],\n",
            "        [-0.0081, -0.0275,  0.0159,  ..., -0.0145,  0.0254, -0.0264]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.lora_down.weight', tensor([[-0.0229,  0.0008, -0.0195,  ..., -0.0152, -0.0046,  0.0225],\n",
            "        [ 0.0250,  0.0271, -0.0275,  ..., -0.0264,  0.0223, -0.0228],\n",
            "        [-0.0205,  0.0217, -0.0216,  ...,  0.0073,  0.0202,  0.0101],\n",
            "        ...,\n",
            "        [ 0.0030,  0.0026, -0.0094,  ..., -0.0014,  0.0039,  0.0015],\n",
            "        [-0.0088, -0.0233, -0.0196,  ...,  0.0067,  0.0248, -0.0276],\n",
            "        [-0.0122, -0.0156, -0.0118,  ...,  0.0225,  0.0086,  0.0193]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.lora_down.weight', tensor([[-0.0167,  0.0082, -0.0154,  ..., -0.0195,  0.0170,  0.0095],\n",
            "        [ 0.0111, -0.0143,  0.0164,  ...,  0.0006, -0.0024,  0.0213],\n",
            "        [ 0.0120,  0.0079, -0.0027,  ...,  0.0177, -0.0085, -0.0137],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0088,  0.0146,  ..., -0.0136, -0.0119,  0.0209],\n",
            "        [ 0.0216, -0.0036,  0.0113,  ...,  0.0150, -0.0020, -0.0169],\n",
            "        [ 0.0106, -0.0178,  0.0202,  ...,  0.0032,  0.0157,  0.0063]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight', tensor([[-0.0082, -0.0230, -0.0018,  ..., -0.0102, -0.0227,  0.0055],\n",
            "        [ 0.0028, -0.0085,  0.0100,  ...,  0.0228, -0.0187,  0.0224],\n",
            "        [ 0.0032, -0.0136,  0.0113,  ..., -0.0152,  0.0205, -0.0020],\n",
            "        ...,\n",
            "        [-0.0246,  0.0146, -0.0097,  ...,  0.0251,  0.0060,  0.0090],\n",
            "        [ 0.0128, -0.0061, -0.0051,  ...,  0.0060, -0.0265, -0.0035],\n",
            "        [-0.0190, -0.0171, -0.0210,  ...,  0.0020,  0.0131, -0.0123]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.lora_down.weight', tensor([[-0.0137,  0.0033,  0.0086,  ...,  0.0079,  0.0050,  0.0041],\n",
            "        [-0.0055,  0.0036,  0.0011,  ...,  0.0031, -0.0057, -0.0101],\n",
            "        [-0.0053,  0.0071,  0.0040,  ..., -0.0097, -0.0109,  0.0048],\n",
            "        ...,\n",
            "        [-0.0123, -0.0084, -0.0063,  ...,  0.0105,  0.0015, -0.0069],\n",
            "        [ 0.0073, -0.0092, -0.0008,  ..., -0.0070,  0.0001, -0.0091],\n",
            "        [-0.0004, -0.0012, -0.0116,  ...,  0.0072,  0.0019,  0.0031]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_proj_in.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_proj_in.lora_down.weight', tensor([[ 0.0185, -0.0120, -0.0207,  ...,  0.0253,  0.0128, -0.0100],\n",
            "        [ 0.0074, -0.0122,  0.0243,  ..., -0.0216,  0.0127,  0.0217],\n",
            "        [ 0.0136, -0.0129, -0.0067,  ..., -0.0228,  0.0051,  0.0113],\n",
            "        ...,\n",
            "        [-0.0174,  0.0199, -0.0038,  ..., -0.0247,  0.0050,  0.0169],\n",
            "        [-0.0124, -0.0125,  0.0157,  ...,  0.0048,  0.0017, -0.0030],\n",
            "        [-0.0034,  0.0190,  0.0200,  ..., -0.0050,  0.0238, -0.0066]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_proj_in.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_proj_out.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_proj_out.lora_down.weight', tensor([[-0.0129,  0.0179,  0.0157,  ...,  0.0116, -0.0033,  0.0011],\n",
            "        [ 0.0027, -0.0183, -0.0193,  ...,  0.0143,  0.0265,  0.0273],\n",
            "        [ 0.0029, -0.0198,  0.0098,  ...,  0.0226,  0.0157,  0.0095],\n",
            "        ...,\n",
            "        [-0.0051,  0.0044,  0.0257,  ..., -0.0158, -0.0198, -0.0047],\n",
            "        [ 0.0222,  0.0139,  0.0055,  ...,  0.0188,  0.0258, -0.0226],\n",
            "        [ 0.0015,  0.0085, -0.0027,  ...,  0.0225,  0.0178, -0.0206]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_proj_out.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.lora_down.weight', tensor([[ 0.0211,  0.0134, -0.0076,  ...,  0.0032, -0.0152,  0.0173],\n",
            "        [-0.0053, -0.0163,  0.0083,  ...,  0.0249,  0.0023, -0.0245],\n",
            "        [ 0.0041, -0.0089, -0.0271,  ...,  0.0121, -0.0024, -0.0276],\n",
            "        ...,\n",
            "        [ 0.0085, -0.0075, -0.0099,  ..., -0.0040,  0.0217, -0.0143],\n",
            "        [-0.0204, -0.0277,  0.0227,  ...,  0.0032, -0.0084, -0.0153],\n",
            "        [-0.0218,  0.0150,  0.0203,  ...,  0.0154, -0.0269, -0.0112]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight', tensor([[ 0.0242, -0.0151, -0.0192,  ...,  0.0093,  0.0113, -0.0109],\n",
            "        [ 0.0111,  0.0075,  0.0046,  ...,  0.0205,  0.0087, -0.0266],\n",
            "        [ 0.0071, -0.0191,  0.0231,  ...,  0.0254,  0.0017,  0.0073],\n",
            "        ...,\n",
            "        [ 0.0233, -0.0233, -0.0103,  ..., -0.0021, -0.0164, -0.0007],\n",
            "        [-0.0145, -0.0082, -0.0269,  ..., -0.0246,  0.0154,  0.0033],\n",
            "        [ 0.0013,  0.0243, -0.0157,  ..., -0.0196, -0.0057,  0.0029]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.lora_down.weight', tensor([[ 0.0110,  0.0189,  0.0188,  ...,  0.0264,  0.0244, -0.0270],\n",
            "        [ 0.0164, -0.0170,  0.0149,  ..., -0.0213,  0.0161,  0.0265],\n",
            "        [ 0.0123,  0.0173, -0.0012,  ..., -0.0122,  0.0227,  0.0159],\n",
            "        ...,\n",
            "        [ 0.0106, -0.0055,  0.0272,  ..., -0.0129,  0.0271, -0.0063],\n",
            "        [ 0.0005, -0.0214,  0.0204,  ...,  0.0140,  0.0087, -0.0156],\n",
            "        [-0.0106, -0.0053,  0.0090,  ...,  0.0240, -0.0144, -0.0011]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.lora_down.weight', tensor([[ 0.0071, -0.0222,  0.0103,  ..., -0.0017,  0.0205, -0.0246],\n",
            "        [-0.0133, -0.0032, -0.0126,  ...,  0.0089,  0.0231, -0.0134],\n",
            "        [-0.0192, -0.0126, -0.0246,  ...,  0.0093,  0.0255, -0.0235],\n",
            "        ...,\n",
            "        [-0.0129,  0.0073,  0.0123,  ..., -0.0264, -0.0155,  0.0168],\n",
            "        [ 0.0080,  0.0247,  0.0059,  ...,  0.0234,  0.0165,  0.0219],\n",
            "        [ 0.0168, -0.0055, -0.0229,  ...,  0.0022,  0.0064,  0.0222]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.lora_down.weight', tensor([[-0.0152, -0.0221, -0.0091,  ..., -0.0024, -0.0205,  0.0004],\n",
            "        [-0.0025,  0.0093,  0.0161,  ...,  0.0119,  0.0190, -0.0145],\n",
            "        [-0.0220,  0.0013,  0.0207,  ...,  0.0201,  0.0041, -0.0149],\n",
            "        ...,\n",
            "        [ 0.0037,  0.0091,  0.0154,  ...,  0.0187, -0.0151,  0.0007],\n",
            "        [-0.0108, -0.0186,  0.0070,  ..., -0.0037,  0.0172, -0.0030],\n",
            "        [-0.0190, -0.0187,  0.0132,  ..., -0.0094,  0.0130, -0.0069]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight', tensor([[ 0.0022, -0.0172, -0.0026,  ...,  0.0258, -0.0002, -0.0137],\n",
            "        [-0.0198,  0.0260,  0.0146,  ...,  0.0255, -0.0053, -0.0270],\n",
            "        [ 0.0083, -0.0177, -0.0123,  ...,  0.0156,  0.0258,  0.0043],\n",
            "        ...,\n",
            "        [-0.0256,  0.0121,  0.0126,  ..., -0.0042, -0.0272,  0.0066],\n",
            "        [ 0.0036,  0.0160, -0.0007,  ..., -0.0127, -0.0008, -0.0083],\n",
            "        [ 0.0197,  0.0208, -0.0075,  ..., -0.0271,  0.0059,  0.0236]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.lora_down.weight', tensor([[ 0.0203, -0.0175, -0.0142,  ...,  0.0181, -0.0117,  0.0029],\n",
            "        [-0.0127,  0.0040, -0.0175,  ..., -0.0255,  0.0036,  0.0152],\n",
            "        [-0.0003,  0.0013, -0.0196,  ...,  0.0075,  0.0052, -0.0004],\n",
            "        ...,\n",
            "        [-0.0129,  0.0278, -0.0078,  ...,  0.0109,  0.0045, -0.0119],\n",
            "        [ 0.0171,  0.0193, -0.0078,  ...,  0.0163,  0.0265, -0.0187],\n",
            "        [-0.0044, -0.0203, -0.0175,  ...,  0.0055,  0.0119,  0.0082]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.lora_down.weight', tensor([[-0.0041, -0.0159,  0.0189,  ..., -0.0030, -0.0078, -0.0112],\n",
            "        [-0.0181, -0.0035, -0.0153,  ...,  0.0155, -0.0002,  0.0195],\n",
            "        [ 0.0185, -0.0197,  0.0116,  ..., -0.0073,  0.0021,  0.0138],\n",
            "        ...,\n",
            "        [-0.0014,  0.0217,  0.0181,  ...,  0.0206,  0.0115,  0.0024],\n",
            "        [ 0.0150, -0.0045,  0.0021,  ...,  0.0154,  0.0207, -0.0026],\n",
            "        [-0.0159, -0.0144, -0.0148,  ...,  0.0199,  0.0195, -0.0010]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight', tensor([[-1.8402e-02, -2.3788e-02, -1.4328e-02,  ...,  8.3625e-05,\n",
            "         -1.6724e-02,  1.6479e-02],\n",
            "        [ 1.6190e-02,  2.6474e-02,  2.4628e-02,  ...,  9.2239e-03,\n",
            "         -2.6169e-03, -5.4779e-03],\n",
            "        [-2.7023e-02, -1.3977e-02, -2.6035e-03,  ...,  3.1996e-04,\n",
            "          2.3174e-03, -4.9353e-04],\n",
            "        ...,\n",
            "        [ 1.3947e-02, -5.6725e-03, -1.1726e-02,  ...,  2.0538e-02,\n",
            "          8.7433e-03,  5.5428e-03],\n",
            "        [ 2.4521e-02, -2.5681e-02, -4.1847e-03,  ..., -1.6571e-02,\n",
            "         -5.3749e-03,  7.5798e-03],\n",
            "        [-2.3712e-02,  2.6123e-02,  1.3985e-02,  ...,  2.0432e-02,\n",
            "          1.3763e-02, -2.7374e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.lora_down.weight', tensor([[-0.0130, -0.0088, -0.0056,  ..., -0.0103, -0.0012,  0.0036],\n",
            "        [-0.0042,  0.0126,  0.0106,  ..., -0.0014, -0.0033,  0.0100],\n",
            "        [ 0.0030,  0.0010,  0.0055,  ...,  0.0139, -0.0029, -0.0026],\n",
            "        ...,\n",
            "        [-0.0096, -0.0078,  0.0009,  ...,  0.0013,  0.0014,  0.0034],\n",
            "        [-0.0013,  0.0047, -0.0033,  ..., -0.0015, -0.0083,  0.0029],\n",
            "        [-0.0051,  0.0121,  0.0034,  ..., -0.0097,  0.0065,  0.0055]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.lora_down.weight', tensor([[ 2.7603e-02,  2.6016e-02, -1.1803e-02,  ..., -2.1729e-02,\n",
            "         -6.5460e-03,  2.3804e-03],\n",
            "        [-1.0529e-03, -1.2993e-02,  2.0390e-03,  ...,  1.7242e-02,\n",
            "          2.1851e-02,  7.2136e-03],\n",
            "        [ 2.0325e-02,  4.8447e-03, -2.2415e-02,  ...,  9.1076e-04,\n",
            "         -1.4610e-02,  2.0050e-02],\n",
            "        ...,\n",
            "        [ 2.2186e-02, -2.8467e-04,  1.5442e-02,  ...,  3.6011e-03,\n",
            "          6.5029e-05, -1.8555e-02],\n",
            "        [-2.5269e-02,  2.1667e-02, -1.0078e-02,  ...,  1.0063e-02,\n",
            "          2.1194e-02, -2.0966e-02],\n",
            "        [-8.9493e-03,  1.1909e-02,  1.7227e-02,  ..., -2.6535e-02,\n",
            "          4.4060e-03,  2.0752e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight', tensor([[ 0.0088, -0.0015,  0.0275,  ...,  0.0042,  0.0037,  0.0034],\n",
            "        [-0.0037, -0.0240, -0.0033,  ..., -0.0193,  0.0040, -0.0113],\n",
            "        [-0.0136,  0.0166,  0.0240,  ..., -0.0154,  0.0083,  0.0160],\n",
            "        ...,\n",
            "        [-0.0095,  0.0167, -0.0055,  ..., -0.0257,  0.0240,  0.0279],\n",
            "        [ 0.0099,  0.0258,  0.0263,  ..., -0.0051, -0.0113,  0.0244],\n",
            "        [-0.0046,  0.0094,  0.0130,  ..., -0.0095,  0.0121,  0.0138]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.lora_down.weight', tensor([[ 0.0250,  0.0254,  0.0095,  ...,  0.0105, -0.0085,  0.0233],\n",
            "        [-0.0091,  0.0075,  0.0006,  ..., -0.0146, -0.0014,  0.0130],\n",
            "        [-0.0202, -0.0262,  0.0231,  ..., -0.0182,  0.0164, -0.0015],\n",
            "        ...,\n",
            "        [ 0.0262, -0.0021, -0.0052,  ...,  0.0180, -0.0045,  0.0089],\n",
            "        [ 0.0237,  0.0150, -0.0100,  ...,  0.0055,  0.0004,  0.0223],\n",
            "        [-0.0208,  0.0024, -0.0264,  ...,  0.0193, -0.0229, -0.0118]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.lora_down.weight', tensor([[ 0.0238,  0.0132,  0.0277,  ..., -0.0140, -0.0120,  0.0017],\n",
            "        [ 0.0034, -0.0072, -0.0081,  ...,  0.0234, -0.0057,  0.0020],\n",
            "        [-0.0255,  0.0134,  0.0009,  ..., -0.0026, -0.0010,  0.0261],\n",
            "        ...,\n",
            "        [ 0.0023, -0.0115,  0.0185,  ...,  0.0159, -0.0096,  0.0193],\n",
            "        [ 0.0174, -0.0085,  0.0080,  ..., -0.0188,  0.0277,  0.0164],\n",
            "        [ 0.0009, -0.0256,  0.0040,  ...,  0.0165, -0.0072,  0.0275]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.lora_down.weight', tensor([[ 0.0092, -0.0119,  0.0126,  ..., -0.0161,  0.0195,  0.0206],\n",
            "        [-0.0180,  0.0204,  0.0214,  ...,  0.0038, -0.0109,  0.0199],\n",
            "        [-0.0186, -0.0103, -0.0137,  ...,  0.0090,  0.0021, -0.0167],\n",
            "        ...,\n",
            "        [ 0.0075,  0.0145, -0.0088,  ...,  0.0168, -0.0043, -0.0186],\n",
            "        [ 0.0094,  0.0191, -0.0027,  ..., -0.0075, -0.0219, -0.0025],\n",
            "        [-0.0026, -0.0055,  0.0025,  ..., -0.0190,  0.0001, -0.0199]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight', tensor([[-0.0255, -0.0030, -0.0091,  ...,  0.0078,  0.0095,  0.0148],\n",
            "        [-0.0132, -0.0275,  0.0107,  ...,  0.0097, -0.0198,  0.0235],\n",
            "        [ 0.0242, -0.0224, -0.0204,  ...,  0.0009,  0.0192, -0.0247],\n",
            "        ...,\n",
            "        [-0.0140, -0.0176, -0.0049,  ...,  0.0012,  0.0197, -0.0271],\n",
            "        [ 0.0280,  0.0124,  0.0082,  ...,  0.0185, -0.0158,  0.0279],\n",
            "        [-0.0100, -0.0219,  0.0211,  ..., -0.0112, -0.0003,  0.0260]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.lora_down.weight', tensor([[-0.0048,  0.0193, -0.0165,  ..., -0.0080,  0.0274,  0.0129],\n",
            "        [ 0.0095, -0.0159, -0.0044,  ...,  0.0278, -0.0228,  0.0093],\n",
            "        [ 0.0253,  0.0243, -0.0205,  ..., -0.0090, -0.0085, -0.0209],\n",
            "        ...,\n",
            "        [ 0.0054, -0.0172, -0.0270,  ...,  0.0052, -0.0096, -0.0109],\n",
            "        [ 0.0272, -0.0078, -0.0147,  ..., -0.0264, -0.0234, -0.0063],\n",
            "        [ 0.0063, -0.0275,  0.0213,  ..., -0.0111, -0.0178,  0.0215]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.lora_down.weight', tensor([[ 0.0214,  0.0197, -0.0104,  ...,  0.0086,  0.0199, -0.0200],\n",
            "        [-0.0034, -0.0176, -0.0031,  ..., -0.0070,  0.0186,  0.0018],\n",
            "        [ 0.0019,  0.0198,  0.0059,  ...,  0.0096,  0.0102,  0.0178],\n",
            "        ...,\n",
            "        [ 0.0017, -0.0092, -0.0094,  ..., -0.0072, -0.0007, -0.0019],\n",
            "        [ 0.0181, -0.0015,  0.0178,  ...,  0.0009, -0.0200, -0.0070],\n",
            "        [ 0.0118,  0.0212, -0.0098,  ..., -0.0129, -0.0001,  0.0212]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight', tensor([[ 0.0235, -0.0019,  0.0049,  ...,  0.0148,  0.0185,  0.0068],\n",
            "        [-0.0094,  0.0072,  0.0215,  ...,  0.0171,  0.0199, -0.0075],\n",
            "        [-0.0126,  0.0258,  0.0119,  ..., -0.0222, -0.0215, -0.0219],\n",
            "        ...,\n",
            "        [-0.0098,  0.0236,  0.0195,  ...,  0.0203,  0.0040,  0.0279],\n",
            "        [-0.0029,  0.0117,  0.0077,  ...,  0.0060,  0.0218,  0.0264],\n",
            "        [-0.0075,  0.0015, -0.0185,  ..., -0.0195,  0.0219, -0.0260]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.lora_down.weight', tensor([[ 0.0113, -0.0109,  0.0125,  ...,  0.0025,  0.0115,  0.0090],\n",
            "        [ 0.0002,  0.0060,  0.0025,  ..., -0.0102,  0.0053,  0.0035],\n",
            "        [ 0.0024,  0.0005,  0.0027,  ...,  0.0002, -0.0110,  0.0064],\n",
            "        ...,\n",
            "        [ 0.0080, -0.0091,  0.0104,  ..., -0.0108, -0.0043,  0.0106],\n",
            "        [ 0.0018,  0.0125, -0.0070,  ...,  0.0072, -0.0135,  0.0032],\n",
            "        [-0.0047, -0.0112,  0.0025,  ...,  0.0014, -0.0022,  0.0098]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.lora_down.weight', tensor([[-0.0248,  0.0122, -0.0244,  ..., -0.0039, -0.0234,  0.0137],\n",
            "        [ 0.0210,  0.0122,  0.0080,  ..., -0.0278,  0.0068, -0.0158],\n",
            "        [ 0.0211,  0.0257,  0.0242,  ..., -0.0113, -0.0145, -0.0247],\n",
            "        ...,\n",
            "        [ 0.0138,  0.0100,  0.0235,  ..., -0.0267,  0.0004,  0.0231],\n",
            "        [-0.0140, -0.0137, -0.0211,  ...,  0.0104, -0.0009, -0.0077],\n",
            "        [-0.0169,  0.0137,  0.0093,  ..., -0.0154,  0.0033, -0.0029]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight', tensor([[-0.0005,  0.0146,  0.0127,  ...,  0.0101,  0.0241,  0.0041],\n",
            "        [-0.0247,  0.0251, -0.0200,  ...,  0.0080,  0.0269,  0.0189],\n",
            "        [ 0.0153, -0.0097, -0.0249,  ..., -0.0234, -0.0139,  0.0267],\n",
            "        ...,\n",
            "        [-0.0188,  0.0200, -0.0208,  ..., -0.0071,  0.0267,  0.0206],\n",
            "        [-0.0212,  0.0257,  0.0129,  ..., -0.0192,  0.0115, -0.0219],\n",
            "        [-0.0174, -0.0221,  0.0176,  ..., -0.0068, -0.0003,  0.0177]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.lora_down.weight', tensor([[-0.0082,  0.0157,  0.0064,  ...,  0.0236, -0.0051, -0.0034],\n",
            "        [-0.0246,  0.0196,  0.0186,  ...,  0.0070, -0.0018,  0.0105],\n",
            "        [-0.0232, -0.0144,  0.0130,  ..., -0.0163,  0.0192, -0.0012],\n",
            "        ...,\n",
            "        [-0.0251, -0.0046,  0.0235,  ...,  0.0163, -0.0209,  0.0113],\n",
            "        [-0.0100,  0.0187, -0.0027,  ..., -0.0271, -0.0232, -0.0199],\n",
            "        [-0.0114, -0.0183, -0.0159,  ...,  0.0066, -0.0119, -0.0015]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.lora_down.weight', tensor([[-0.0121,  0.0044, -0.0141,  ..., -0.0191,  0.0087,  0.0162],\n",
            "        [ 0.0002, -0.0230, -0.0271,  ...,  0.0010,  0.0231, -0.0095],\n",
            "        [ 0.0141, -0.0191,  0.0054,  ..., -0.0089, -0.0246, -0.0105],\n",
            "        ...,\n",
            "        [-0.0245,  0.0088, -0.0062,  ..., -0.0215, -0.0181, -0.0114],\n",
            "        [-0.0053, -0.0109, -0.0223,  ...,  0.0056,  0.0204,  0.0135],\n",
            "        [-0.0017,  0.0093, -0.0059,  ..., -0.0171, -0.0205, -0.0098]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.lora_down.weight', tensor([[-0.0215,  0.0176, -0.0211,  ..., -0.0159, -0.0121,  0.0117],\n",
            "        [ 0.0056,  0.0058, -0.0041,  ...,  0.0012, -0.0176, -0.0217],\n",
            "        [ 0.0163,  0.0140,  0.0172,  ...,  0.0007,  0.0196,  0.0081],\n",
            "        ...,\n",
            "        [-0.0011,  0.0070, -0.0026,  ...,  0.0048, -0.0199, -0.0136],\n",
            "        [-0.0064, -0.0202, -0.0107,  ..., -0.0122,  0.0084,  0.0163],\n",
            "        [ 0.0077, -0.0186, -0.0029,  ...,  0.0155, -0.0084,  0.0104]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight', tensor([[-0.0094,  0.0101,  0.0213,  ..., -0.0021,  0.0011,  0.0202],\n",
            "        [ 0.0205, -0.0072, -0.0214,  ..., -0.0204,  0.0161,  0.0059],\n",
            "        [-0.0233, -0.0163, -0.0249,  ...,  0.0121, -0.0214, -0.0241],\n",
            "        ...,\n",
            "        [-0.0167,  0.0052,  0.0198,  ...,  0.0143,  0.0186,  0.0147],\n",
            "        [ 0.0189,  0.0120,  0.0231,  ...,  0.0116, -0.0202,  0.0046],\n",
            "        [ 0.0266, -0.0170,  0.0134,  ..., -0.0205, -0.0243, -0.0222]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.lora_down.weight', tensor([[ 0.0086, -0.0104,  0.0212,  ...,  0.0040, -0.0033, -0.0215],\n",
            "        [ 0.0001,  0.0245, -0.0226,  ...,  0.0260, -0.0254,  0.0074],\n",
            "        [ 0.0060, -0.0270, -0.0041,  ...,  0.0088, -0.0029,  0.0045],\n",
            "        ...,\n",
            "        [ 0.0008, -0.0212, -0.0163,  ...,  0.0034,  0.0211,  0.0125],\n",
            "        [-0.0238,  0.0238,  0.0089,  ..., -0.0152, -0.0246, -0.0133],\n",
            "        [-0.0206,  0.0020,  0.0160,  ..., -0.0150, -0.0138, -0.0063]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.lora_down.weight', tensor([[ 1.2344e-02, -1.9424e-02, -1.5518e-02,  ...,  3.1986e-03,\n",
            "          1.5015e-02,  1.1642e-02],\n",
            "        [ 3.9077e-04, -2.1042e-02,  5.3139e-03,  ..., -1.6327e-02,\n",
            "         -1.6510e-02,  1.1108e-02],\n",
            "        [-9.9258e-03,  9.2392e-03,  1.2329e-02,  ...,  1.5945e-02,\n",
            "         -9.6054e-03, -7.7667e-03],\n",
            "        ...,\n",
            "        [-8.0643e-03, -9.9468e-04,  2.2812e-03,  ..., -1.7563e-02,\n",
            "          4.2152e-03, -1.9363e-02],\n",
            "        [-3.1071e-03,  1.1047e-02,  1.1917e-02,  ..., -1.3222e-02,\n",
            "          1.9287e-02,  1.4107e-02],\n",
            "        [ 4.7386e-05,  4.9896e-03, -2.0981e-02,  ...,  7.4768e-03,\n",
            "          3.5691e-04,  7.4234e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight', tensor([[-1.3954e-02,  1.7517e-02,  5.0774e-03,  ..., -1.2779e-02,\n",
            "         -1.6434e-02, -1.5427e-02],\n",
            "        [-1.4954e-02, -2.3727e-02,  4.3259e-03,  ...,  1.5266e-02,\n",
            "          1.9333e-02, -1.1688e-02],\n",
            "        [-3.3283e-03,  1.7471e-02, -5.3673e-03,  ..., -7.0632e-05,\n",
            "         -1.1993e-02, -7.0839e-03],\n",
            "        ...,\n",
            "        [ 4.3983e-03, -1.4778e-02, -3.1319e-03,  ...,  1.2634e-02,\n",
            "          5.1994e-03,  2.3468e-02],\n",
            "        [-2.0111e-02,  2.8667e-03,  2.7191e-02,  ..., -6.6490e-03,\n",
            "          1.5762e-02,  2.1191e-03],\n",
            "        [ 1.8132e-04, -7.4234e-03, -1.8829e-02,  ...,  5.3406e-03,\n",
            "         -1.2367e-02, -1.0513e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.lora_down.weight', tensor([[-0.0095, -0.0031,  0.0097,  ..., -0.0050, -0.0093,  0.0081],\n",
            "        [-0.0103, -0.0011, -0.0040,  ..., -0.0056,  0.0086,  0.0056],\n",
            "        [ 0.0134, -0.0125,  0.0086,  ..., -0.0030, -0.0055,  0.0066],\n",
            "        ...,\n",
            "        [ 0.0009,  0.0056,  0.0101,  ..., -0.0044,  0.0122,  0.0083],\n",
            "        [ 0.0039, -0.0092, -0.0081,  ..., -0.0090,  0.0060, -0.0031],\n",
            "        [-0.0058, -0.0121,  0.0005,  ...,  0.0001, -0.0035, -0.0023]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.lora_down.weight', tensor([[-0.0199, -0.0126, -0.0164,  ..., -0.0033, -0.0102,  0.0135],\n",
            "        [-0.0085, -0.0016, -0.0269,  ..., -0.0238, -0.0163,  0.0153],\n",
            "        [-0.0140,  0.0011,  0.0196,  ...,  0.0160,  0.0188, -0.0209],\n",
            "        ...,\n",
            "        [ 0.0068, -0.0113,  0.0011,  ...,  0.0019, -0.0016,  0.0242],\n",
            "        [-0.0014, -0.0111, -0.0209,  ...,  0.0062,  0.0161,  0.0216],\n",
            "        [ 0.0248,  0.0115, -0.0003,  ...,  0.0210,  0.0006, -0.0079]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight', tensor([[-0.0118, -0.0023, -0.0072,  ..., -0.0002,  0.0211, -0.0075],\n",
            "        [ 0.0207, -0.0151,  0.0203,  ...,  0.0248,  0.0062, -0.0067],\n",
            "        [ 0.0200, -0.0162,  0.0227,  ..., -0.0190,  0.0278,  0.0164],\n",
            "        ...,\n",
            "        [-0.0061, -0.0151,  0.0246,  ...,  0.0196, -0.0084,  0.0067],\n",
            "        [ 0.0153, -0.0213,  0.0040,  ...,  0.0079, -0.0179,  0.0006],\n",
            "        [ 0.0212, -0.0276, -0.0267,  ..., -0.0023,  0.0273,  0.0121]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.lora_down.weight', tensor([[-0.0055,  0.0166,  0.0006,  ..., -0.0087,  0.0263,  0.0166],\n",
            "        [-0.0236, -0.0249, -0.0021,  ..., -0.0050, -0.0274,  0.0269],\n",
            "        [ 0.0264,  0.0007,  0.0171,  ..., -0.0103,  0.0182, -0.0057],\n",
            "        ...,\n",
            "        [-0.0227,  0.0209,  0.0039,  ...,  0.0225,  0.0060, -0.0201],\n",
            "        [ 0.0135, -0.0182, -0.0064,  ..., -0.0061, -0.0175,  0.0039],\n",
            "        [ 0.0069, -0.0016, -0.0120,  ...,  0.0193,  0.0249,  0.0229]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.lora_down.weight', tensor([[-2.3499e-02, -6.3629e-03, -2.0351e-03,  ..., -1.3733e-02,\n",
            "          1.9638e-02, -1.4153e-02],\n",
            "        [ 1.0147e-02,  6.8550e-03,  2.3331e-02,  ..., -1.5190e-02,\n",
            "          1.1452e-02,  1.3885e-02],\n",
            "        [-6.8331e-04,  1.9379e-02,  2.6794e-02,  ...,  1.4412e-02,\n",
            "          2.6443e-02,  9.2545e-03],\n",
            "        ...,\n",
            "        [ 1.3535e-02, -1.6365e-03,  3.9978e-03,  ...,  1.6647e-02,\n",
            "         -1.6724e-02, -1.7502e-02],\n",
            "        [ 1.1063e-03, -2.1866e-02, -7.3776e-03,  ..., -2.1927e-02,\n",
            "         -1.5625e-02,  1.1902e-02],\n",
            "        [-8.0643e-03,  7.9498e-03,  2.3723e-05,  ...,  4.9973e-03,\n",
            "         -6.3248e-03, -6.4888e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.lora_down.weight', tensor([[ 0.0039,  0.0120, -0.0028,  ...,  0.0165,  0.0036,  0.0076],\n",
            "        [ 0.0018, -0.0072, -0.0111,  ...,  0.0122,  0.0158,  0.0083],\n",
            "        [-0.0036, -0.0125,  0.0090,  ...,  0.0216,  0.0125,  0.0012],\n",
            "        ...,\n",
            "        [-0.0191, -0.0194, -0.0068,  ...,  0.0056,  0.0142, -0.0176],\n",
            "        [-0.0180, -0.0097, -0.0052,  ...,  0.0114, -0.0208,  0.0090],\n",
            "        [ 0.0115,  0.0037, -0.0051,  ...,  0.0120,  0.0209, -0.0180]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight', tensor([[ 0.0181,  0.0018,  0.0173,  ..., -0.0051, -0.0112, -0.0251],\n",
            "        [ 0.0261,  0.0156,  0.0068,  ...,  0.0232,  0.0114, -0.0269],\n",
            "        [-0.0045,  0.0210, -0.0181,  ...,  0.0065,  0.0186,  0.0006],\n",
            "        ...,\n",
            "        [-0.0084, -0.0154,  0.0028,  ...,  0.0135,  0.0152, -0.0033],\n",
            "        [ 0.0026,  0.0219, -0.0264,  ..., -0.0218, -0.0256,  0.0007],\n",
            "        [ 0.0224, -0.0199, -0.0105,  ...,  0.0109,  0.0250,  0.0244]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.lora_down.weight', tensor([[-8.2855e-03, -5.9814e-03,  1.7481e-03,  ...,  1.8265e-02,\n",
            "          2.0035e-02, -7.9193e-03],\n",
            "        [ 1.7563e-02, -7.4081e-03, -1.9623e-02,  ..., -6.1646e-03,\n",
            "         -1.3176e-02,  1.3481e-02],\n",
            "        [ 6.3705e-03, -2.2171e-02,  6.3095e-03,  ..., -1.4374e-02,\n",
            "          7.0953e-03, -3.5324e-03],\n",
            "        ...,\n",
            "        [-4.6387e-03, -1.2863e-02,  2.4368e-02,  ...,  1.2421e-02,\n",
            "          8.1177e-03,  2.2659e-03],\n",
            "        [-1.6647e-02,  9.5749e-03, -2.3575e-03,  ..., -2.0325e-02,\n",
            "          9.6817e-03, -2.1477e-03],\n",
            "        [ 2.2202e-02, -1.1070e-02,  4.3869e-03,  ..., -3.4034e-05,\n",
            "         -6.3095e-03, -1.5060e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.lora_down.weight', tensor([[-0.0068, -0.0028, -0.0088,  ...,  0.0170, -0.0004, -0.0188],\n",
            "        [-0.0158,  0.0039,  0.0108,  ..., -0.0029, -0.0164, -0.0074],\n",
            "        [ 0.0217, -0.0122,  0.0218,  ..., -0.0177, -0.0129, -0.0048],\n",
            "        ...,\n",
            "        [-0.0107,  0.0142, -0.0014,  ...,  0.0152, -0.0094, -0.0197],\n",
            "        [ 0.0139,  0.0071,  0.0115,  ..., -0.0017,  0.0183, -0.0048],\n",
            "        [-0.0176,  0.0028, -0.0059,  ..., -0.0216, -0.0071, -0.0201]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight', tensor([[ 0.0202, -0.0124, -0.0273,  ..., -0.0048,  0.0230, -0.0149],\n",
            "        [-0.0043, -0.0118, -0.0082,  ...,  0.0251, -0.0172, -0.0245],\n",
            "        [ 0.0064,  0.0218, -0.0269,  ...,  0.0035,  0.0138,  0.0190],\n",
            "        ...,\n",
            "        [-0.0241, -0.0067,  0.0189,  ..., -0.0018,  0.0043, -0.0082],\n",
            "        [-0.0190, -0.0013,  0.0006,  ..., -0.0091, -0.0230, -0.0201],\n",
            "        [ 0.0131, -0.0030,  0.0106,  ...,  0.0127,  0.0124,  0.0129]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.lora_down.weight', tensor([[-0.0099,  0.0082,  0.0069,  ..., -0.0098, -0.0045, -0.0121],\n",
            "        [ 0.0085,  0.0137,  0.0021,  ...,  0.0086,  0.0002,  0.0090],\n",
            "        [ 0.0079,  0.0090, -0.0084,  ...,  0.0071,  0.0079, -0.0132],\n",
            "        ...,\n",
            "        [-0.0022,  0.0108, -0.0101,  ...,  0.0105,  0.0046,  0.0062],\n",
            "        [ 0.0093,  0.0107,  0.0039,  ...,  0.0124,  0.0061,  0.0019],\n",
            "        [ 0.0065,  0.0082, -0.0010,  ..., -0.0139,  0.0081, -0.0008]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.lora_down.weight', tensor([[ 0.0252, -0.0009,  0.0003,  ..., -0.0025,  0.0091,  0.0129],\n",
            "        [ 0.0096, -0.0129,  0.0161,  ...,  0.0049, -0.0196, -0.0231],\n",
            "        [-0.0092,  0.0244,  0.0138,  ..., -0.0173,  0.0035, -0.0041],\n",
            "        ...,\n",
            "        [ 0.0168, -0.0093, -0.0025,  ...,  0.0240,  0.0189,  0.0233],\n",
            "        [-0.0228,  0.0124,  0.0253,  ...,  0.0013,  0.0115, -0.0013],\n",
            "        [ 0.0119, -0.0272,  0.0207,  ...,  0.0258, -0.0253, -0.0223]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight', tensor([[-0.0080,  0.0037, -0.0242,  ...,  0.0267,  0.0251,  0.0129],\n",
            "        [-0.0026, -0.0102,  0.0254,  ...,  0.0117, -0.0184, -0.0216],\n",
            "        [ 0.0136, -0.0196,  0.0036,  ..., -0.0245, -0.0041,  0.0081],\n",
            "        ...,\n",
            "        [-0.0107, -0.0210,  0.0050,  ..., -0.0240,  0.0148, -0.0148],\n",
            "        [-0.0075, -0.0261, -0.0134,  ..., -0.0110,  0.0277,  0.0064],\n",
            "        [ 0.0189, -0.0258,  0.0062,  ...,  0.0237, -0.0054, -0.0176]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.lora_down.weight', tensor([[-0.0195, -0.0122, -0.0025,  ..., -0.0124, -0.0226, -0.0120],\n",
            "        [-0.0277, -0.0003, -0.0232,  ..., -0.0171,  0.0221, -0.0275],\n",
            "        [ 0.0255,  0.0041, -0.0039,  ...,  0.0060,  0.0044,  0.0238],\n",
            "        ...,\n",
            "        [ 0.0249, -0.0193, -0.0092,  ..., -0.0047,  0.0200, -0.0135],\n",
            "        [ 0.0053,  0.0225, -0.0267,  ...,  0.0210, -0.0012,  0.0078],\n",
            "        [-0.0141,  0.0175,  0.0257,  ..., -0.0219, -0.0133, -0.0220]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.lora_down.weight', tensor([[-0.0186, -0.0232, -0.0137,  ..., -0.0051, -0.0241, -0.0031],\n",
            "        [ 0.0129, -0.0272,  0.0005,  ...,  0.0133, -0.0199,  0.0038],\n",
            "        [-0.0268, -0.0085,  0.0150,  ...,  0.0266, -0.0115, -0.0036],\n",
            "        ...,\n",
            "        [ 0.0244, -0.0075, -0.0042,  ...,  0.0118, -0.0245,  0.0258],\n",
            "        [-0.0100,  0.0101, -0.0273,  ...,  0.0070, -0.0189,  0.0241],\n",
            "        [-0.0010, -0.0060, -0.0021,  ..., -0.0241,  0.0167,  0.0096]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.lora_down.weight', tensor([[-0.0193, -0.0205, -0.0171,  ...,  0.0040,  0.0034,  0.0172],\n",
            "        [-0.0127,  0.0212,  0.0210,  ..., -0.0196,  0.0014, -0.0106],\n",
            "        [ 0.0053, -0.0037, -0.0192,  ...,  0.0157,  0.0139,  0.0149],\n",
            "        ...,\n",
            "        [ 0.0054, -0.0038,  0.0082,  ...,  0.0053,  0.0169, -0.0063],\n",
            "        [-0.0204,  0.0194, -0.0217,  ...,  0.0101, -0.0056, -0.0168],\n",
            "        [ 0.0181, -0.0105, -0.0054,  ...,  0.0128,  0.0123,  0.0213]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight', tensor([[-0.0258,  0.0194,  0.0141,  ..., -0.0201,  0.0019, -0.0061],\n",
            "        [ 0.0083, -0.0164, -0.0265,  ..., -0.0041, -0.0269, -0.0113],\n",
            "        [ 0.0043, -0.0011,  0.0005,  ..., -0.0249,  0.0200,  0.0221],\n",
            "        ...,\n",
            "        [ 0.0026, -0.0111, -0.0140,  ...,  0.0227,  0.0242, -0.0175],\n",
            "        [ 0.0055, -0.0009,  0.0154,  ..., -0.0276, -0.0060,  0.0096],\n",
            "        [ 0.0154,  0.0035, -0.0083,  ..., -0.0136,  0.0014,  0.0269]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.lora_down.weight', tensor([[-0.0270,  0.0066, -0.0256,  ..., -0.0232,  0.0251, -0.0013],\n",
            "        [-0.0193,  0.0068,  0.0028,  ...,  0.0136,  0.0104, -0.0138],\n",
            "        [ 0.0229, -0.0253, -0.0185,  ...,  0.0073,  0.0004, -0.0028],\n",
            "        ...,\n",
            "        [-0.0156,  0.0004,  0.0208,  ...,  0.0267, -0.0190, -0.0173],\n",
            "        [-0.0125, -0.0084, -0.0103,  ..., -0.0167, -0.0194,  0.0016],\n",
            "        [ 0.0105, -0.0137,  0.0063,  ...,  0.0101,  0.0083, -0.0126]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.lora_down.weight', tensor([[-0.0151,  0.0187, -0.0109,  ..., -0.0101, -0.0209, -0.0142],\n",
            "        [ 0.0025,  0.0043,  0.0141,  ..., -0.0069,  0.0169, -0.0060],\n",
            "        [ 0.0215, -0.0008,  0.0175,  ..., -0.0166,  0.0156, -0.0184],\n",
            "        ...,\n",
            "        [-0.0006,  0.0043,  0.0070,  ..., -0.0113, -0.0121, -0.0125],\n",
            "        [ 0.0096,  0.0118, -0.0043,  ..., -0.0133,  0.0106,  0.0023],\n",
            "        [ 0.0113, -0.0089,  0.0044,  ..., -0.0110,  0.0163, -0.0012]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight', tensor([[ 0.0222, -0.0157,  0.0004,  ...,  0.0001, -0.0242, -0.0038],\n",
            "        [ 0.0269, -0.0033,  0.0146,  ..., -0.0124,  0.0238, -0.0100],\n",
            "        [ 0.0105,  0.0219, -0.0234,  ...,  0.0103,  0.0245,  0.0128],\n",
            "        ...,\n",
            "        [-0.0224,  0.0078,  0.0142,  ...,  0.0273, -0.0076, -0.0277],\n",
            "        [-0.0085,  0.0202, -0.0264,  ..., -0.0190,  0.0095, -0.0182],\n",
            "        [-0.0187,  0.0172, -0.0034,  ...,  0.0039,  0.0233, -0.0220]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.lora_down.weight', tensor([[ 0.0024,  0.0017,  0.0134,  ...,  0.0035, -0.0121,  0.0009],\n",
            "        [-0.0113,  0.0072, -0.0054,  ..., -0.0109,  0.0127, -0.0121],\n",
            "        [ 0.0039, -0.0051,  0.0093,  ...,  0.0111, -0.0109,  0.0007],\n",
            "        ...,\n",
            "        [-0.0120, -0.0009, -0.0015,  ...,  0.0099,  0.0048,  0.0121],\n",
            "        [ 0.0101,  0.0026, -0.0066,  ..., -0.0027, -0.0015, -0.0105],\n",
            "        [-0.0049,  0.0044,  0.0102,  ..., -0.0083,  0.0111,  0.0089]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.lora_down.weight', tensor([[ 0.0085,  0.0270,  0.0248,  ..., -0.0069,  0.0075,  0.0228],\n",
            "        [-0.0047, -0.0120,  0.0055,  ...,  0.0122,  0.0262,  0.0113],\n",
            "        [ 0.0071, -0.0155,  0.0095,  ...,  0.0006,  0.0003,  0.0087],\n",
            "        ...,\n",
            "        [ 0.0220, -0.0175,  0.0025,  ...,  0.0275,  0.0175, -0.0147],\n",
            "        [-0.0015,  0.0068,  0.0010,  ...,  0.0206,  0.0247, -0.0228],\n",
            "        [-0.0214, -0.0068,  0.0123,  ..., -0.0005,  0.0222,  0.0084]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight', tensor([[-0.0222,  0.0274, -0.0137,  ...,  0.0007,  0.0262, -0.0161],\n",
            "        [ 0.0113,  0.0161, -0.0014,  ...,  0.0193,  0.0243, -0.0127],\n",
            "        [-0.0179,  0.0027, -0.0033,  ...,  0.0121, -0.0018, -0.0187],\n",
            "        ...,\n",
            "        [ 0.0179, -0.0090, -0.0233,  ...,  0.0206, -0.0137, -0.0098],\n",
            "        [ 0.0183, -0.0163,  0.0033,  ..., -0.0143, -0.0200, -0.0030],\n",
            "        [-0.0100, -0.0095, -0.0101,  ...,  0.0196,  0.0021, -0.0019]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.lora_down.weight', tensor([[ 0.0271,  0.0192,  0.0013,  ...,  0.0036, -0.0207,  0.0178],\n",
            "        [ 0.0241, -0.0100, -0.0066,  ...,  0.0112, -0.0257,  0.0099],\n",
            "        [ 0.0266, -0.0218,  0.0005,  ..., -0.0194,  0.0164, -0.0184],\n",
            "        ...,\n",
            "        [ 0.0150, -0.0100, -0.0020,  ..., -0.0186, -0.0232,  0.0065],\n",
            "        [-0.0184, -0.0155,  0.0047,  ..., -0.0096,  0.0163, -0.0065],\n",
            "        [-0.0076,  0.0248,  0.0010,  ..., -0.0256, -0.0059,  0.0264]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.lora_down.weight', tensor([[-0.0193,  0.0176, -0.0085,  ...,  0.0266, -0.0026,  0.0041],\n",
            "        [ 0.0073,  0.0267,  0.0085,  ...,  0.0136, -0.0212,  0.0066],\n",
            "        [ 0.0161,  0.0194,  0.0047,  ..., -0.0112,  0.0005,  0.0171],\n",
            "        ...,\n",
            "        [ 0.0197, -0.0016,  0.0278,  ...,  0.0069,  0.0219, -0.0177],\n",
            "        [-0.0080,  0.0165,  0.0087,  ..., -0.0089, -0.0221,  0.0279],\n",
            "        [-0.0264, -0.0222,  0.0239,  ...,  0.0056,  0.0240,  0.0030]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.lora_down.weight', tensor([[ 0.0035,  0.0011,  0.0129,  ...,  0.0153, -0.0104, -0.0203],\n",
            "        [ 0.0051,  0.0050, -0.0076,  ...,  0.0091,  0.0185,  0.0211],\n",
            "        [ 0.0193,  0.0135, -0.0085,  ...,  0.0046,  0.0141,  0.0123],\n",
            "        ...,\n",
            "        [-0.0185,  0.0198,  0.0221,  ...,  0.0014, -0.0162, -0.0157],\n",
            "        [-0.0024, -0.0169, -0.0216,  ..., -0.0073,  0.0112,  0.0095],\n",
            "        [ 0.0045,  0.0078, -0.0182,  ...,  0.0084, -0.0219,  0.0089]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight', tensor([[ 0.0032, -0.0097,  0.0141,  ...,  0.0166, -0.0081,  0.0105],\n",
            "        [-0.0032,  0.0148, -0.0235,  ...,  0.0167,  0.0064,  0.0056],\n",
            "        [-0.0130,  0.0125, -0.0278,  ..., -0.0191,  0.0160, -0.0229],\n",
            "        ...,\n",
            "        [-0.0003,  0.0254, -0.0034,  ..., -0.0050, -0.0131,  0.0115],\n",
            "        [ 0.0023,  0.0048, -0.0169,  ..., -0.0053,  0.0038,  0.0104],\n",
            "        [-0.0126,  0.0225, -0.0127,  ..., -0.0209,  0.0210, -0.0059]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.lora_down.weight', tensor([[ 0.0203,  0.0251, -0.0002,  ..., -0.0057,  0.0263,  0.0249],\n",
            "        [-0.0167, -0.0026, -0.0209,  ..., -0.0160, -0.0021,  0.0112],\n",
            "        [-0.0234, -0.0039,  0.0169,  ...,  0.0274,  0.0083, -0.0115],\n",
            "        ...,\n",
            "        [-0.0244, -0.0264,  0.0062,  ...,  0.0228,  0.0231,  0.0022],\n",
            "        [ 0.0023, -0.0048, -0.0009,  ..., -0.0086,  0.0219, -0.0089],\n",
            "        [ 0.0001,  0.0163, -0.0277,  ..., -0.0213,  0.0230,  0.0075]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.lora_down.weight', tensor([[ 0.0004, -0.0131, -0.0174,  ...,  0.0158,  0.0126,  0.0086],\n",
            "        [ 0.0015, -0.0101, -0.0079,  ...,  0.0192, -0.0169, -0.0215],\n",
            "        [-0.0044,  0.0078,  0.0082,  ...,  0.0067, -0.0177,  0.0154],\n",
            "        ...,\n",
            "        [ 0.0155,  0.0173,  0.0218,  ..., -0.0193, -0.0031, -0.0111],\n",
            "        [-0.0054, -0.0044, -0.0086,  ..., -0.0214,  0.0217,  0.0140],\n",
            "        [ 0.0057, -0.0132,  0.0210,  ..., -0.0022, -0.0094,  0.0079]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight', tensor([[-0.0210, -0.0232, -0.0120,  ...,  0.0207, -0.0114, -0.0197],\n",
            "        [ 0.0092, -0.0179, -0.0153,  ...,  0.0187, -0.0227, -0.0218],\n",
            "        [ 0.0052, -0.0122, -0.0247,  ..., -0.0257,  0.0194, -0.0029],\n",
            "        ...,\n",
            "        [-0.0013, -0.0274,  0.0154,  ..., -0.0033,  0.0116,  0.0150],\n",
            "        [ 0.0242,  0.0219,  0.0055,  ..., -0.0224,  0.0157, -0.0073],\n",
            "        [ 0.0075,  0.0208, -0.0145,  ...,  0.0037,  0.0153,  0.0024]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.lora_down.weight', tensor([[-0.0050,  0.0078, -0.0132,  ..., -0.0032, -0.0010,  0.0082],\n",
            "        [-0.0136,  0.0121,  0.0093,  ...,  0.0062, -0.0036,  0.0087],\n",
            "        [-0.0121,  0.0002,  0.0022,  ...,  0.0067,  0.0034,  0.0133],\n",
            "        ...,\n",
            "        [ 0.0059, -0.0038, -0.0106,  ..., -0.0071, -0.0093, -0.0126],\n",
            "        [-0.0028, -0.0110, -0.0114,  ...,  0.0066,  0.0098, -0.0078],\n",
            "        [ 0.0003, -0.0095, -0.0084,  ...,  0.0124, -0.0084, -0.0052]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.lora_down.weight', tensor([[-0.0109,  0.0108, -0.0236,  ...,  0.0072, -0.0139,  0.0209],\n",
            "        [ 0.0224,  0.0133, -0.0008,  ...,  0.0070,  0.0021,  0.0115],\n",
            "        [-0.0079,  0.0079, -0.0148,  ...,  0.0194, -0.0053,  0.0126],\n",
            "        ...,\n",
            "        [-0.0245,  0.0229, -0.0072,  ...,  0.0144,  0.0192, -0.0116],\n",
            "        [ 0.0163, -0.0100,  0.0037,  ..., -0.0170,  0.0081,  0.0110],\n",
            "        [ 0.0251, -0.0072, -0.0009,  ...,  0.0070,  0.0099, -0.0227]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight', tensor([[-0.0036, -0.0197,  0.0229,  ...,  0.0193,  0.0138,  0.0252],\n",
            "        [ 0.0180,  0.0043,  0.0083,  ..., -0.0108, -0.0148,  0.0011],\n",
            "        [-0.0239,  0.0181,  0.0062,  ..., -0.0243, -0.0099, -0.0152],\n",
            "        ...,\n",
            "        [-0.0175,  0.0252,  0.0143,  ...,  0.0239, -0.0212, -0.0150],\n",
            "        [ 0.0099,  0.0087, -0.0066,  ..., -0.0035, -0.0132,  0.0019],\n",
            "        [-0.0067,  0.0145, -0.0146,  ...,  0.0121,  0.0205,  0.0220]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.lora_down.weight', tensor([[ 0.0261,  0.0209,  0.0033,  ..., -0.0225, -0.0249, -0.0101],\n",
            "        [ 0.0237, -0.0220, -0.0018,  ..., -0.0147,  0.0100,  0.0079],\n",
            "        [-0.0252,  0.0023, -0.0195,  ...,  0.0213,  0.0224, -0.0016],\n",
            "        ...,\n",
            "        [-0.0219,  0.0033,  0.0193,  ...,  0.0112,  0.0113,  0.0250],\n",
            "        [-0.0186,  0.0260, -0.0163,  ..., -0.0011,  0.0075,  0.0230],\n",
            "        [ 0.0151, -0.0222,  0.0123,  ...,  0.0173,  0.0087, -0.0204]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.lora_down.weight', tensor([[ 0.0266,  0.0020,  0.0140,  ..., -0.0273,  0.0046,  0.0084],\n",
            "        [-0.0223, -0.0046,  0.0237,  ...,  0.0017, -0.0274,  0.0117],\n",
            "        [-0.0089,  0.0113,  0.0014,  ..., -0.0062,  0.0103, -0.0231],\n",
            "        ...,\n",
            "        [-0.0272, -0.0171,  0.0090,  ...,  0.0270, -0.0264,  0.0251],\n",
            "        [-0.0015, -0.0279,  0.0256,  ..., -0.0239,  0.0034,  0.0201],\n",
            "        [ 0.0260, -0.0196, -0.0086,  ...,  0.0237, -0.0131, -0.0091]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.lora_down.weight', tensor([[-3.0594e-03,  8.6060e-03, -2.0996e-02,  ..., -5.1765e-03,\n",
            "         -2.9526e-03,  6.3667e-03],\n",
            "        [-1.1322e-02,  1.3137e-04, -9.3155e-03,  ...,  1.8967e-02,\n",
            "         -5.1346e-03,  2.0920e-02],\n",
            "        [-2.8744e-03, -1.2360e-02,  1.0651e-02,  ..., -1.1162e-02,\n",
            "         -1.7380e-02,  2.0981e-02],\n",
            "        ...,\n",
            "        [ 9.4147e-03,  8.1711e-03, -2.4986e-03,  ...,  1.6499e-03,\n",
            "         -1.5343e-02, -8.6517e-03],\n",
            "        [ 5.1727e-03,  6.0730e-03, -2.8439e-03,  ...,  1.7181e-02,\n",
            "          9.7504e-03,  2.7561e-03],\n",
            "        [ 2.1324e-03,  1.9333e-02,  3.0935e-05,  ..., -1.5213e-02,\n",
            "         -7.3280e-03,  1.9836e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight', tensor([[ 1.0132e-02, -2.8229e-03, -2.7756e-02,  ...,  1.1192e-02,\n",
            "         -3.0746e-03,  9.3842e-03],\n",
            "        [ 2.7481e-02,  2.5094e-05, -5.7068e-03,  ..., -1.3428e-02,\n",
            "         -5.9166e-03, -9.6817e-03],\n",
            "        [-2.1667e-02, -1.8433e-02,  3.9520e-03,  ..., -1.7578e-02,\n",
            "          1.8036e-02,  1.6800e-02],\n",
            "        ...,\n",
            "        [ 2.0004e-02,  4.4727e-04, -4.0283e-03,  ..., -2.2278e-02,\n",
            "          2.6718e-02,  2.5925e-02],\n",
            "        [ 1.6632e-02,  1.1978e-03, -2.2751e-02,  ...,  4.3106e-03,\n",
            "          2.4261e-02, -1.3512e-02],\n",
            "        [-1.1757e-02, -2.5482e-02, -1.9760e-02,  ...,  4.5052e-03,\n",
            "          1.3588e-02, -1.0567e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.lora_down.weight', tensor([[ 0.0036,  0.0201,  0.0028,  ...,  0.0004,  0.0081, -0.0211],\n",
            "        [ 0.0190,  0.0075,  0.0272,  ..., -0.0239, -0.0210,  0.0176],\n",
            "        [-0.0193, -0.0116, -0.0145,  ...,  0.0166,  0.0056, -0.0273],\n",
            "        ...,\n",
            "        [-0.0166,  0.0049, -0.0129,  ...,  0.0151, -0.0262, -0.0137],\n",
            "        [ 0.0146,  0.0208, -0.0110,  ...,  0.0268,  0.0077, -0.0261],\n",
            "        [-0.0197, -0.0177, -0.0113,  ..., -0.0025, -0.0212, -0.0098]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.lora_down.weight', tensor([[ 0.0145,  0.0008,  0.0207,  ..., -0.0054,  0.0050, -0.0144],\n",
            "        [-0.0142,  0.0058,  0.0005,  ..., -0.0001, -0.0059, -0.0203],\n",
            "        [-0.0210,  0.0094,  0.0092,  ...,  0.0162, -0.0153, -0.0086],\n",
            "        ...,\n",
            "        [ 0.0175,  0.0176, -0.0171,  ...,  0.0200, -0.0127, -0.0068],\n",
            "        [ 0.0207,  0.0183,  0.0049,  ...,  0.0083, -0.0087,  0.0126],\n",
            "        [-0.0035, -0.0126, -0.0173,  ...,  0.0180,  0.0126,  0.0096]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight', tensor([[-0.0020,  0.0247,  0.0092,  ..., -0.0076, -0.0237,  0.0208],\n",
            "        [ 0.0118, -0.0267, -0.0021,  ..., -0.0121, -0.0278,  0.0137],\n",
            "        [ 0.0025,  0.0192,  0.0158,  ..., -0.0273,  0.0048,  0.0266],\n",
            "        ...,\n",
            "        [ 0.0146,  0.0074,  0.0255,  ...,  0.0133, -0.0167, -0.0082],\n",
            "        [ 0.0091, -0.0039, -0.0019,  ...,  0.0210,  0.0181,  0.0086],\n",
            "        [-0.0038, -0.0230, -0.0209,  ...,  0.0026,  0.0162, -0.0024]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.lora_down.weight', tensor([[-0.0042, -0.0031, -0.0106,  ..., -0.0118,  0.0080, -0.0008],\n",
            "        [ 0.0082, -0.0059,  0.0118,  ..., -0.0090,  0.0105, -0.0032],\n",
            "        [-0.0015,  0.0060,  0.0038,  ...,  0.0053,  0.0050,  0.0123],\n",
            "        ...,\n",
            "        [ 0.0037, -0.0035, -0.0047,  ..., -0.0015, -0.0039,  0.0019],\n",
            "        [-0.0037,  0.0044, -0.0077,  ..., -0.0005,  0.0102, -0.0045],\n",
            "        [-0.0010, -0.0059, -0.0138,  ...,  0.0136, -0.0045,  0.0126]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.lora_down.weight', tensor([[-0.0202,  0.0157, -0.0030,  ...,  0.0003, -0.0090,  0.0205],\n",
            "        [-0.0249,  0.0171, -0.0231,  ...,  0.0222,  0.0005,  0.0237],\n",
            "        [-0.0088, -0.0104,  0.0014,  ...,  0.0039, -0.0086,  0.0276],\n",
            "        ...,\n",
            "        [-0.0119, -0.0161, -0.0112,  ...,  0.0133,  0.0175, -0.0074],\n",
            "        [-0.0143, -0.0036, -0.0270,  ..., -0.0007,  0.0229, -0.0020],\n",
            "        [ 0.0035, -0.0138,  0.0075,  ..., -0.0020, -0.0161,  0.0201]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight', tensor([[-8.4209e-04,  3.4542e-03,  7.5951e-03,  ...,  2.0370e-02,\n",
            "          1.0323e-02,  8.1177e-03],\n",
            "        [-2.3438e-02,  2.2003e-02, -1.4931e-02,  ...,  2.1255e-02,\n",
            "         -9.1851e-05,  2.7447e-03],\n",
            "        [-2.4147e-03,  7.6981e-03,  1.1780e-02,  ..., -5.2223e-03,\n",
            "         -1.0849e-02, -2.2476e-02],\n",
            "        ...,\n",
            "        [-2.7603e-02,  2.5223e-02, -2.3727e-02,  ..., -2.1225e-02,\n",
            "          2.4246e-02,  2.7084e-03],\n",
            "        [ 2.7298e-02,  1.2947e-02,  2.6886e-02,  ..., -7.5607e-03,\n",
            "         -2.7180e-03, -1.3039e-02],\n",
            "        [-1.2779e-02,  2.5879e-02, -8.9722e-03,  ...,  1.0452e-02,\n",
            "          8.2321e-03, -2.1973e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.lora_down.weight', tensor([[-0.0239, -0.0236, -0.0025,  ..., -0.0263, -0.0059,  0.0124],\n",
            "        [-0.0165, -0.0225,  0.0144,  ..., -0.0204, -0.0046, -0.0245],\n",
            "        [-0.0058,  0.0165,  0.0212,  ...,  0.0275, -0.0167, -0.0240],\n",
            "        ...,\n",
            "        [-0.0263,  0.0178, -0.0041,  ..., -0.0061,  0.0122,  0.0086],\n",
            "        [-0.0112,  0.0163, -0.0277,  ..., -0.0213,  0.0177,  0.0276],\n",
            "        [ 0.0108,  0.0277,  0.0127,  ..., -0.0102,  0.0219, -0.0255]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.lora_down.weight', tensor([[ 0.0139, -0.0098,  0.0251,  ..., -0.0190, -0.0170, -0.0161],\n",
            "        [-0.0030,  0.0086,  0.0144,  ..., -0.0129, -0.0028,  0.0133],\n",
            "        [ 0.0126,  0.0042, -0.0122,  ..., -0.0198, -0.0129,  0.0224],\n",
            "        ...,\n",
            "        [-0.0147, -0.0162,  0.0219,  ...,  0.0255, -0.0138,  0.0252],\n",
            "        [-0.0072, -0.0069, -0.0165,  ..., -0.0139,  0.0084, -0.0097],\n",
            "        [-0.0072,  0.0084, -0.0006,  ..., -0.0157, -0.0168,  0.0072]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.lora_down.weight', tensor([[ 0.0129, -0.0117, -0.0107,  ..., -0.0121,  0.0189,  0.0207],\n",
            "        [-0.0161, -0.0210, -0.0115,  ...,  0.0063, -0.0180, -0.0014],\n",
            "        [-0.0133,  0.0117, -0.0099,  ..., -0.0057, -0.0124,  0.0035],\n",
            "        ...,\n",
            "        [-0.0203, -0.0145,  0.0077,  ...,  0.0070,  0.0210,  0.0030],\n",
            "        [-0.0070, -0.0145,  0.0013,  ...,  0.0121,  0.0097,  0.0109],\n",
            "        [-0.0201,  0.0130,  0.0024,  ..., -0.0133,  0.0038,  0.0105]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight', tensor([[-0.0020,  0.0263, -0.0228,  ...,  0.0099, -0.0081,  0.0151],\n",
            "        [-0.0210, -0.0082, -0.0133,  ...,  0.0134, -0.0059, -0.0177],\n",
            "        [ 0.0021, -0.0051,  0.0106,  ...,  0.0181, -0.0192, -0.0277],\n",
            "        ...,\n",
            "        [-0.0007,  0.0170, -0.0051,  ..., -0.0054,  0.0021, -0.0201],\n",
            "        [-0.0028,  0.0225,  0.0045,  ...,  0.0215, -0.0244, -0.0165],\n",
            "        [-0.0003, -0.0274,  0.0072,  ..., -0.0195, -0.0087,  0.0112]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.lora_down.weight', tensor([[-0.0251,  0.0238, -0.0044,  ..., -0.0113,  0.0014,  0.0169],\n",
            "        [ 0.0245,  0.0034, -0.0147,  ..., -0.0220, -0.0268,  0.0278],\n",
            "        [ 0.0028,  0.0200,  0.0059,  ...,  0.0279,  0.0050,  0.0162],\n",
            "        ...,\n",
            "        [ 0.0052, -0.0234, -0.0087,  ..., -0.0144,  0.0070,  0.0256],\n",
            "        [-0.0021,  0.0207,  0.0147,  ...,  0.0063,  0.0031,  0.0006],\n",
            "        [ 0.0247, -0.0144, -0.0089,  ..., -0.0017,  0.0215,  0.0154]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.lora_down.weight', tensor([[ 0.0153, -0.0075, -0.0155,  ...,  0.0057,  0.0212,  0.0003],\n",
            "        [-0.0098,  0.0039,  0.0103,  ...,  0.0013,  0.0220,  0.0180],\n",
            "        [ 0.0020, -0.0145, -0.0178,  ...,  0.0156,  0.0079, -0.0184],\n",
            "        ...,\n",
            "        [-0.0087, -0.0189,  0.0125,  ..., -0.0032,  0.0041,  0.0031],\n",
            "        [ 0.0151, -0.0070, -0.0047,  ..., -0.0184, -0.0133, -0.0217],\n",
            "        [-0.0167,  0.0194,  0.0059,  ...,  0.0101,  0.0164,  0.0221]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight', tensor([[ 0.0275,  0.0191, -0.0250,  ..., -0.0072, -0.0207,  0.0225],\n",
            "        [ 0.0096,  0.0178, -0.0065,  ...,  0.0230, -0.0266, -0.0007],\n",
            "        [ 0.0231, -0.0144, -0.0239,  ...,  0.0221, -0.0275,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0250,  0.0246, -0.0040,  ..., -0.0038, -0.0205, -0.0024],\n",
            "        [ 0.0112, -0.0262, -0.0118,  ...,  0.0189, -0.0145,  0.0176],\n",
            "        [ 0.0180,  0.0266,  0.0103,  ...,  0.0137, -0.0092, -0.0080]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.lora_down.weight', tensor([[-5.8327e-03,  9.4452e-03, -1.2085e-02,  ...,  3.7231e-03,\n",
            "          1.0689e-02, -1.2276e-02],\n",
            "        [ 7.3814e-03,  9.0866e-03,  2.8801e-03,  ...,  1.2634e-02,\n",
            "         -1.5888e-03, -4.3716e-03],\n",
            "        [-1.5001e-03,  9.5062e-03,  4.2534e-03,  ...,  1.3283e-02,\n",
            "         -4.0092e-03,  3.4771e-03],\n",
            "        ...,\n",
            "        [ 3.1185e-03, -1.2756e-02, -5.6381e-03,  ..., -1.1330e-02,\n",
            "          1.0979e-02,  1.3485e-03],\n",
            "        [-2.9144e-03, -3.3455e-03, -5.8708e-03,  ..., -3.6180e-05,\n",
            "         -5.3787e-03,  1.0853e-03],\n",
            "        [-1.3939e-02,  1.3380e-03, -1.2726e-02,  ..., -3.8319e-03,\n",
            "         -6.2714e-03,  1.8911e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.lora_down.weight', tensor([[ 0.0185, -0.0256, -0.0089,  ...,  0.0267, -0.0043,  0.0042],\n",
            "        [-0.0172, -0.0075,  0.0147,  ...,  0.0024, -0.0275, -0.0014],\n",
            "        [-0.0062, -0.0194, -0.0097,  ...,  0.0114, -0.0204, -0.0055],\n",
            "        ...,\n",
            "        [ 0.0059, -0.0054,  0.0070,  ..., -0.0198,  0.0101,  0.0008],\n",
            "        [ 0.0256, -0.0043, -0.0217,  ...,  0.0210, -0.0021,  0.0127],\n",
            "        [ 0.0245,  0.0125, -0.0057,  ...,  0.0122,  0.0144, -0.0008]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight', tensor([[ 0.0049,  0.0101,  0.0269,  ..., -0.0050, -0.0107, -0.0089],\n",
            "        [-0.0211, -0.0266,  0.0200,  ..., -0.0245,  0.0007, -0.0085],\n",
            "        [ 0.0164,  0.0161,  0.0191,  ..., -0.0256, -0.0182,  0.0055],\n",
            "        ...,\n",
            "        [ 0.0149,  0.0103, -0.0267,  ..., -0.0176, -0.0086, -0.0180],\n",
            "        [ 0.0183, -0.0003, -0.0152,  ..., -0.0076, -0.0249,  0.0176],\n",
            "        [ 0.0104, -0.0265, -0.0088,  ...,  0.0063, -0.0257, -0.0068]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.lora_down.weight', tensor([[ 0.0075, -0.0134, -0.0078,  ..., -0.0036, -0.0267,  0.0219],\n",
            "        [ 0.0200,  0.0273, -0.0107,  ..., -0.0212,  0.0269,  0.0029],\n",
            "        [-0.0225, -0.0187, -0.0031,  ...,  0.0060, -0.0168, -0.0120],\n",
            "        ...,\n",
            "        [-0.0214,  0.0218, -0.0145,  ...,  0.0024, -0.0083,  0.0104],\n",
            "        [ 0.0067, -0.0195,  0.0119,  ...,  0.0064,  0.0136, -0.0024],\n",
            "        [ 0.0103, -0.0024, -0.0004,  ..., -0.0099,  0.0061,  0.0239]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.lora_down.weight', tensor([[-0.0219,  0.0065,  0.0151,  ...,  0.0166,  0.0249, -0.0085],\n",
            "        [-0.0272,  0.0072,  0.0203,  ..., -0.0067,  0.0177,  0.0002],\n",
            "        [-0.0231, -0.0176, -0.0226,  ...,  0.0004,  0.0146,  0.0136],\n",
            "        ...,\n",
            "        [ 0.0098,  0.0106, -0.0262,  ...,  0.0071, -0.0276,  0.0173],\n",
            "        [-0.0074,  0.0103, -0.0053,  ...,  0.0124, -0.0049, -0.0231],\n",
            "        [-0.0068, -0.0030,  0.0101,  ...,  0.0103,  0.0174,  0.0089]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.lora_down.weight', tensor([[-0.0035, -0.0151,  0.0206,  ..., -0.0134, -0.0175, -0.0214],\n",
            "        [ 0.0144, -0.0221, -0.0183,  ..., -0.0094, -0.0119, -0.0091],\n",
            "        [ 0.0058, -0.0103, -0.0142,  ..., -0.0217, -0.0152, -0.0201],\n",
            "        ...,\n",
            "        [ 0.0065,  0.0130, -0.0151,  ..., -0.0013, -0.0173, -0.0023],\n",
            "        [ 0.0168, -0.0195,  0.0134,  ...,  0.0083,  0.0022,  0.0040],\n",
            "        [ 0.0183, -0.0006,  0.0067,  ...,  0.0062,  0.0050,  0.0067]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight', tensor([[-0.0261, -0.0126, -0.0153,  ...,  0.0157,  0.0267,  0.0262],\n",
            "        [ 0.0201,  0.0246,  0.0013,  ...,  0.0013, -0.0072,  0.0039],\n",
            "        [-0.0202,  0.0129, -0.0079,  ...,  0.0088,  0.0178, -0.0145],\n",
            "        ...,\n",
            "        [-0.0279,  0.0134,  0.0195,  ...,  0.0092,  0.0259,  0.0216],\n",
            "        [-0.0256,  0.0112,  0.0015,  ..., -0.0172, -0.0099, -0.0025],\n",
            "        [ 0.0184, -0.0209,  0.0020,  ...,  0.0201,  0.0195, -0.0272]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.lora_down.weight', tensor([[-0.0234, -0.0138, -0.0242,  ...,  0.0251,  0.0256, -0.0041],\n",
            "        [ 0.0212, -0.0117, -0.0026,  ...,  0.0080,  0.0144, -0.0276],\n",
            "        [ 0.0016, -0.0036, -0.0236,  ...,  0.0084,  0.0057,  0.0163],\n",
            "        ...,\n",
            "        [-0.0111,  0.0109,  0.0046,  ..., -0.0123, -0.0106,  0.0007],\n",
            "        [ 0.0029, -0.0155, -0.0234,  ..., -0.0256,  0.0084, -0.0003],\n",
            "        [ 0.0202,  0.0179, -0.0167,  ..., -0.0083, -0.0266,  0.0216]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.lora_down.weight', tensor([[-0.0209, -0.0093,  0.0136,  ...,  0.0093, -0.0147,  0.0204],\n",
            "        [-0.0033,  0.0183, -0.0201,  ...,  0.0178,  0.0033, -0.0032],\n",
            "        [-0.0209, -0.0088,  0.0086,  ...,  0.0186, -0.0006,  0.0145],\n",
            "        ...,\n",
            "        [-0.0120,  0.0206,  0.0032,  ..., -0.0023,  0.0035, -0.0091],\n",
            "        [-0.0110,  0.0086,  0.0073,  ...,  0.0200,  0.0076, -0.0045],\n",
            "        [-0.0176, -0.0210,  0.0136,  ..., -0.0057,  0.0200, -0.0036]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight', tensor([[-0.0076,  0.0050,  0.0152,  ...,  0.0256, -0.0224,  0.0059],\n",
            "        [-0.0031,  0.0022, -0.0251,  ..., -0.0010,  0.0267,  0.0251],\n",
            "        [ 0.0033,  0.0045, -0.0194,  ..., -0.0137, -0.0205, -0.0144],\n",
            "        ...,\n",
            "        [ 0.0251,  0.0278, -0.0093,  ..., -0.0189, -0.0171,  0.0241],\n",
            "        [-0.0144, -0.0226, -0.0005,  ..., -0.0159,  0.0081,  0.0174],\n",
            "        [ 0.0128,  0.0095, -0.0102,  ..., -0.0108,  0.0122, -0.0198]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.lora_down.weight', tensor([[ 0.0062, -0.0014, -0.0074,  ..., -0.0067, -0.0029, -0.0133],\n",
            "        [-0.0097,  0.0003, -0.0032,  ..., -0.0075,  0.0020, -0.0052],\n",
            "        [ 0.0018,  0.0123, -0.0122,  ...,  0.0085,  0.0001, -0.0032],\n",
            "        ...,\n",
            "        [ 0.0113,  0.0040,  0.0104,  ...,  0.0058,  0.0075,  0.0083],\n",
            "        [-0.0047, -0.0100, -0.0010,  ...,  0.0093,  0.0088, -0.0034],\n",
            "        [-0.0007, -0.0133,  0.0021,  ...,  0.0034, -0.0134,  0.0117]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.lora_down.weight', tensor([[ 0.0217,  0.0137, -0.0084,  ...,  0.0218,  0.0146, -0.0157],\n",
            "        [ 0.0063,  0.0236, -0.0142,  ..., -0.0053,  0.0014,  0.0061],\n",
            "        [-0.0278, -0.0221, -0.0259,  ..., -0.0234,  0.0038,  0.0211],\n",
            "        ...,\n",
            "        [-0.0030, -0.0278, -0.0079,  ..., -0.0004,  0.0143,  0.0045],\n",
            "        [ 0.0147, -0.0253,  0.0186,  ...,  0.0059,  0.0058, -0.0066],\n",
            "        [-0.0106, -0.0243,  0.0202,  ...,  0.0071, -0.0012,  0.0124]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight', tensor([[ 0.0146, -0.0084,  0.0068,  ...,  0.0143, -0.0190,  0.0169],\n",
            "        [ 0.0186,  0.0023, -0.0045,  ...,  0.0145,  0.0120,  0.0090],\n",
            "        [-0.0101,  0.0184,  0.0124,  ..., -0.0086,  0.0201, -0.0232],\n",
            "        ...,\n",
            "        [ 0.0263, -0.0042,  0.0275,  ..., -0.0149,  0.0081,  0.0067],\n",
            "        [-0.0079,  0.0244, -0.0275,  ...,  0.0170, -0.0164,  0.0179],\n",
            "        [-0.0108,  0.0275,  0.0081,  ...,  0.0195,  0.0091, -0.0027]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.lora_down.weight', tensor([[ 0.0100,  0.0097, -0.0029,  ...,  0.0107, -0.0072, -0.0203],\n",
            "        [ 0.0012, -0.0279,  0.0094,  ...,  0.0275,  0.0014,  0.0122],\n",
            "        [-0.0180, -0.0247,  0.0265,  ...,  0.0198, -0.0167,  0.0172],\n",
            "        ...,\n",
            "        [ 0.0097, -0.0188, -0.0087,  ...,  0.0083,  0.0060, -0.0045],\n",
            "        [-0.0072,  0.0198,  0.0132,  ..., -0.0016, -0.0060, -0.0252],\n",
            "        [-0.0001,  0.0202,  0.0041,  ..., -0.0020, -0.0085, -0.0117]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.lora_down.weight', tensor([[ 0.0113,  0.0243, -0.0252,  ...,  0.0255,  0.0031,  0.0201],\n",
            "        [-0.0063, -0.0241,  0.0039,  ...,  0.0010,  0.0133, -0.0204],\n",
            "        [ 0.0098,  0.0009,  0.0017,  ...,  0.0229, -0.0184, -0.0046],\n",
            "        ...,\n",
            "        [-0.0252, -0.0125,  0.0091,  ...,  0.0256, -0.0067, -0.0070],\n",
            "        [-0.0174,  0.0212,  0.0185,  ...,  0.0170,  0.0050, -0.0038],\n",
            "        [-0.0031, -0.0160,  0.0069,  ..., -0.0146, -0.0261, -0.0130]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.lora_down.weight', tensor([[-0.0040,  0.0096,  0.0112,  ..., -0.0088, -0.0206, -0.0032],\n",
            "        [-0.0014, -0.0152,  0.0081,  ...,  0.0151, -0.0204,  0.0172],\n",
            "        [-0.0072, -0.0089,  0.0047,  ..., -0.0010, -0.0009,  0.0126],\n",
            "        ...,\n",
            "        [-0.0042, -0.0103,  0.0144,  ...,  0.0128,  0.0177,  0.0191],\n",
            "        [-0.0065, -0.0116,  0.0096,  ..., -0.0139,  0.0043, -0.0143],\n",
            "        [-0.0115,  0.0166, -0.0049,  ..., -0.0051, -0.0028, -0.0006]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight', tensor([[ 0.0129, -0.0190,  0.0254,  ..., -0.0091,  0.0128, -0.0130],\n",
            "        [ 0.0266,  0.0273, -0.0164,  ...,  0.0126, -0.0024,  0.0258],\n",
            "        [ 0.0182, -0.0013,  0.0164,  ...,  0.0247, -0.0166,  0.0193],\n",
            "        ...,\n",
            "        [ 0.0175,  0.0007, -0.0034,  ...,  0.0066,  0.0124, -0.0011],\n",
            "        [-0.0037,  0.0173, -0.0215,  ...,  0.0023, -0.0241, -0.0261],\n",
            "        [ 0.0049,  0.0017, -0.0274,  ...,  0.0168,  0.0093, -0.0077]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.lora_down.weight', tensor([[ 0.0257,  0.0011, -0.0034,  ..., -0.0086,  0.0055,  0.0133],\n",
            "        [ 0.0235,  0.0157,  0.0053,  ..., -0.0060,  0.0079, -0.0009],\n",
            "        [-0.0065,  0.0200,  0.0066,  ..., -0.0097, -0.0168, -0.0116],\n",
            "        ...,\n",
            "        [-0.0266,  0.0029, -0.0104,  ..., -0.0092,  0.0094, -0.0222],\n",
            "        [ 0.0220,  0.0164, -0.0268,  ..., -0.0244,  0.0083, -0.0246],\n",
            "        [-0.0082, -0.0172,  0.0014,  ...,  0.0097,  0.0211, -0.0095]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.lora_down.weight', tensor([[-0.0039, -0.0079,  0.0024,  ..., -0.0039,  0.0070, -0.0085],\n",
            "        [-0.0049, -0.0091, -0.0128,  ..., -0.0192, -0.0120,  0.0174],\n",
            "        [-0.0100,  0.0189, -0.0164,  ..., -0.0134, -0.0098, -0.0202],\n",
            "        ...,\n",
            "        [-0.0120,  0.0129,  0.0125,  ...,  0.0075, -0.0028, -0.0133],\n",
            "        [-0.0100, -0.0018,  0.0123,  ...,  0.0152,  0.0106,  0.0076],\n",
            "        [ 0.0199, -0.0157, -0.0093,  ..., -0.0182, -0.0174, -0.0020]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight', tensor([[-0.0257,  0.0240, -0.0093,  ...,  0.0118, -0.0080, -0.0139],\n",
            "        [-0.0033, -0.0165,  0.0128,  ...,  0.0138,  0.0035, -0.0214],\n",
            "        [-0.0111, -0.0144,  0.0184,  ..., -0.0090, -0.0140, -0.0205],\n",
            "        ...,\n",
            "        [ 0.0267,  0.0088, -0.0074,  ...,  0.0253, -0.0005,  0.0194],\n",
            "        [-0.0249, -0.0124, -0.0186,  ..., -0.0060,  0.0239, -0.0257],\n",
            "        [-0.0062,  0.0198, -0.0134,  ...,  0.0182,  0.0162, -0.0127]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.lora_down.weight', tensor([[ 0.0063, -0.0013, -0.0092,  ...,  0.0130, -0.0061, -0.0024],\n",
            "        [ 0.0124,  0.0059, -0.0077,  ...,  0.0065, -0.0138,  0.0028],\n",
            "        [-0.0113,  0.0080, -0.0061,  ...,  0.0036, -0.0040, -0.0126],\n",
            "        ...,\n",
            "        [-0.0091, -0.0086, -0.0028,  ..., -0.0039, -0.0108,  0.0126],\n",
            "        [ 0.0125, -0.0138, -0.0082,  ..., -0.0122,  0.0122, -0.0019],\n",
            "        [ 0.0056, -0.0131, -0.0073,  ..., -0.0103, -0.0043, -0.0091]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_proj_in.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_proj_in.lora_down.weight', tensor([[ 0.0128,  0.0012,  0.0125,  ..., -0.0207,  0.0233,  0.0209],\n",
            "        [ 0.0276, -0.0046,  0.0162,  ...,  0.0233, -0.0198,  0.0134],\n",
            "        [-0.0165,  0.0258,  0.0084,  ..., -0.0066,  0.0056,  0.0166],\n",
            "        ...,\n",
            "        [ 0.0035, -0.0030, -0.0199,  ..., -0.0173,  0.0252,  0.0073],\n",
            "        [ 0.0061,  0.0267, -0.0046,  ...,  0.0010, -0.0205, -0.0098],\n",
            "        [-0.0114, -0.0078, -0.0110,  ..., -0.0242,  0.0054,  0.0224]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_proj_in.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_proj_out.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_proj_out.lora_down.weight', tensor([[-0.0276, -0.0090, -0.0214,  ..., -0.0148, -0.0116,  0.0279],\n",
            "        [-0.0276,  0.0011, -0.0102,  ...,  0.0274,  0.0215, -0.0164],\n",
            "        [ 0.0104, -0.0214, -0.0041,  ..., -0.0211,  0.0227,  0.0266],\n",
            "        ...,\n",
            "        [-0.0150,  0.0103,  0.0266,  ...,  0.0166, -0.0215, -0.0081],\n",
            "        [ 0.0063,  0.0003, -0.0252,  ..., -0.0241, -0.0107,  0.0131],\n",
            "        [-0.0106, -0.0035, -0.0133,  ..., -0.0086, -0.0022,  0.0203]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_proj_out.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.lora_down.weight', tensor([[ 0.0181, -0.0078,  0.0190,  ..., -0.0279, -0.0107, -0.0218],\n",
            "        [ 0.0091, -0.0278, -0.0124,  ..., -0.0238,  0.0274,  0.0241],\n",
            "        [ 0.0090,  0.0037, -0.0262,  ...,  0.0236,  0.0009, -0.0134],\n",
            "        ...,\n",
            "        [-0.0124, -0.0147, -0.0012,  ...,  0.0023,  0.0242, -0.0264],\n",
            "        [-0.0172, -0.0257,  0.0018,  ..., -0.0138,  0.0052, -0.0128],\n",
            "        [-0.0020, -0.0099,  0.0038,  ..., -0.0046,  0.0107,  0.0070]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight', tensor([[-0.0244,  0.0043, -0.0162,  ..., -0.0017,  0.0223,  0.0172],\n",
            "        [-0.0122,  0.0069, -0.0057,  ..., -0.0265, -0.0055, -0.0167],\n",
            "        [-0.0270, -0.0263, -0.0043,  ...,  0.0066,  0.0064, -0.0060],\n",
            "        ...,\n",
            "        [ 0.0205, -0.0260,  0.0061,  ...,  0.0265,  0.0001, -0.0157],\n",
            "        [-0.0034,  0.0187, -0.0157,  ..., -0.0080,  0.0064,  0.0098],\n",
            "        [-0.0082,  0.0194,  0.0115,  ..., -0.0135, -0.0105, -0.0267]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.lora_down.weight', tensor([[ 0.0214,  0.0025,  0.0007,  ..., -0.0267, -0.0091, -0.0156],\n",
            "        [-0.0098, -0.0096,  0.0258,  ...,  0.0019, -0.0132, -0.0042],\n",
            "        [-0.0043,  0.0178, -0.0055,  ..., -0.0158,  0.0183,  0.0069],\n",
            "        ...,\n",
            "        [ 0.0197, -0.0257, -0.0265,  ...,  0.0248,  0.0158,  0.0112],\n",
            "        [ 0.0116,  0.0063, -0.0145,  ...,  0.0092,  0.0124, -0.0265],\n",
            "        [ 0.0035, -0.0023, -0.0108,  ...,  0.0173, -0.0146,  0.0052]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.lora_down.weight', tensor([[ 0.0058, -0.0231, -0.0120,  ...,  0.0146, -0.0216, -0.0025],\n",
            "        [ 0.0131, -0.0086,  0.0073,  ..., -0.0029, -0.0226, -0.0262],\n",
            "        [ 0.0188, -0.0116, -0.0085,  ...,  0.0192, -0.0174, -0.0085],\n",
            "        ...,\n",
            "        [ 0.0034,  0.0055, -0.0088,  ..., -0.0088,  0.0219,  0.0187],\n",
            "        [-0.0231,  0.0017, -0.0089,  ...,  0.0205, -0.0261,  0.0121],\n",
            "        [ 0.0073,  0.0253,  0.0278,  ..., -0.0090, -0.0159, -0.0124]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.lora_down.weight', tensor([[ 0.0204, -0.0167, -0.0055,  ...,  0.0186, -0.0098,  0.0170],\n",
            "        [ 0.0147,  0.0138,  0.0189,  ...,  0.0015, -0.0123, -0.0091],\n",
            "        [ 0.0203,  0.0216, -0.0007,  ...,  0.0156,  0.0206, -0.0136],\n",
            "        ...,\n",
            "        [ 0.0080,  0.0098,  0.0034,  ..., -0.0138, -0.0209,  0.0147],\n",
            "        [-0.0159, -0.0004,  0.0206,  ..., -0.0110,  0.0155,  0.0135],\n",
            "        [-0.0010,  0.0162, -0.0150,  ...,  0.0078, -0.0024,  0.0179]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight', tensor([[ 0.0111,  0.0026, -0.0225,  ..., -0.0128, -0.0089, -0.0092],\n",
            "        [ 0.0033, -0.0235,  0.0239,  ..., -0.0105, -0.0065,  0.0027],\n",
            "        [-0.0222, -0.0041,  0.0029,  ...,  0.0195, -0.0048, -0.0155],\n",
            "        ...,\n",
            "        [ 0.0094,  0.0134,  0.0013,  ...,  0.0200,  0.0234, -0.0087],\n",
            "        [ 0.0142,  0.0091, -0.0113,  ...,  0.0062,  0.0171, -0.0012],\n",
            "        [-0.0008, -0.0135, -0.0120,  ...,  0.0014,  0.0063,  0.0140]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.lora_down.weight', tensor([[-0.0140, -0.0026,  0.0129,  ..., -0.0241,  0.0205,  0.0110],\n",
            "        [ 0.0035, -0.0101,  0.0035,  ...,  0.0123,  0.0142,  0.0056],\n",
            "        [-0.0265, -0.0188,  0.0231,  ..., -0.0193,  0.0221, -0.0264],\n",
            "        ...,\n",
            "        [ 0.0027,  0.0254,  0.0045,  ...,  0.0220,  0.0147, -0.0116],\n",
            "        [ 0.0136,  0.0151, -0.0129,  ..., -0.0145,  0.0181,  0.0035],\n",
            "        [ 0.0043, -0.0015,  0.0110,  ...,  0.0154,  0.0205,  0.0039]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.lora_down.weight', tensor([[ 0.0128, -0.0056,  0.0157,  ..., -0.0140,  0.0154,  0.0217],\n",
            "        [ 0.0067, -0.0195,  0.0159,  ...,  0.0157, -0.0068, -0.0119],\n",
            "        [ 0.0063,  0.0072,  0.0143,  ..., -0.0069,  0.0035,  0.0087],\n",
            "        ...,\n",
            "        [ 0.0201,  0.0106,  0.0140,  ...,  0.0088,  0.0067,  0.0031],\n",
            "        [ 0.0175,  0.0095, -0.0083,  ..., -0.0122, -0.0132,  0.0165],\n",
            "        [ 0.0033, -0.0160,  0.0078,  ...,  0.0143,  0.0055,  0.0151]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight', tensor([[ 0.0033, -0.0123,  0.0081,  ..., -0.0040,  0.0167, -0.0147],\n",
            "        [-0.0166,  0.0135, -0.0246,  ..., -0.0261, -0.0034,  0.0031],\n",
            "        [ 0.0218,  0.0012,  0.0028,  ...,  0.0086, -0.0041,  0.0018],\n",
            "        ...,\n",
            "        [-0.0146,  0.0123,  0.0105,  ...,  0.0274,  0.0260,  0.0202],\n",
            "        [ 0.0274,  0.0088,  0.0079,  ..., -0.0142, -0.0216, -0.0260],\n",
            "        [ 0.0185,  0.0001, -0.0103,  ...,  0.0013,  0.0100, -0.0093]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.lora_down.weight', tensor([[-0.0101, -0.0045, -0.0110,  ...,  0.0105, -0.0023, -0.0004],\n",
            "        [-0.0053, -0.0077,  0.0117,  ..., -0.0010,  0.0091,  0.0138],\n",
            "        [-0.0085, -0.0135, -0.0095,  ..., -0.0090,  0.0079,  0.0054],\n",
            "        ...,\n",
            "        [-0.0113, -0.0075,  0.0089,  ..., -0.0138, -0.0059,  0.0051],\n",
            "        [-0.0079, -0.0116,  0.0012,  ..., -0.0114,  0.0006,  0.0004],\n",
            "        [ 0.0030, -0.0132, -0.0102,  ...,  0.0059,  0.0021,  0.0010]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.lora_down.weight', tensor([[-2.3102e-02,  1.5289e-02, -4.4632e-03,  ...,  6.0158e-03,\n",
            "          9.4681e-03, -1.7181e-02],\n",
            "        [ 1.0254e-02,  1.2047e-02,  1.7557e-03,  ..., -2.4872e-02,\n",
            "          2.1225e-02,  8.3804e-05],\n",
            "        [-1.6296e-02, -1.8188e-02,  2.4376e-03,  ...,  2.0447e-02,\n",
            "          1.1955e-02, -2.7161e-02],\n",
            "        ...,\n",
            "        [-6.9771e-03, -1.3290e-02, -4.3983e-03,  ..., -2.3232e-03,\n",
            "         -1.6144e-02, -5.3101e-03],\n",
            "        [ 1.6434e-02,  2.1118e-02,  7.5417e-03,  ..., -1.0086e-02,\n",
            "         -2.0508e-02, -1.1642e-02],\n",
            "        [ 1.3779e-02, -1.9623e-02, -8.0566e-03,  ...,  2.3453e-02,\n",
            "         -8.8120e-03,  7.4959e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight', tensor([[-0.0127,  0.0257,  0.0052,  ...,  0.0091,  0.0222,  0.0140],\n",
            "        [ 0.0040,  0.0055,  0.0158,  ...,  0.0099,  0.0147, -0.0040],\n",
            "        [ 0.0154,  0.0228,  0.0088,  ..., -0.0143,  0.0151, -0.0206],\n",
            "        ...,\n",
            "        [ 0.0161, -0.0120,  0.0177,  ..., -0.0241, -0.0261, -0.0125],\n",
            "        [ 0.0122, -0.0238,  0.0069,  ..., -0.0088, -0.0086, -0.0225],\n",
            "        [ 0.0119,  0.0114, -0.0153,  ...,  0.0041, -0.0127,  0.0264]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.lora_down.weight', tensor([[ 0.0032,  0.0141,  0.0037,  ..., -0.0009,  0.0214,  0.0103],\n",
            "        [ 0.0271,  0.0192, -0.0267,  ...,  0.0241,  0.0222, -0.0278],\n",
            "        [ 0.0254, -0.0019, -0.0256,  ...,  0.0126,  0.0120, -0.0132],\n",
            "        ...,\n",
            "        [ 0.0275, -0.0183,  0.0122,  ..., -0.0133,  0.0101,  0.0170],\n",
            "        [-0.0175,  0.0035,  0.0235,  ..., -0.0224,  0.0182,  0.0084],\n",
            "        [ 0.0119, -0.0240,  0.0188,  ..., -0.0084,  0.0080,  0.0270]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.lora_down.weight', tensor([[-0.0235,  0.0274,  0.0013,  ...,  0.0231,  0.0097,  0.0174],\n",
            "        [-0.0178, -0.0081, -0.0195,  ..., -0.0132,  0.0036,  0.0214],\n",
            "        [-0.0165,  0.0094, -0.0009,  ...,  0.0052,  0.0204,  0.0013],\n",
            "        ...,\n",
            "        [-0.0084,  0.0044,  0.0241,  ...,  0.0058,  0.0240, -0.0121],\n",
            "        [ 0.0205,  0.0207,  0.0209,  ...,  0.0137, -0.0116, -0.0113],\n",
            "        [-0.0250,  0.0046, -0.0266,  ...,  0.0110, -0.0164,  0.0251]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.lora_down.weight', tensor([[-0.0099, -0.0216, -0.0107,  ..., -0.0011, -0.0212,  0.0015],\n",
            "        [ 0.0022, -0.0128,  0.0056,  ..., -0.0061, -0.0075, -0.0128],\n",
            "        [-0.0147,  0.0043,  0.0093,  ..., -0.0069, -0.0013,  0.0172],\n",
            "        ...,\n",
            "        [ 0.0066,  0.0018, -0.0175,  ..., -0.0202,  0.0024, -0.0180],\n",
            "        [ 0.0171, -0.0060, -0.0086,  ..., -0.0185,  0.0154,  0.0161],\n",
            "        [-0.0026, -0.0110, -0.0170,  ..., -0.0197, -0.0186, -0.0149]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight', tensor([[-6.2218e-03, -1.8234e-02,  4.1428e-03,  ..., -2.0187e-02,\n",
            "          1.3266e-03,  2.0981e-02],\n",
            "        [-1.0307e-02,  9.0561e-03,  1.5945e-02,  ..., -8.7662e-03,\n",
            "          9.8801e-03, -1.4626e-02],\n",
            "        [ 2.6535e-02, -2.0248e-02, -7.6332e-03,  ...,  1.3039e-02,\n",
            "         -2.4612e-02,  1.6907e-02],\n",
            "        ...,\n",
            "        [-8.7380e-05, -9.6893e-03, -1.7822e-02,  ...,  2.2827e-02,\n",
            "          2.7442e-04, -9.7351e-03],\n",
            "        [ 9.1019e-03, -2.2446e-02, -1.7195e-03,  ...,  1.1421e-02,\n",
            "          9.7275e-03,  8.5373e-03],\n",
            "        [-2.5463e-03,  2.6001e-02,  1.6632e-02,  ...,  2.1179e-02,\n",
            "         -9.3079e-03,  3.2616e-04]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.lora_down.weight', tensor([[ 0.0210,  0.0206, -0.0033,  ..., -0.0116, -0.0156,  0.0211],\n",
            "        [-0.0196, -0.0132,  0.0008,  ...,  0.0084,  0.0137, -0.0240],\n",
            "        [-0.0084,  0.0021, -0.0006,  ...,  0.0130, -0.0140,  0.0005],\n",
            "        ...,\n",
            "        [-0.0206,  0.0013, -0.0087,  ..., -0.0053, -0.0218,  0.0228],\n",
            "        [ 0.0276, -0.0191,  0.0053,  ...,  0.0197, -0.0254,  0.0192],\n",
            "        [-0.0236,  0.0073,  0.0225,  ...,  0.0199,  0.0013,  0.0034]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.lora_down.weight', tensor([[-0.0087,  0.0112,  0.0055,  ..., -0.0111,  0.0165,  0.0218],\n",
            "        [ 0.0031,  0.0209,  0.0074,  ..., -0.0037,  0.0144,  0.0021],\n",
            "        [ 0.0126, -0.0189, -0.0024,  ..., -0.0117,  0.0029,  0.0157],\n",
            "        ...,\n",
            "        [ 0.0001, -0.0135,  0.0009,  ..., -0.0202, -0.0192, -0.0137],\n",
            "        [ 0.0163,  0.0205,  0.0049,  ..., -0.0212, -0.0027, -0.0007],\n",
            "        [-0.0037,  0.0165,  0.0132,  ..., -0.0081,  0.0219, -0.0189]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight', tensor([[-0.0062, -0.0209, -0.0154,  ...,  0.0027,  0.0249,  0.0071],\n",
            "        [ 0.0218,  0.0134, -0.0120,  ...,  0.0067,  0.0117,  0.0223],\n",
            "        [ 0.0276,  0.0124,  0.0085,  ..., -0.0101, -0.0259,  0.0014],\n",
            "        ...,\n",
            "        [-0.0278, -0.0157,  0.0108,  ..., -0.0151, -0.0255, -0.0110],\n",
            "        [-0.0069,  0.0106, -0.0179,  ..., -0.0202, -0.0209,  0.0190],\n",
            "        [ 0.0016,  0.0070,  0.0275,  ...,  0.0035, -0.0026, -0.0205]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.lora_down.weight', tensor([[ 0.0104, -0.0067,  0.0049,  ..., -0.0132, -0.0025, -0.0133],\n",
            "        [ 0.0041, -0.0104,  0.0125,  ...,  0.0118,  0.0083, -0.0072],\n",
            "        [-0.0083, -0.0011, -0.0016,  ...,  0.0032, -0.0088,  0.0047],\n",
            "        ...,\n",
            "        [-0.0097, -0.0085,  0.0052,  ..., -0.0138,  0.0112, -0.0112],\n",
            "        [-0.0064, -0.0074,  0.0024,  ...,  0.0064,  0.0117,  0.0006],\n",
            "        [-0.0036, -0.0082,  0.0049,  ..., -0.0013, -0.0133, -0.0104]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.lora_down.weight', tensor([[-0.0189,  0.0117, -0.0236,  ..., -0.0241, -0.0021, -0.0190],\n",
            "        [ 0.0166,  0.0173, -0.0235,  ..., -0.0029, -0.0053,  0.0225],\n",
            "        [-0.0119, -0.0139, -0.0278,  ..., -0.0160, -0.0111, -0.0047],\n",
            "        ...,\n",
            "        [ 0.0063,  0.0183, -0.0276,  ...,  0.0230, -0.0156,  0.0089],\n",
            "        [-0.0105,  0.0049, -0.0206,  ..., -0.0049, -0.0103, -0.0199],\n",
            "        [ 0.0246, -0.0041,  0.0042,  ..., -0.0253,  0.0210, -0.0133]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight', tensor([[ 0.0176,  0.0068, -0.0076,  ...,  0.0227, -0.0116,  0.0254],\n",
            "        [ 0.0157,  0.0068, -0.0222,  ..., -0.0161, -0.0097,  0.0100],\n",
            "        [ 0.0005,  0.0032, -0.0108,  ..., -0.0102,  0.0022, -0.0026],\n",
            "        ...,\n",
            "        [-0.0065, -0.0096, -0.0266,  ..., -0.0200, -0.0159,  0.0265],\n",
            "        [-0.0101,  0.0099, -0.0114,  ..., -0.0158,  0.0032, -0.0062],\n",
            "        [ 0.0034,  0.0003,  0.0050,  ...,  0.0035, -0.0118,  0.0208]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.lora_down.weight', tensor([[ 0.0086,  0.0248, -0.0076,  ...,  0.0269, -0.0074, -0.0030],\n",
            "        [ 0.0198, -0.0105, -0.0122,  ..., -0.0218,  0.0267,  0.0081],\n",
            "        [ 0.0167, -0.0224, -0.0023,  ..., -0.0152, -0.0264, -0.0107],\n",
            "        ...,\n",
            "        [-0.0117, -0.0199, -0.0197,  ...,  0.0120,  0.0167, -0.0061],\n",
            "        [ 0.0241, -0.0154, -0.0010,  ...,  0.0244,  0.0228,  0.0179],\n",
            "        [ 0.0251,  0.0041,  0.0058,  ..., -0.0098,  0.0040,  0.0211]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.lora_down.weight', tensor([[-0.0269, -0.0019,  0.0082,  ..., -0.0213, -0.0123,  0.0103],\n",
            "        [-0.0232,  0.0200, -0.0127,  ..., -0.0035, -0.0175,  0.0213],\n",
            "        [-0.0260, -0.0088, -0.0187,  ..., -0.0227, -0.0158, -0.0197],\n",
            "        ...,\n",
            "        [-0.0131,  0.0208,  0.0192,  ..., -0.0175, -0.0123,  0.0160],\n",
            "        [ 0.0113, -0.0213,  0.0207,  ..., -0.0066,  0.0250,  0.0244],\n",
            "        [-0.0107,  0.0119, -0.0117,  ..., -0.0150,  0.0114,  0.0160]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.lora_down.weight', tensor([[-0.0172, -0.0105,  0.0020,  ...,  0.0043,  0.0032,  0.0035],\n",
            "        [-0.0073, -0.0132, -0.0096,  ...,  0.0092,  0.0021, -0.0220],\n",
            "        [ 0.0026, -0.0011, -0.0059,  ..., -0.0030, -0.0125,  0.0176],\n",
            "        ...,\n",
            "        [ 0.0075,  0.0197,  0.0089,  ...,  0.0161, -0.0195, -0.0094],\n",
            "        [ 0.0004, -0.0050, -0.0081,  ..., -0.0106,  0.0049, -0.0055],\n",
            "        [ 0.0148,  0.0152,  0.0142,  ...,  0.0102, -0.0029, -0.0112]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight', tensor([[-0.0223, -0.0199, -0.0246,  ..., -0.0229, -0.0084, -0.0030],\n",
            "        [ 0.0123, -0.0261, -0.0171,  ...,  0.0092, -0.0142, -0.0127],\n",
            "        [ 0.0079,  0.0064,  0.0222,  ...,  0.0064,  0.0061,  0.0237],\n",
            "        ...,\n",
            "        [-0.0064,  0.0090, -0.0116,  ...,  0.0255, -0.0024,  0.0009],\n",
            "        [-0.0064, -0.0118, -0.0030,  ..., -0.0222, -0.0207,  0.0033],\n",
            "        [-0.0176,  0.0117,  0.0193,  ...,  0.0235, -0.0086,  0.0237]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.lora_down.weight', tensor([[ 0.0224, -0.0080,  0.0071,  ...,  0.0167,  0.0067, -0.0209],\n",
            "        [ 0.0261,  0.0167, -0.0032,  ...,  0.0028, -0.0071, -0.0063],\n",
            "        [ 0.0089, -0.0122,  0.0009,  ...,  0.0229, -0.0094,  0.0166],\n",
            "        ...,\n",
            "        [-0.0240, -0.0149,  0.0174,  ..., -0.0128,  0.0240,  0.0224],\n",
            "        [ 0.0109, -0.0109, -0.0265,  ...,  0.0248,  0.0018,  0.0205],\n",
            "        [-0.0227, -0.0184, -0.0150,  ...,  0.0095,  0.0149, -0.0215]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.lora_down.weight', tensor([[-0.0174, -0.0213, -0.0006,  ..., -0.0027,  0.0139,  0.0047],\n",
            "        [ 0.0089,  0.0187, -0.0182,  ..., -0.0114,  0.0096,  0.0035],\n",
            "        [-0.0106,  0.0053,  0.0168,  ...,  0.0098, -0.0104,  0.0107],\n",
            "        ...,\n",
            "        [-0.0104, -0.0099,  0.0212,  ..., -0.0021, -0.0193,  0.0004],\n",
            "        [-0.0072, -0.0128, -0.0024,  ..., -0.0089, -0.0027,  0.0127],\n",
            "        [ 0.0197, -0.0192, -0.0084,  ..., -0.0189, -0.0131, -0.0087]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight', tensor([[-1.9226e-02,  6.4731e-05, -2.7905e-03,  ...,  2.2263e-02,\n",
            "         -2.6123e-02,  1.4046e-02],\n",
            "        [-6.5880e-03,  1.6861e-02, -2.3682e-02,  ..., -2.6840e-02,\n",
            "         -1.1833e-02,  2.1011e-02],\n",
            "        [ 8.1100e-03, -1.8478e-02,  1.6165e-03,  ...,  1.5961e-02,\n",
            "         -4.0321e-03,  1.5297e-02],\n",
            "        ...,\n",
            "        [ 8.3084e-03, -6.3324e-03,  2.0447e-02,  ..., -2.1194e-02,\n",
            "          1.6708e-02,  8.4915e-03],\n",
            "        [ 8.8739e-04, -1.2550e-02,  2.2293e-02,  ..., -2.0935e-02,\n",
            "         -1.5244e-02,  1.7090e-02],\n",
            "        [ 2.3300e-02,  7.4806e-03, -2.3666e-02,  ...,  1.7151e-02,\n",
            "         -2.7981e-03, -6.8426e-04]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.lora_down.weight', tensor([[-0.0062,  0.0115, -0.0026,  ..., -0.0068,  0.0062, -0.0048],\n",
            "        [-0.0067,  0.0056, -0.0125,  ..., -0.0013, -0.0039, -0.0078],\n",
            "        [-0.0013,  0.0035,  0.0060,  ...,  0.0062,  0.0044,  0.0124],\n",
            "        ...,\n",
            "        [-0.0046,  0.0023,  0.0139,  ...,  0.0004, -0.0057, -0.0054],\n",
            "        [-0.0026, -0.0076, -0.0075,  ..., -0.0134, -0.0098, -0.0112],\n",
            "        [-0.0090,  0.0064,  0.0089,  ...,  0.0010,  0.0097, -0.0015]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.lora_down.weight', tensor([[ 0.0186, -0.0113, -0.0123,  ...,  0.0271, -0.0172,  0.0075],\n",
            "        [ 0.0215, -0.0131, -0.0219,  ..., -0.0178, -0.0004, -0.0118],\n",
            "        [ 0.0050,  0.0275, -0.0238,  ..., -0.0257,  0.0117, -0.0235],\n",
            "        ...,\n",
            "        [ 0.0156, -0.0169,  0.0141,  ..., -0.0170,  0.0092,  0.0092],\n",
            "        [ 0.0151,  0.0136,  0.0036,  ...,  0.0148,  0.0029, -0.0104],\n",
            "        [-0.0188, -0.0208,  0.0055,  ..., -0.0271,  0.0226, -0.0098]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight', tensor([[-0.0192,  0.0231,  0.0182,  ..., -0.0167,  0.0041, -0.0158],\n",
            "        [-0.0138, -0.0234, -0.0242,  ...,  0.0274, -0.0231, -0.0064],\n",
            "        [ 0.0163,  0.0018, -0.0260,  ..., -0.0224,  0.0179, -0.0204],\n",
            "        ...,\n",
            "        [-0.0006, -0.0240,  0.0056,  ...,  0.0141,  0.0049, -0.0275],\n",
            "        [ 0.0102,  0.0135,  0.0151,  ...,  0.0117,  0.0201,  0.0264],\n",
            "        [ 0.0266,  0.0059, -0.0132,  ..., -0.0003,  0.0233, -0.0129]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.lora_down.weight', tensor([[ 4.9896e-03,  1.2878e-02, -1.6068e-02,  ..., -5.9509e-03,\n",
            "          2.2049e-02, -1.6464e-02],\n",
            "        [-1.0788e-02,  1.8768e-02, -2.2858e-02,  ...,  9.4833e-03,\n",
            "          6.5689e-03, -1.9287e-02],\n",
            "        [ 2.4628e-02,  5.1141e-05,  2.6871e-02,  ..., -3.2177e-03,\n",
            "         -9.5673e-03, -1.7792e-02],\n",
            "        ...,\n",
            "        [-3.5210e-03,  5.4588e-03, -1.4842e-05,  ...,  1.9943e-02,\n",
            "          4.6539e-03,  1.2428e-02],\n",
            "        [ 2.4155e-02, -5.5580e-03,  2.3682e-02,  ..., -8.6441e-03,\n",
            "         -4.9362e-03, -5.9280e-03],\n",
            "        [ 1.5427e-02,  1.4069e-02, -8.1711e-03,  ..., -7.6828e-03,\n",
            "         -1.4641e-02, -9.4070e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.lora_down.weight', tensor([[ 0.0033,  0.0271,  0.0150,  ...,  0.0198,  0.0021, -0.0242],\n",
            "        [-0.0149,  0.0193, -0.0183,  ...,  0.0106,  0.0015,  0.0060],\n",
            "        [ 0.0108, -0.0207,  0.0177,  ...,  0.0161,  0.0154,  0.0262],\n",
            "        ...,\n",
            "        [-0.0027,  0.0206,  0.0068,  ...,  0.0077,  0.0274,  0.0255],\n",
            "        [ 0.0275,  0.0092, -0.0102,  ..., -0.0241,  0.0157, -0.0024],\n",
            "        [-0.0217,  0.0011, -0.0123,  ...,  0.0112, -0.0097,  0.0012]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.lora_down.weight', tensor([[-0.0053,  0.0057, -0.0095,  ..., -0.0001,  0.0024, -0.0202],\n",
            "        [ 0.0070, -0.0093,  0.0145,  ..., -0.0163,  0.0154, -0.0061],\n",
            "        [ 0.0043, -0.0084,  0.0162,  ..., -0.0138, -0.0144,  0.0102],\n",
            "        ...,\n",
            "        [-0.0132, -0.0167,  0.0036,  ...,  0.0020, -0.0125, -0.0071],\n",
            "        [ 0.0012, -0.0052,  0.0151,  ..., -0.0120,  0.0107, -0.0096],\n",
            "        [-0.0171, -0.0139,  0.0178,  ...,  0.0119,  0.0087, -0.0132]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight', tensor([[-0.0263,  0.0258, -0.0261,  ...,  0.0143,  0.0102, -0.0056],\n",
            "        [ 0.0241,  0.0012, -0.0143,  ...,  0.0236,  0.0228,  0.0022],\n",
            "        [-0.0175, -0.0018,  0.0110,  ...,  0.0002,  0.0061, -0.0032],\n",
            "        ...,\n",
            "        [-0.0094,  0.0133,  0.0148,  ..., -0.0012,  0.0061, -0.0049],\n",
            "        [ 0.0193,  0.0243, -0.0062,  ..., -0.0265,  0.0223,  0.0147],\n",
            "        [ 0.0051, -0.0075,  0.0060,  ..., -0.0225, -0.0165,  0.0276]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.lora_down.weight', tensor([[-0.0248, -0.0100, -0.0088,  ..., -0.0122,  0.0054,  0.0076],\n",
            "        [-0.0059,  0.0031,  0.0201,  ...,  0.0208, -0.0046,  0.0104],\n",
            "        [-0.0062, -0.0020,  0.0139,  ...,  0.0263, -0.0191,  0.0135],\n",
            "        ...,\n",
            "        [ 0.0064,  0.0242,  0.0121,  ..., -0.0155, -0.0009, -0.0019],\n",
            "        [ 0.0043, -0.0271, -0.0198,  ..., -0.0223,  0.0217, -0.0017],\n",
            "        [-0.0249, -0.0173, -0.0210,  ..., -0.0263, -0.0111, -0.0274]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.lora_down.weight', tensor([[ 0.0080, -0.0128, -0.0157,  ..., -0.0185,  0.0002, -0.0051],\n",
            "        [ 0.0191,  0.0066, -0.0216,  ..., -0.0091,  0.0051,  0.0085],\n",
            "        [-0.0070,  0.0116,  0.0026,  ..., -0.0166, -0.0191, -0.0130],\n",
            "        ...,\n",
            "        [ 0.0112, -0.0037,  0.0028,  ..., -0.0014, -0.0209,  0.0139],\n",
            "        [ 0.0100,  0.0172,  0.0183,  ...,  0.0079,  0.0170,  0.0027],\n",
            "        [-0.0042, -0.0066,  0.0009,  ..., -0.0157,  0.0038, -0.0219]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight', tensor([[-0.0237, -0.0089,  0.0130,  ...,  0.0024, -0.0118,  0.0038],\n",
            "        [-0.0154, -0.0073, -0.0126,  ...,  0.0081,  0.0206, -0.0011],\n",
            "        [ 0.0127, -0.0053, -0.0163,  ..., -0.0066, -0.0179,  0.0216],\n",
            "        ...,\n",
            "        [ 0.0007, -0.0003,  0.0198,  ..., -0.0061,  0.0078, -0.0207],\n",
            "        [-0.0008,  0.0227, -0.0062,  ..., -0.0072,  0.0138, -0.0215],\n",
            "        [ 0.0197,  0.0152,  0.0114,  ...,  0.0034,  0.0018, -0.0210]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.lora_down.weight', tensor([[-0.0031,  0.0102,  0.0122,  ...,  0.0085,  0.0040,  0.0032],\n",
            "        [-0.0110,  0.0086,  0.0093,  ...,  0.0073,  0.0137,  0.0103],\n",
            "        [ 0.0037,  0.0063,  0.0070,  ..., -0.0095, -0.0037, -0.0040],\n",
            "        ...,\n",
            "        [-0.0101, -0.0070, -0.0036,  ...,  0.0133, -0.0123,  0.0110],\n",
            "        [ 0.0043, -0.0099, -0.0099,  ...,  0.0047,  0.0121,  0.0050],\n",
            "        [-0.0069,  0.0031, -0.0077,  ..., -0.0049,  0.0047,  0.0108]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.lora_down.weight', tensor([[ 0.0253, -0.0173,  0.0234,  ...,  0.0124, -0.0164,  0.0258],\n",
            "        [-0.0125, -0.0216,  0.0006,  ..., -0.0111,  0.0274,  0.0212],\n",
            "        [-0.0022, -0.0161, -0.0213,  ...,  0.0143,  0.0226,  0.0272],\n",
            "        ...,\n",
            "        [ 0.0205,  0.0065,  0.0034,  ..., -0.0206,  0.0012,  0.0128],\n",
            "        [-0.0143,  0.0112,  0.0127,  ..., -0.0147, -0.0072,  0.0175],\n",
            "        [-0.0167, -0.0271, -0.0254,  ..., -0.0255, -0.0155,  0.0107]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight', tensor([[ 0.0126,  0.0152, -0.0129,  ..., -0.0228, -0.0207,  0.0220],\n",
            "        [ 0.0231,  0.0059,  0.0264,  ...,  0.0138, -0.0106, -0.0231],\n",
            "        [-0.0136, -0.0241,  0.0119,  ...,  0.0224, -0.0086, -0.0207],\n",
            "        ...,\n",
            "        [ 0.0070, -0.0153,  0.0084,  ...,  0.0183,  0.0218, -0.0090],\n",
            "        [-0.0117, -0.0278, -0.0030,  ...,  0.0227, -0.0187,  0.0240],\n",
            "        [ 0.0279,  0.0174, -0.0186,  ...,  0.0064, -0.0142, -0.0147]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.lora_down.weight', tensor([[ 0.0126,  0.0246, -0.0025,  ..., -0.0235, -0.0203, -0.0164],\n",
            "        [ 0.0273, -0.0011,  0.0227,  ..., -0.0138, -0.0101,  0.0270],\n",
            "        [-0.0267,  0.0174, -0.0071,  ...,  0.0080, -0.0160,  0.0009],\n",
            "        ...,\n",
            "        [-0.0036, -0.0046,  0.0208,  ..., -0.0041,  0.0085, -0.0255],\n",
            "        [ 0.0240, -0.0241, -0.0088,  ..., -0.0156, -0.0085, -0.0266],\n",
            "        [ 0.0257, -0.0246,  0.0030,  ...,  0.0095, -0.0225, -0.0060]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.lora_down.weight', tensor([[ 0.0206, -0.0221,  0.0009,  ..., -0.0202, -0.0258,  0.0139],\n",
            "        [-0.0173, -0.0259, -0.0145,  ...,  0.0113,  0.0234, -0.0107],\n",
            "        [-0.0086, -0.0107, -0.0197,  ...,  0.0167, -0.0172,  0.0150],\n",
            "        ...,\n",
            "        [-0.0046, -0.0152,  0.0080,  ..., -0.0161, -0.0187,  0.0276],\n",
            "        [-0.0029, -0.0212, -0.0186,  ...,  0.0234,  0.0067, -0.0225],\n",
            "        [ 0.0009, -0.0120,  0.0117,  ...,  0.0247, -0.0083, -0.0020]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.lora_down.weight', tensor([[ 0.0055,  0.0033, -0.0001,  ...,  0.0162,  0.0192,  0.0204],\n",
            "        [-0.0132,  0.0165,  0.0024,  ..., -0.0197,  0.0165, -0.0039],\n",
            "        [-0.0060, -0.0002, -0.0032,  ...,  0.0070,  0.0076,  0.0059],\n",
            "        ...,\n",
            "        [-0.0170,  0.0069,  0.0205,  ..., -0.0043,  0.0219,  0.0154],\n",
            "        [-0.0151, -0.0049, -0.0126,  ..., -0.0075,  0.0221,  0.0131],\n",
            "        [-0.0013,  0.0064, -0.0024,  ..., -0.0116,  0.0176, -0.0211]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight', tensor([[-0.0210, -0.0275, -0.0220,  ..., -0.0013, -0.0019, -0.0090],\n",
            "        [-0.0019,  0.0274,  0.0143,  ..., -0.0172,  0.0229, -0.0018],\n",
            "        [-0.0113,  0.0197, -0.0174,  ...,  0.0161, -0.0254, -0.0131],\n",
            "        ...,\n",
            "        [ 0.0077,  0.0233, -0.0016,  ..., -0.0077, -0.0175, -0.0147],\n",
            "        [-0.0042, -0.0045,  0.0057,  ...,  0.0255, -0.0255,  0.0060],\n",
            "        [-0.0211, -0.0224,  0.0007,  ..., -0.0220,  0.0048,  0.0073]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.lora_down.weight', tensor([[ 0.0170,  0.0125, -0.0025,  ...,  0.0196, -0.0051, -0.0043],\n",
            "        [-0.0003, -0.0021,  0.0042,  ..., -0.0177,  0.0065,  0.0009],\n",
            "        [-0.0077, -0.0024, -0.0133,  ...,  0.0226, -0.0088, -0.0064],\n",
            "        ...,\n",
            "        [-0.0200, -0.0106,  0.0116,  ...,  0.0209,  0.0025, -0.0083],\n",
            "        [ 0.0099,  0.0261,  0.0275,  ..., -0.0095, -0.0241, -0.0017],\n",
            "        [-0.0248, -0.0024,  0.0248,  ..., -0.0161,  0.0024, -0.0136]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.lora_down.weight', tensor([[-0.0152,  0.0078,  0.0153,  ...,  0.0098,  0.0189, -0.0159],\n",
            "        [-0.0003, -0.0153, -0.0078,  ...,  0.0083, -0.0217,  0.0169],\n",
            "        [-0.0131, -0.0126, -0.0053,  ...,  0.0013, -0.0036, -0.0188],\n",
            "        ...,\n",
            "        [ 0.0210, -0.0127, -0.0125,  ..., -0.0104,  0.0068,  0.0136],\n",
            "        [-0.0198,  0.0218,  0.0066,  ..., -0.0026, -0.0048, -0.0075],\n",
            "        [ 0.0046,  0.0073,  0.0082,  ..., -0.0025,  0.0121,  0.0025]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight', tensor([[ 0.0048,  0.0040, -0.0114,  ...,  0.0154, -0.0255, -0.0040],\n",
            "        [-0.0118, -0.0244, -0.0104,  ...,  0.0145,  0.0186, -0.0104],\n",
            "        [ 0.0190,  0.0186,  0.0042,  ..., -0.0202,  0.0070,  0.0122],\n",
            "        ...,\n",
            "        [-0.0081, -0.0094, -0.0038,  ..., -0.0135, -0.0273,  0.0228],\n",
            "        [-0.0009,  0.0111, -0.0161,  ..., -0.0177,  0.0148,  0.0209],\n",
            "        [ 0.0247,  0.0053, -0.0241,  ...,  0.0035,  0.0002, -0.0178]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.lora_down.weight', tensor([[ 0.0090,  0.0018, -0.0106,  ..., -0.0122,  0.0017,  0.0137],\n",
            "        [ 0.0046, -0.0086,  0.0015,  ..., -0.0108, -0.0089,  0.0091],\n",
            "        [-0.0135, -0.0018,  0.0122,  ...,  0.0009, -0.0087, -0.0076],\n",
            "        ...,\n",
            "        [ 0.0032,  0.0024, -0.0126,  ..., -0.0058,  0.0087, -0.0127],\n",
            "        [-0.0058,  0.0107, -0.0072,  ..., -0.0079,  0.0128, -0.0114],\n",
            "        [ 0.0046, -0.0027,  0.0106,  ...,  0.0088,  0.0050, -0.0068]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.lora_down.weight', tensor([[-0.0220, -0.0021, -0.0209,  ..., -0.0037, -0.0178, -0.0157],\n",
            "        [-0.0096,  0.0039,  0.0118,  ...,  0.0123,  0.0275, -0.0229],\n",
            "        [-0.0253, -0.0099,  0.0077,  ...,  0.0005,  0.0210, -0.0046],\n",
            "        ...,\n",
            "        [ 0.0032, -0.0012,  0.0082,  ...,  0.0112,  0.0177, -0.0252],\n",
            "        [-0.0126, -0.0145, -0.0020,  ...,  0.0276,  0.0050, -0.0094],\n",
            "        [-0.0095,  0.0125, -0.0221,  ...,  0.0080,  0.0179,  0.0039]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight', tensor([[ 2.1179e-02,  2.2736e-02, -5.9166e-03,  ...,  2.6306e-02,\n",
            "         -4.7760e-03, -2.4811e-02],\n",
            "        [-6.5689e-03, -1.7426e-02, -2.3804e-02,  ...,  6.8855e-03,\n",
            "          9.1019e-03,  1.4206e-02],\n",
            "        [-2.4490e-02,  2.6913e-03, -1.7044e-02,  ...,  1.6434e-02,\n",
            "          2.6489e-02, -8.7662e-03],\n",
            "        ...,\n",
            "        [-2.6093e-02,  3.2597e-03,  2.5330e-02,  ..., -9.7504e-03,\n",
            "         -2.6428e-02,  8.0338e-03],\n",
            "        [ 1.9592e-02, -1.9806e-02,  2.1820e-02,  ...,  5.7373e-03,\n",
            "          3.5226e-05,  7.5455e-03],\n",
            "        [-7.4577e-03,  2.0935e-02,  2.4994e-02,  ...,  1.5915e-02,\n",
            "          1.0582e-02, -1.5793e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.lora_down.weight', tensor([[ 2.6108e-02,  1.5511e-02,  1.7319e-02,  ...,  1.4870e-02,\n",
            "         -2.0996e-02, -2.3560e-02],\n",
            "        [ 1.2436e-02, -2.5253e-02, -1.0815e-03,  ...,  2.0721e-02,\n",
            "         -1.6998e-02,  7.3357e-03],\n",
            "        [-2.3804e-02, -2.1652e-02,  1.3298e-02,  ...,  1.5450e-02,\n",
            "         -1.8787e-03,  2.2995e-02],\n",
            "        ...,\n",
            "        [-2.4246e-02,  1.1101e-02,  4.4098e-03,  ...,  1.8524e-02,\n",
            "          1.3412e-02,  2.4338e-02],\n",
            "        [-2.4338e-02, -1.0750e-02, -3.6955e-05,  ...,  2.4780e-02,\n",
            "         -1.3523e-03,  6.1111e-03],\n",
            "        [-8.8310e-04, -1.8448e-02,  3.2711e-03,  ..., -2.1912e-02,\n",
            "         -2.7924e-02, -4.1809e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.lora_down.weight', tensor([[-0.0195,  0.0025, -0.0075,  ...,  0.0279,  0.0022,  0.0022],\n",
            "        [ 0.0275, -0.0134, -0.0228,  ...,  0.0222, -0.0051, -0.0210],\n",
            "        [ 0.0097,  0.0094,  0.0140,  ...,  0.0096,  0.0272, -0.0040],\n",
            "        ...,\n",
            "        [ 0.0224, -0.0067,  0.0069,  ...,  0.0097, -0.0183,  0.0260],\n",
            "        [-0.0185, -0.0024,  0.0243,  ...,  0.0017,  0.0138,  0.0054],\n",
            "        [ 0.0162, -0.0165, -0.0095,  ..., -0.0231, -0.0016,  0.0271]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.lora_down.weight', tensor([[ 1.7212e-02, -7.5378e-03,  2.1271e-02,  ..., -1.7212e-02,\n",
            "          1.4565e-02,  1.3237e-02],\n",
            "        [ 9.1705e-03,  1.8188e-02, -1.9501e-02,  ...,  1.0010e-02,\n",
            "          2.1386e-04,  5.5265e-04],\n",
            "        [ 6.0081e-03, -2.0157e-02,  6.7413e-05,  ...,  1.8921e-02,\n",
            "         -9.4986e-03, -2.1729e-02],\n",
            "        ...,\n",
            "        [ 1.6205e-02,  2.9907e-03, -1.9073e-02,  ...,  2.7466e-03,\n",
            "          1.9638e-02,  1.5839e-02],\n",
            "        [-1.9424e-02,  3.1166e-03, -7.1602e-03,  ...,  1.4467e-03,\n",
            "         -1.7517e-02,  2.9335e-03],\n",
            "        [ 2.5768e-03,  8.2626e-03,  5.9128e-03,  ...,  1.9821e-02,\n",
            "          5.7068e-03, -8.3542e-04]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight', tensor([[-0.0145,  0.0045,  0.0066,  ...,  0.0048, -0.0188, -0.0075],\n",
            "        [-0.0037, -0.0047,  0.0005,  ...,  0.0060,  0.0140,  0.0161],\n",
            "        [-0.0111, -0.0213,  0.0192,  ...,  0.0128, -0.0185,  0.0211],\n",
            "        ...,\n",
            "        [-0.0127,  0.0040,  0.0096,  ..., -0.0196, -0.0113,  0.0019],\n",
            "        [-0.0217,  0.0069,  0.0115,  ...,  0.0004, -0.0002, -0.0156],\n",
            "        [ 0.0068,  0.0264,  0.0107,  ..., -0.0160, -0.0260, -0.0139]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.lora_down.weight', tensor([[-0.0130, -0.0205,  0.0192,  ..., -0.0140, -0.0011,  0.0124],\n",
            "        [-0.0027,  0.0147, -0.0210,  ...,  0.0269, -0.0083,  0.0240],\n",
            "        [-0.0056,  0.0204, -0.0229,  ..., -0.0179,  0.0187,  0.0042],\n",
            "        ...,\n",
            "        [-0.0077, -0.0167, -0.0257,  ..., -0.0038, -0.0202, -0.0028],\n",
            "        [ 0.0085,  0.0118, -0.0032,  ..., -0.0146,  0.0011, -0.0016],\n",
            "        [-0.0156,  0.0138,  0.0071,  ...,  0.0007,  0.0172,  0.0097]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.lora_down.weight', tensor([[-0.0145,  0.0052, -0.0008,  ...,  0.0034, -0.0053,  0.0190],\n",
            "        [-0.0095,  0.0063,  0.0021,  ..., -0.0199,  0.0086, -0.0126],\n",
            "        [-0.0165,  0.0026, -0.0130,  ..., -0.0201,  0.0068,  0.0215],\n",
            "        ...,\n",
            "        [ 0.0115, -0.0183,  0.0085,  ...,  0.0198, -0.0177, -0.0170],\n",
            "        [-0.0028,  0.0138, -0.0140,  ...,  0.0134,  0.0186,  0.0063],\n",
            "        [-0.0142, -0.0209, -0.0190,  ..., -0.0096,  0.0110,  0.0177]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight', tensor([[-0.0159, -0.0061, -0.0234,  ..., -0.0195, -0.0232,  0.0218],\n",
            "        [ 0.0263,  0.0131, -0.0094,  ..., -0.0069,  0.0199,  0.0233],\n",
            "        [ 0.0162, -0.0091, -0.0275,  ...,  0.0042,  0.0019, -0.0268],\n",
            "        ...,\n",
            "        [-0.0179,  0.0103,  0.0169,  ...,  0.0273,  0.0257, -0.0110],\n",
            "        [ 0.0011, -0.0048, -0.0086,  ..., -0.0219,  0.0270, -0.0034],\n",
            "        [-0.0059, -0.0112,  0.0089,  ..., -0.0130,  0.0246, -0.0259]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.lora_down.weight', tensor([[ 0.0113,  0.0057, -0.0026,  ..., -0.0099, -0.0126,  0.0123],\n",
            "        [ 0.0055, -0.0007,  0.0029,  ...,  0.0124, -0.0134, -0.0124],\n",
            "        [ 0.0005, -0.0140, -0.0005,  ..., -0.0125,  0.0022,  0.0009],\n",
            "        ...,\n",
            "        [-0.0020, -0.0002, -0.0009,  ...,  0.0040, -0.0080, -0.0075],\n",
            "        [ 0.0017, -0.0038, -0.0005,  ..., -0.0122,  0.0060, -0.0045],\n",
            "        [-0.0054,  0.0015, -0.0094,  ..., -0.0027, -0.0008, -0.0121]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.lora_down.weight', tensor([[ 0.0111,  0.0246,  0.0186,  ...,  0.0215, -0.0209,  0.0050],\n",
            "        [-0.0051,  0.0060,  0.0065,  ...,  0.0113,  0.0088, -0.0105],\n",
            "        [-0.0230,  0.0204, -0.0021,  ..., -0.0092, -0.0053, -0.0248],\n",
            "        ...,\n",
            "        [ 0.0066, -0.0230, -0.0240,  ..., -0.0009,  0.0019,  0.0237],\n",
            "        [-0.0021, -0.0084,  0.0085,  ...,  0.0013, -0.0046, -0.0174],\n",
            "        [-0.0009, -0.0173, -0.0090,  ...,  0.0086,  0.0170, -0.0247]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight', tensor([[ 0.0158, -0.0252, -0.0256,  ..., -0.0131,  0.0258,  0.0039],\n",
            "        [ 0.0214, -0.0109,  0.0210,  ..., -0.0120,  0.0265,  0.0170],\n",
            "        [ 0.0085, -0.0029, -0.0124,  ..., -0.0135,  0.0015, -0.0277],\n",
            "        ...,\n",
            "        [ 0.0089, -0.0159,  0.0042,  ...,  0.0229,  0.0023,  0.0112],\n",
            "        [ 0.0009, -0.0131, -0.0159,  ...,  0.0135, -0.0056,  0.0157],\n",
            "        [-0.0200, -0.0059, -0.0070,  ..., -0.0148,  0.0079, -0.0275]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.lora_down.weight', tensor([[ 0.0046, -0.0031, -0.0104,  ..., -0.0027,  0.0224, -0.0107],\n",
            "        [ 0.0268, -0.0230,  0.0008,  ...,  0.0242, -0.0258,  0.0128],\n",
            "        [-0.0145,  0.0162, -0.0248,  ...,  0.0038, -0.0276, -0.0234],\n",
            "        ...,\n",
            "        [-0.0225,  0.0097,  0.0038,  ..., -0.0069,  0.0213, -0.0104],\n",
            "        [-0.0249,  0.0100, -0.0068,  ...,  0.0026,  0.0127,  0.0268],\n",
            "        [-0.0069,  0.0180,  0.0251,  ..., -0.0096,  0.0151,  0.0192]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.lora_down.weight', tensor([[ 0.0153,  0.0037,  0.0243,  ...,  0.0245,  0.0278,  0.0179],\n",
            "        [-0.0185, -0.0011,  0.0156,  ..., -0.0271, -0.0096,  0.0183],\n",
            "        [-0.0139, -0.0194, -0.0128,  ..., -0.0083, -0.0170,  0.0131],\n",
            "        ...,\n",
            "        [-0.0107,  0.0185, -0.0139,  ...,  0.0128,  0.0095,  0.0137],\n",
            "        [ 0.0201,  0.0169, -0.0104,  ..., -0.0176, -0.0120,  0.0214],\n",
            "        [ 0.0069, -0.0089, -0.0118,  ...,  0.0205, -0.0199, -0.0017]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.lora_down.weight', tensor([[ 0.0177,  0.0090,  0.0048,  ...,  0.0007,  0.0110,  0.0210],\n",
            "        [ 0.0031, -0.0126,  0.0173,  ...,  0.0119, -0.0065,  0.0081],\n",
            "        [-0.0070, -0.0038,  0.0047,  ...,  0.0136,  0.0082, -0.0105],\n",
            "        ...,\n",
            "        [ 0.0129, -0.0036,  0.0070,  ...,  0.0164, -0.0097,  0.0122],\n",
            "        [ 0.0056, -0.0106,  0.0045,  ...,  0.0120,  0.0057,  0.0017],\n",
            "        [ 0.0020, -0.0117,  0.0209,  ..., -0.0077, -0.0091,  0.0143]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight', tensor([[ 0.0229, -0.0152, -0.0277,  ...,  0.0117,  0.0192,  0.0227],\n",
            "        [-0.0072, -0.0189,  0.0026,  ...,  0.0096, -0.0229, -0.0239],\n",
            "        [ 0.0232, -0.0021, -0.0272,  ...,  0.0069,  0.0076,  0.0062],\n",
            "        ...,\n",
            "        [ 0.0039, -0.0080,  0.0023,  ..., -0.0155, -0.0030,  0.0219],\n",
            "        [ 0.0045,  0.0150,  0.0108,  ...,  0.0123,  0.0131,  0.0092],\n",
            "        [ 0.0121, -0.0184,  0.0179,  ...,  0.0244,  0.0060, -0.0088]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.lora_down.weight', tensor([[-0.0131,  0.0049,  0.0273,  ..., -0.0085,  0.0085, -0.0111],\n",
            "        [-0.0123,  0.0129, -0.0212,  ..., -0.0140,  0.0057, -0.0164],\n",
            "        [ 0.0265,  0.0006, -0.0200,  ...,  0.0154,  0.0142,  0.0049],\n",
            "        ...,\n",
            "        [ 0.0180, -0.0175, -0.0115,  ...,  0.0212, -0.0075, -0.0092],\n",
            "        [-0.0017, -0.0019, -0.0156,  ...,  0.0001,  0.0242,  0.0173],\n",
            "        [ 0.0088,  0.0031,  0.0141,  ..., -0.0199, -0.0206, -0.0094]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.lora_down.weight', tensor([[ 0.0164, -0.0124, -0.0079,  ...,  0.0141,  0.0203,  0.0137],\n",
            "        [-0.0013,  0.0117, -0.0169,  ..., -0.0057,  0.0096, -0.0145],\n",
            "        [ 0.0150, -0.0045, -0.0095,  ..., -0.0055,  0.0019, -0.0086],\n",
            "        ...,\n",
            "        [-0.0157, -0.0184, -0.0119,  ..., -0.0128,  0.0056, -0.0015],\n",
            "        [ 0.0089, -0.0166, -0.0123,  ...,  0.0214,  0.0095, -0.0188],\n",
            "        [-0.0091, -0.0152,  0.0073,  ..., -0.0121, -0.0073, -0.0014]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight', tensor([[ 0.0207, -0.0138, -0.0074,  ...,  0.0049, -0.0185,  0.0186],\n",
            "        [ 0.0116,  0.0138, -0.0142,  ..., -0.0111, -0.0037,  0.0067],\n",
            "        [-0.0206,  0.0034, -0.0260,  ...,  0.0103,  0.0095,  0.0271],\n",
            "        ...,\n",
            "        [ 0.0174,  0.0057,  0.0206,  ...,  0.0125,  0.0023, -0.0244],\n",
            "        [-0.0116,  0.0105, -0.0187,  ...,  0.0145,  0.0090, -0.0092],\n",
            "        [ 0.0044,  0.0098, -0.0239,  ..., -0.0099, -0.0129, -0.0162]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.lora_down.weight', tensor([[-0.0017,  0.0062,  0.0037,  ..., -0.0014, -0.0095,  0.0004],\n",
            "        [-0.0050,  0.0130,  0.0026,  ...,  0.0091,  0.0106, -0.0002],\n",
            "        [-0.0041,  0.0045,  0.0012,  ..., -0.0086,  0.0110, -0.0036],\n",
            "        ...,\n",
            "        [-0.0120, -0.0073, -0.0135,  ...,  0.0083,  0.0060, -0.0016],\n",
            "        [ 0.0010, -0.0092, -0.0052,  ..., -0.0061,  0.0120, -0.0078],\n",
            "        [ 0.0119, -0.0116,  0.0050,  ..., -0.0060, -0.0072, -0.0087]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.lora_down.weight', tensor([[-0.0206,  0.0277,  0.0128,  ..., -0.0051,  0.0253, -0.0003],\n",
            "        [-0.0103,  0.0010, -0.0208,  ...,  0.0239,  0.0261, -0.0125],\n",
            "        [-0.0200,  0.0169,  0.0046,  ...,  0.0263,  0.0217, -0.0149],\n",
            "        ...,\n",
            "        [-0.0136,  0.0007,  0.0087,  ...,  0.0169, -0.0133,  0.0027],\n",
            "        [-0.0218,  0.0084, -0.0092,  ...,  0.0038, -0.0055, -0.0173],\n",
            "        [ 0.0268, -0.0031,  0.0238,  ..., -0.0194,  0.0127,  0.0215]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight', tensor([[-0.0029, -0.0068,  0.0213,  ...,  0.0067, -0.0154,  0.0119],\n",
            "        [-0.0186, -0.0097,  0.0035,  ...,  0.0255, -0.0262, -0.0220],\n",
            "        [ 0.0137, -0.0027, -0.0209,  ...,  0.0188,  0.0070, -0.0187],\n",
            "        ...,\n",
            "        [-0.0070, -0.0055,  0.0063,  ...,  0.0100, -0.0228, -0.0134],\n",
            "        [-0.0260,  0.0015, -0.0220,  ...,  0.0244,  0.0150,  0.0013],\n",
            "        [ 0.0101,  0.0005,  0.0036,  ...,  0.0263,  0.0100,  0.0184]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.lora_down.weight', tensor([[ 0.0155,  0.0173,  0.0211,  ...,  0.0143, -0.0279, -0.0118],\n",
            "        [ 0.0095, -0.0179, -0.0122,  ..., -0.0123, -0.0270, -0.0218],\n",
            "        [-0.0100,  0.0015,  0.0054,  ...,  0.0231, -0.0166, -0.0039],\n",
            "        ...,\n",
            "        [-0.0238,  0.0075, -0.0218,  ...,  0.0095, -0.0167, -0.0079],\n",
            "        [-0.0148,  0.0233, -0.0134,  ..., -0.0114, -0.0077, -0.0146],\n",
            "        [-0.0014,  0.0187, -0.0013,  ...,  0.0174, -0.0126,  0.0031]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.lora_down.weight', tensor([[-6.9809e-03, -1.2444e-02,  1.3351e-02,  ...,  1.1566e-02,\n",
            "         -2.7435e-02,  3.2723e-05],\n",
            "        [ 2.0966e-02, -2.1133e-02,  8.6288e-03,  ...,  1.4374e-02,\n",
            "         -2.3911e-02,  7.5264e-03],\n",
            "        [ 2.0874e-02,  2.6184e-02, -8.6441e-03,  ..., -1.1536e-02,\n",
            "          3.7594e-03, -3.3550e-03],\n",
            "        ...,\n",
            "        [ 1.6794e-03,  7.7934e-03,  1.5518e-02,  ..., -1.3489e-02,\n",
            "          2.1515e-02,  7.0691e-05],\n",
            "        [-1.5556e-02, -1.2001e-02, -1.6870e-03,  ..., -2.0004e-02,\n",
            "         -8.0414e-03,  1.0216e-02],\n",
            "        [ 2.0966e-02, -2.9030e-03, -5.5733e-03,  ...,  2.9421e-04,\n",
            "          1.7746e-02, -3.0270e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.lora_down.weight', tensor([[ 1.6495e-02, -1.2007e-03,  5.1117e-03,  ...,  2.0859e-02,\n",
            "          6.2828e-03, -2.0508e-02],\n",
            "        [ 1.5396e-02, -8.4152e-03,  1.5442e-02,  ...,  2.1076e-03,\n",
            "          2.0126e-02, -1.6434e-02],\n",
            "        [ 2.1652e-02, -1.5869e-02,  1.7166e-03,  ...,  1.9875e-03,\n",
            "         -1.8570e-02,  4.1771e-03],\n",
            "        ...,\n",
            "        [-1.3672e-02,  1.9073e-02,  7.9575e-03,  ...,  2.0233e-02,\n",
            "         -4.3221e-03,  1.4755e-02],\n",
            "        [ 4.5776e-03,  1.1467e-02,  1.9852e-02,  ..., -6.9082e-05,\n",
            "         -1.9836e-02,  1.2863e-02],\n",
            "        [ 1.0635e-02,  1.2276e-02, -1.6144e-02,  ...,  1.8387e-02,\n",
            "         -1.7715e-02, -1.1971e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight', tensor([[-0.0041, -0.0116,  0.0241,  ..., -0.0257,  0.0068,  0.0215],\n",
            "        [-0.0240,  0.0177, -0.0157,  ..., -0.0042, -0.0164,  0.0207],\n",
            "        [ 0.0186,  0.0263,  0.0070,  ...,  0.0214,  0.0104,  0.0167],\n",
            "        ...,\n",
            "        [ 0.0010,  0.0073, -0.0232,  ..., -0.0207, -0.0251,  0.0023],\n",
            "        [ 0.0276,  0.0001,  0.0085,  ..., -0.0082, -0.0088, -0.0032],\n",
            "        [ 0.0116, -0.0255,  0.0065,  ..., -0.0057, -0.0003,  0.0241]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.lora_down.weight', tensor([[-0.0258,  0.0029,  0.0229,  ..., -0.0050, -0.0223, -0.0162],\n",
            "        [-0.0240,  0.0275, -0.0123,  ..., -0.0235,  0.0129, -0.0259],\n",
            "        [ 0.0177, -0.0215, -0.0100,  ..., -0.0040,  0.0086,  0.0267],\n",
            "        ...,\n",
            "        [-0.0195,  0.0259, -0.0031,  ...,  0.0147, -0.0196, -0.0210],\n",
            "        [-0.0128, -0.0064, -0.0008,  ..., -0.0118,  0.0093, -0.0020],\n",
            "        [-0.0233, -0.0124, -0.0169,  ..., -0.0028,  0.0274,  0.0064]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.lora_down.weight', tensor([[ 0.0145, -0.0089,  0.0094,  ...,  0.0159,  0.0073, -0.0154],\n",
            "        [ 0.0085, -0.0067, -0.0081,  ..., -0.0082, -0.0218, -0.0012],\n",
            "        [-0.0024, -0.0182, -0.0215,  ..., -0.0041, -0.0029, -0.0137],\n",
            "        ...,\n",
            "        [ 0.0160, -0.0047,  0.0115,  ...,  0.0217,  0.0094,  0.0134],\n",
            "        [-0.0204, -0.0073,  0.0144,  ...,  0.0197,  0.0162, -0.0074],\n",
            "        [ 0.0116, -0.0008, -0.0045,  ..., -0.0204, -0.0214,  0.0048]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight', tensor([[ 0.0220, -0.0074, -0.0109,  ..., -0.0140, -0.0094, -0.0023],\n",
            "        [ 0.0043, -0.0210, -0.0247,  ..., -0.0242, -0.0022,  0.0069],\n",
            "        [-0.0041, -0.0066,  0.0247,  ..., -0.0259,  0.0132,  0.0145],\n",
            "        ...,\n",
            "        [-0.0132,  0.0071,  0.0048,  ..., -0.0057,  0.0083,  0.0186],\n",
            "        [ 0.0182,  0.0118,  0.0016,  ..., -0.0172,  0.0138, -0.0268],\n",
            "        [ 0.0138,  0.0156, -0.0027,  ..., -0.0105,  0.0123,  0.0028]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.lora_down.weight', tensor([[-0.0018,  0.0042, -0.0058,  ..., -0.0069,  0.0088, -0.0037],\n",
            "        [-0.0044,  0.0086,  0.0016,  ...,  0.0037, -0.0129,  0.0021],\n",
            "        [ 0.0094, -0.0093,  0.0079,  ...,  0.0080,  0.0135, -0.0052],\n",
            "        ...,\n",
            "        [ 0.0076,  0.0059, -0.0113,  ...,  0.0023, -0.0038, -0.0090],\n",
            "        [-0.0121, -0.0041, -0.0112,  ...,  0.0036,  0.0030, -0.0006],\n",
            "        [ 0.0038, -0.0069, -0.0092,  ...,  0.0027,  0.0034,  0.0052]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.lora_down.weight', tensor([[-0.0199,  0.0078,  0.0067,  ...,  0.0209, -0.0275,  0.0196],\n",
            "        [-0.0103,  0.0033, -0.0158,  ..., -0.0087,  0.0142, -0.0194],\n",
            "        [ 0.0167, -0.0177, -0.0103,  ..., -0.0132,  0.0148, -0.0161],\n",
            "        ...,\n",
            "        [-0.0273,  0.0040, -0.0224,  ...,  0.0131,  0.0068,  0.0272],\n",
            "        [-0.0212, -0.0232, -0.0243,  ...,  0.0157, -0.0044, -0.0244],\n",
            "        [-0.0156, -0.0145, -0.0026,  ..., -0.0136,  0.0177, -0.0172]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight', tensor([[ 0.0278, -0.0241, -0.0168,  ..., -0.0172, -0.0225,  0.0237],\n",
            "        [ 0.0008, -0.0011,  0.0093,  ..., -0.0024,  0.0020,  0.0089],\n",
            "        [ 0.0057,  0.0053,  0.0061,  ..., -0.0140, -0.0075, -0.0213],\n",
            "        ...,\n",
            "        [ 0.0266,  0.0157,  0.0021,  ...,  0.0033, -0.0055, -0.0254],\n",
            "        [-0.0149,  0.0070,  0.0152,  ...,  0.0276, -0.0076, -0.0111],\n",
            "        [-0.0272, -0.0259,  0.0209,  ...,  0.0182,  0.0118, -0.0163]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.lora_down.weight', tensor([[-0.0101,  0.0260, -0.0078,  ..., -0.0140,  0.0213, -0.0016],\n",
            "        [ 0.0007,  0.0228, -0.0216,  ..., -0.0039, -0.0229, -0.0197],\n",
            "        [-0.0110, -0.0144, -0.0096,  ...,  0.0219, -0.0122, -0.0194],\n",
            "        ...,\n",
            "        [ 0.0101,  0.0122, -0.0108,  ..., -0.0035,  0.0232, -0.0220],\n",
            "        [-0.0116, -0.0056,  0.0001,  ..., -0.0157, -0.0181, -0.0276],\n",
            "        [ 0.0080,  0.0077,  0.0171,  ...,  0.0137, -0.0224,  0.0087]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.lora_down.weight', tensor([[ 0.0055,  0.0082, -0.0200,  ..., -0.0075,  0.0102, -0.0057],\n",
            "        [-0.0118,  0.0041, -0.0068,  ...,  0.0177, -0.0135,  0.0056],\n",
            "        [-0.0231, -0.0025, -0.0226,  ..., -0.0128, -0.0197, -0.0186],\n",
            "        ...,\n",
            "        [-0.0207,  0.0012,  0.0237,  ..., -0.0115, -0.0259,  0.0163],\n",
            "        [-0.0136,  0.0264, -0.0146,  ...,  0.0196, -0.0098,  0.0135],\n",
            "        [ 0.0101,  0.0267, -0.0279,  ..., -0.0234, -0.0204,  0.0015]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.lora_down.weight', tensor([[-0.0109, -0.0079,  0.0030,  ..., -0.0052,  0.0151,  0.0100],\n",
            "        [ 0.0068,  0.0039,  0.0102,  ...,  0.0029,  0.0020,  0.0056],\n",
            "        [ 0.0008,  0.0022, -0.0050,  ..., -0.0210,  0.0168, -0.0174],\n",
            "        ...,\n",
            "        [ 0.0183, -0.0038, -0.0028,  ..., -0.0038, -0.0140,  0.0066],\n",
            "        [-0.0050, -0.0208, -0.0103,  ...,  0.0166,  0.0187,  0.0043],\n",
            "        [ 0.0149, -0.0066, -0.0128,  ..., -0.0118,  0.0067, -0.0077]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight', tensor([[-0.0004, -0.0107,  0.0164,  ...,  0.0006,  0.0219,  0.0258],\n",
            "        [ 0.0196, -0.0049, -0.0253,  ..., -0.0070,  0.0164,  0.0241],\n",
            "        [ 0.0109,  0.0225, -0.0102,  ...,  0.0066,  0.0224,  0.0081],\n",
            "        ...,\n",
            "        [-0.0190,  0.0103, -0.0046,  ...,  0.0036, -0.0097,  0.0136],\n",
            "        [ 0.0089,  0.0069,  0.0265,  ..., -0.0196, -0.0032, -0.0170],\n",
            "        [-0.0175,  0.0207,  0.0143,  ..., -0.0212, -0.0258,  0.0249]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.lora_down.weight', tensor([[-0.0030,  0.0090,  0.0203,  ...,  0.0117,  0.0126,  0.0242],\n",
            "        [ 0.0264,  0.0035,  0.0085,  ...,  0.0251, -0.0005, -0.0097],\n",
            "        [-0.0047,  0.0159,  0.0202,  ...,  0.0204, -0.0043,  0.0036],\n",
            "        ...,\n",
            "        [-0.0129, -0.0015,  0.0012,  ..., -0.0213,  0.0204, -0.0251],\n",
            "        [ 0.0152, -0.0257, -0.0082,  ...,  0.0215,  0.0264, -0.0021],\n",
            "        [ 0.0165,  0.0077,  0.0194,  ..., -0.0172,  0.0043, -0.0201]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.lora_down.weight', tensor([[-0.0154, -0.0118, -0.0069,  ..., -0.0204,  0.0209,  0.0030],\n",
            "        [-0.0199,  0.0185,  0.0012,  ...,  0.0160,  0.0199,  0.0111],\n",
            "        [ 0.0017,  0.0082,  0.0209,  ..., -0.0213,  0.0167, -0.0057],\n",
            "        ...,\n",
            "        [-0.0102,  0.0204,  0.0133,  ..., -0.0048, -0.0004, -0.0132],\n",
            "        [-0.0138,  0.0164,  0.0179,  ...,  0.0021,  0.0094, -0.0017],\n",
            "        [ 0.0009,  0.0067,  0.0186,  ...,  0.0157, -0.0211,  0.0202]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight', tensor([[ 0.0070, -0.0205, -0.0189,  ...,  0.0045,  0.0141,  0.0092],\n",
            "        [ 0.0261, -0.0062,  0.0175,  ...,  0.0046,  0.0022, -0.0171],\n",
            "        [ 0.0227,  0.0056,  0.0208,  ...,  0.0020,  0.0190,  0.0008],\n",
            "        ...,\n",
            "        [ 0.0152,  0.0102,  0.0024,  ...,  0.0058, -0.0110, -0.0092],\n",
            "        [-0.0252, -0.0154,  0.0196,  ..., -0.0236, -0.0114,  0.0258],\n",
            "        [-0.0256,  0.0174,  0.0271,  ...,  0.0080,  0.0154, -0.0231]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.lora_down.weight', tensor([[-0.0075,  0.0050,  0.0121,  ..., -0.0011, -0.0076,  0.0041],\n",
            "        [ 0.0065,  0.0108, -0.0126,  ...,  0.0011, -0.0088, -0.0019],\n",
            "        [-0.0056,  0.0139,  0.0010,  ...,  0.0113, -0.0129,  0.0116],\n",
            "        ...,\n",
            "        [-0.0120, -0.0121,  0.0094,  ...,  0.0102,  0.0072,  0.0094],\n",
            "        [-0.0095, -0.0139, -0.0007,  ...,  0.0117,  0.0134, -0.0085],\n",
            "        [-0.0086,  0.0065,  0.0096,  ..., -0.0139,  0.0045, -0.0013]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.lora_down.weight', tensor([[ 0.0091,  0.0133,  0.0232,  ..., -0.0144,  0.0260,  0.0015],\n",
            "        [ 0.0070, -0.0107, -0.0008,  ..., -0.0007,  0.0226,  0.0210],\n",
            "        [ 0.0165,  0.0120,  0.0067,  ..., -0.0256,  0.0009, -0.0221],\n",
            "        ...,\n",
            "        [-0.0191, -0.0216,  0.0234,  ...,  0.0133,  0.0261,  0.0236],\n",
            "        [-0.0069,  0.0182, -0.0257,  ...,  0.0124, -0.0126, -0.0012],\n",
            "        [-0.0011, -0.0242,  0.0101,  ..., -0.0179, -0.0035,  0.0215]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight', tensor([[-2.4063e-02, -2.4323e-02,  1.2314e-02,  ...,  2.4338e-02,\n",
            "         -1.2306e-02, -1.9058e-02],\n",
            "        [ 2.1210e-03,  2.3163e-02, -6.2561e-03,  ...,  9.8648e-03,\n",
            "         -1.3741e-02, -8.5688e-04],\n",
            "        [ 1.4061e-02, -5.0926e-04,  2.1317e-02,  ..., -1.4663e-05,\n",
            "         -1.6373e-02,  1.7929e-02],\n",
            "        ...,\n",
            "        [-1.2527e-02, -1.7883e-02,  2.3132e-02,  ..., -2.5101e-02,\n",
            "         -3.2330e-03, -9.9335e-03],\n",
            "        [-9.4528e-03,  1.4473e-02,  1.1147e-02,  ..., -1.5976e-02,\n",
            "         -2.7939e-02,  2.6642e-02],\n",
            "        [-1.6037e-02,  3.5686e-03, -3.6716e-03,  ..., -9.4528e-03,\n",
            "          2.2461e-02,  1.3939e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.lora_down.weight', tensor([[-0.0264, -0.0243, -0.0035,  ...,  0.0155,  0.0157, -0.0018],\n",
            "        [-0.0192, -0.0046,  0.0092,  ...,  0.0186, -0.0060,  0.0160],\n",
            "        [-0.0009, -0.0115, -0.0072,  ..., -0.0119,  0.0204, -0.0104],\n",
            "        ...,\n",
            "        [ 0.0203, -0.0271, -0.0235,  ..., -0.0111,  0.0177, -0.0228],\n",
            "        [-0.0182,  0.0240, -0.0237,  ..., -0.0237, -0.0196,  0.0231],\n",
            "        [ 0.0263,  0.0050,  0.0004,  ..., -0.0217, -0.0142, -0.0097]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.lora_down.weight', tensor([[ 0.0239, -0.0212,  0.0011,  ...,  0.0167,  0.0052,  0.0221],\n",
            "        [-0.0098,  0.0275,  0.0220,  ...,  0.0042,  0.0104,  0.0013],\n",
            "        [-0.0086,  0.0109,  0.0230,  ...,  0.0152,  0.0189,  0.0127],\n",
            "        ...,\n",
            "        [ 0.0150, -0.0181, -0.0018,  ..., -0.0101,  0.0039, -0.0257],\n",
            "        [-0.0087,  0.0106, -0.0192,  ..., -0.0002,  0.0264, -0.0161],\n",
            "        [ 0.0087,  0.0052, -0.0220,  ..., -0.0201,  0.0153,  0.0180]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.lora_down.weight', tensor([[-3.7556e-03,  4.1275e-03,  8.8654e-03,  ...,  7.6218e-03,\n",
            "          1.6088e-03, -2.1088e-02],\n",
            "        [-3.6144e-03, -6.0606e-04, -1.3718e-02,  ..., -6.0387e-03,\n",
            "          1.4397e-02,  2.6608e-03],\n",
            "        [ 1.0765e-02, -6.9809e-03, -1.6136e-03,  ..., -2.1545e-02,\n",
            "         -1.4084e-02,  5.7182e-03],\n",
            "        ...,\n",
            "        [ 1.1795e-02, -1.5823e-02, -6.0577e-03,  ..., -1.4915e-02,\n",
            "          2.6302e-03,  1.9180e-02],\n",
            "        [ 8.9645e-04, -1.0178e-02, -7.4863e-05,  ...,  1.4709e-02,\n",
            "          1.9409e-02,  2.1942e-02],\n",
            "        [-5.1155e-03, -1.8402e-02, -5.5351e-03,  ...,  5.0354e-03,\n",
            "         -2.0050e-02, -1.1543e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight', tensor([[ 0.0014,  0.0072,  0.0250,  ...,  0.0246, -0.0226, -0.0125],\n",
            "        [ 0.0076, -0.0259,  0.0249,  ...,  0.0134,  0.0032,  0.0175],\n",
            "        [-0.0050,  0.0246, -0.0005,  ..., -0.0225, -0.0100, -0.0092],\n",
            "        ...,\n",
            "        [ 0.0173, -0.0001, -0.0222,  ...,  0.0135,  0.0040, -0.0216],\n",
            "        [-0.0074,  0.0051,  0.0159,  ...,  0.0182,  0.0105,  0.0034],\n",
            "        [-0.0203, -0.0044, -0.0120,  ..., -0.0126,  0.0225, -0.0223]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.lora_down.weight', tensor([[-0.0027, -0.0183,  0.0201,  ..., -0.0267, -0.0249,  0.0169],\n",
            "        [ 0.0098,  0.0048,  0.0227,  ..., -0.0236,  0.0113,  0.0237],\n",
            "        [ 0.0065, -0.0099,  0.0161,  ..., -0.0033,  0.0056,  0.0273],\n",
            "        ...,\n",
            "        [ 0.0135, -0.0195, -0.0222,  ...,  0.0173,  0.0277,  0.0052],\n",
            "        [ 0.0247,  0.0057,  0.0143,  ..., -0.0231, -0.0051, -0.0109],\n",
            "        [-0.0027,  0.0245, -0.0086,  ..., -0.0092,  0.0122,  0.0263]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.lora_down.weight', tensor([[-0.0193, -0.0158, -0.0076,  ..., -0.0039,  0.0174,  0.0177],\n",
            "        [-0.0034,  0.0185,  0.0105,  ..., -0.0073, -0.0032,  0.0195],\n",
            "        [-0.0004, -0.0126,  0.0082,  ..., -0.0054, -0.0088, -0.0023],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0053,  0.0123,  ..., -0.0060,  0.0118,  0.0018],\n",
            "        [ 0.0062, -0.0116,  0.0079,  ...,  0.0059, -0.0067,  0.0028],\n",
            "        [ 0.0057, -0.0164,  0.0151,  ..., -0.0095, -0.0007,  0.0206]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight', tensor([[ 0.0226,  0.0279, -0.0188,  ..., -0.0185, -0.0232,  0.0122],\n",
            "        [ 0.0080, -0.0145,  0.0065,  ..., -0.0006,  0.0069,  0.0006],\n",
            "        [-0.0196,  0.0060, -0.0227,  ..., -0.0234, -0.0211, -0.0160],\n",
            "        ...,\n",
            "        [ 0.0052, -0.0115,  0.0086,  ...,  0.0259, -0.0130,  0.0259],\n",
            "        [ 0.0150, -0.0097, -0.0238,  ...,  0.0253, -0.0051,  0.0071],\n",
            "        [ 0.0091, -0.0147,  0.0181,  ..., -0.0126, -0.0005, -0.0201]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.lora_down.weight', tensor([[ 4.3907e-03,  1.1559e-03,  9.0408e-03,  ...,  4.5929e-03,\n",
            "          3.3588e-03,  2.5520e-03],\n",
            "        [ 9.9792e-03, -1.0994e-02, -8.4381e-03,  ..., -3.1395e-03,\n",
            "          4.6997e-03,  3.3970e-03],\n",
            "        [ 9.2621e-03,  4.4594e-03, -1.9007e-03,  ...,  1.2787e-02,\n",
            "         -7.2708e-03, -5.3291e-03],\n",
            "        ...,\n",
            "        [-1.4172e-03, -1.3901e-02, -5.4855e-03,  ..., -5.3291e-03,\n",
            "          5.3291e-03, -5.7297e-03],\n",
            "        [-2.8305e-03, -4.6463e-03,  8.7500e-04,  ..., -9.2850e-03,\n",
            "          8.2626e-03, -5.3635e-03],\n",
            "        [-1.1330e-02, -3.1071e-03, -8.5831e-03,  ...,  7.3204e-03,\n",
            "          5.4419e-05,  5.3525e-05]], dtype=torch.float16)), ('lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_proj_in.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_proj_in.lora_down.weight', tensor([[-0.0340,  0.0177,  0.0149,  ...,  0.0079,  0.0328, -0.0353],\n",
            "        [-0.0231,  0.0089, -0.0262,  ..., -0.0102,  0.0132,  0.0313],\n",
            "        [ 0.0019, -0.0027,  0.0321,  ..., -0.0153,  0.0381, -0.0147],\n",
            "        ...,\n",
            "        [ 0.0022,  0.0298, -0.0102,  ..., -0.0097,  0.0340,  0.0173],\n",
            "        [-0.0034, -0.0349,  0.0361,  ...,  0.0390, -0.0320, -0.0140],\n",
            "        [ 0.0101, -0.0122,  0.0273,  ..., -0.0371, -0.0111, -0.0204]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_proj_in.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_proj_out.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_proj_out.lora_down.weight', tensor([[-0.0074, -0.0240,  0.0096,  ..., -0.0272,  0.0291, -0.0056],\n",
            "        [ 0.0062,  0.0295, -0.0392,  ...,  0.0163,  0.0248,  0.0337],\n",
            "        [ 0.0073, -0.0262,  0.0065,  ..., -0.0162,  0.0380,  0.0304],\n",
            "        ...,\n",
            "        [-0.0309,  0.0276,  0.0094,  ...,  0.0180,  0.0256, -0.0020],\n",
            "        [ 0.0103,  0.0048, -0.0022,  ..., -0.0034, -0.0107,  0.0242],\n",
            "        [-0.0238, -0.0128,  0.0348,  ...,  0.0056,  0.0253,  0.0055]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_proj_out.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_k.lora_down.weight', tensor([[ 0.0122, -0.0389,  0.0083,  ..., -0.0323, -0.0372, -0.0316],\n",
            "        [-0.0390,  0.0199,  0.0393,  ...,  0.0105, -0.0079,  0.0156],\n",
            "        [-0.0311, -0.0264, -0.0321,  ..., -0.0179, -0.0176, -0.0212],\n",
            "        ...,\n",
            "        [ 0.0260, -0.0231,  0.0162,  ...,  0.0054, -0.0046,  0.0145],\n",
            "        [ 0.0328, -0.0043, -0.0108,  ..., -0.0388,  0.0082, -0.0107],\n",
            "        [-0.0382, -0.0360, -0.0148,  ...,  0.0028,  0.0181,  0.0379]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight', tensor([[-0.0036,  0.0018, -0.0275,  ..., -0.0316,  0.0084, -0.0160],\n",
            "        [ 0.0350, -0.0306, -0.0155,  ...,  0.0053, -0.0065, -0.0045],\n",
            "        [-0.0157,  0.0339, -0.0119,  ...,  0.0309,  0.0002,  0.0130],\n",
            "        ...,\n",
            "        [-0.0164, -0.0071,  0.0024,  ..., -0.0209,  0.0223,  0.0274],\n",
            "        [ 0.0023, -0.0387,  0.0225,  ..., -0.0077, -0.0178, -0.0345],\n",
            "        [ 0.0298, -0.0336, -0.0038,  ...,  0.0203,  0.0318,  0.0310]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_q.lora_down.weight', tensor([[-0.0142,  0.0298,  0.0324,  ..., -0.0084, -0.0279,  0.0024],\n",
            "        [ 0.0168, -0.0165, -0.0232,  ...,  0.0137,  0.0243,  0.0317],\n",
            "        [ 0.0226,  0.0020, -0.0160,  ...,  0.0037, -0.0109, -0.0202],\n",
            "        ...,\n",
            "        [-0.0212, -0.0109,  0.0390,  ..., -0.0122,  0.0332,  0.0090],\n",
            "        [-0.0090,  0.0155, -0.0217,  ...,  0.0206, -0.0353, -0.0073],\n",
            "        [ 0.0039,  0.0025, -0.0090,  ..., -0.0268, -0.0309, -0.0239]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_v.lora_down.weight', tensor([[ 0.0209, -0.0070, -0.0243,  ..., -0.0148,  0.0209,  0.0202],\n",
            "        [-0.0198, -0.0154,  0.0254,  ...,  0.0255, -0.0140, -0.0036],\n",
            "        [ 0.0110, -0.0118, -0.0246,  ...,  0.0146, -0.0054, -0.0304],\n",
            "        ...,\n",
            "        [-0.0350,  0.0344,  0.0084,  ...,  0.0385, -0.0353,  0.0007],\n",
            "        [-0.0136, -0.0241, -0.0338,  ..., -0.0087,  0.0287,  0.0190],\n",
            "        [-0.0110, -0.0124,  0.0230,  ..., -0.0048,  0.0174,  0.0031]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_k.lora_down.weight', tensor([[ 2.1606e-02, -1.9699e-02, -1.5274e-02,  ...,  3.5744e-03,\n",
            "         -4.1122e-03,  1.6479e-02],\n",
            "        [-2.2221e-03, -1.7731e-02, -2.0569e-02,  ...,  1.0014e-05,\n",
            "         -2.1332e-02,  2.0615e-02],\n",
            "        [-1.9653e-02,  1.0666e-02,  1.1032e-02,  ...,  1.7151e-02,\n",
            "          4.4847e-04, -5.2986e-03],\n",
            "        ...,\n",
            "        [ 6.6032e-03,  1.5020e-03, -1.6144e-02,  ...,  2.1805e-02,\n",
            "         -6.2180e-03,  3.0804e-03],\n",
            "        [-1.3611e-02, -1.5549e-02, -1.6983e-02,  ...,  6.9008e-03,\n",
            "          1.2383e-02, -1.1597e-02],\n",
            "        [ 5.2719e-03,  4.9477e-03,  3.0899e-03,  ...,  9.5444e-03,\n",
            "          1.6006e-02,  1.7242e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight', tensor([[-0.0184,  0.0242,  0.0319,  ..., -0.0065,  0.0243,  0.0360],\n",
            "        [ 0.0199, -0.0010, -0.0169,  ...,  0.0127,  0.0057,  0.0031],\n",
            "        [ 0.0113,  0.0323,  0.0015,  ...,  0.0100, -0.0370,  0.0096],\n",
            "        ...,\n",
            "        [ 0.0213,  0.0265,  0.0323,  ...,  0.0028,  0.0369, -0.0374],\n",
            "        [ 0.0215,  0.0327,  0.0304,  ..., -0.0062, -0.0084, -0.0266],\n",
            "        [-0.0037, -0.0113, -0.0305,  ..., -0.0147,  0.0044,  0.0238]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_q.lora_down.weight', tensor([[ 0.0239, -0.0065, -0.0094,  ...,  0.0246, -0.0267, -0.0060],\n",
            "        [-0.0065, -0.0045, -0.0072,  ..., -0.0335, -0.0170,  0.0251],\n",
            "        [ 0.0198,  0.0066, -0.0256,  ..., -0.0293,  0.0158,  0.0317],\n",
            "        ...,\n",
            "        [-0.0163, -0.0104,  0.0149,  ..., -0.0075,  0.0049,  0.0375],\n",
            "        [-0.0170,  0.0346, -0.0393,  ..., -0.0257, -0.0190,  0.0130],\n",
            "        [ 0.0134,  0.0111, -0.0310,  ...,  0.0096, -0.0160,  0.0167]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_v.lora_down.weight', tensor([[-0.0009,  0.0213, -0.0153,  ...,  0.0027,  0.0025, -0.0109],\n",
            "        [ 0.0169,  0.0013, -0.0124,  ...,  0.0193, -0.0161,  0.0082],\n",
            "        [ 0.0051, -0.0120, -0.0208,  ..., -0.0059,  0.0008,  0.0097],\n",
            "        ...,\n",
            "        [-0.0123, -0.0057, -0.0172,  ...,  0.0179, -0.0036, -0.0110],\n",
            "        [ 0.0050, -0.0137,  0.0012,  ..., -0.0169,  0.0056,  0.0049],\n",
            "        [ 0.0072, -0.0081, -0.0200,  ...,  0.0209, -0.0059, -0.0220]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight', tensor([[ 0.0074, -0.0213,  0.0256,  ..., -0.0171,  0.0160, -0.0065],\n",
            "        [-0.0311,  0.0290,  0.0134,  ...,  0.0313, -0.0388, -0.0046],\n",
            "        [-0.0326, -0.0165,  0.0200,  ..., -0.0057, -0.0108, -0.0130],\n",
            "        ...,\n",
            "        [ 0.0049,  0.0191, -0.0266,  ..., -0.0294,  0.0244,  0.0299],\n",
            "        [-0.0282,  0.0022,  0.0172,  ..., -0.0050, -0.0149, -0.0172],\n",
            "        [-0.0278, -0.0254,  0.0164,  ...,  0.0342, -0.0249,  0.0195]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_ff_net_2.lora_down.weight', tensor([[-0.0153,  0.0017,  0.0106,  ...,  0.0002,  0.0098, -0.0001],\n",
            "        [-0.0019, -0.0155,  0.0022,  ...,  0.0162, -0.0073,  0.0078],\n",
            "        [ 0.0053,  0.0036,  0.0075,  ...,  0.0153, -0.0012,  0.0060],\n",
            "        ...,\n",
            "        [-0.0077,  0.0179, -0.0013,  ..., -0.0046, -0.0012,  0.0105],\n",
            "        [ 0.0129,  0.0010, -0.0016,  ..., -0.0107,  0.0036, -0.0116],\n",
            "        [-0.0073,  0.0036, -0.0175,  ...,  0.0115,  0.0083, -0.0082]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_0_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_k.lora_down.weight', tensor([[ 4.2725e-03,  3.4973e-02,  3.3569e-02,  ...,  1.6602e-02,\n",
            "         -2.4582e-02, -2.6306e-02],\n",
            "        [ 3.8696e-02, -1.5579e-02,  1.5316e-03,  ..., -1.4519e-02,\n",
            "         -2.3895e-02, -1.5022e-02],\n",
            "        [-2.6489e-02, -1.0147e-02,  2.0737e-02,  ...,  1.4709e-02,\n",
            "         -3.2715e-02,  3.4210e-02],\n",
            "        ...,\n",
            "        [-7.9346e-03,  1.7872e-03,  1.2749e-02,  ..., -2.4780e-02,\n",
            "         -7.4196e-03, -3.4332e-02],\n",
            "        [-8.5297e-03, -2.8244e-02,  1.8646e-02,  ..., -2.0081e-02,\n",
            "         -4.9896e-03, -1.3588e-02],\n",
            "        [-1.2978e-02,  2.2964e-02, -1.7746e-02,  ...,  4.5359e-05,\n",
            "          1.3779e-02,  3.8269e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight', tensor([[ 0.0154, -0.0323, -0.0157,  ..., -0.0015, -0.0038, -0.0011],\n",
            "        [-0.0341,  0.0193, -0.0237,  ..., -0.0367, -0.0279, -0.0244],\n",
            "        [-0.0061,  0.0115, -0.0270,  ..., -0.0105, -0.0098,  0.0163],\n",
            "        ...,\n",
            "        [ 0.0383, -0.0333,  0.0160,  ...,  0.0167,  0.0366,  0.0298],\n",
            "        [ 0.0200, -0.0204,  0.0234,  ...,  0.0256, -0.0284, -0.0170],\n",
            "        [ 0.0315, -0.0106,  0.0300,  ..., -0.0336, -0.0151,  0.0306]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_q.lora_down.weight', tensor([[-0.0278, -0.0092,  0.0179,  ...,  0.0160, -0.0079, -0.0157],\n",
            "        [-0.0369,  0.0147,  0.0157,  ..., -0.0211,  0.0331, -0.0258],\n",
            "        [-0.0194,  0.0061,  0.0166,  ..., -0.0159,  0.0222,  0.0315],\n",
            "        ...,\n",
            "        [-0.0242,  0.0116,  0.0080,  ..., -0.0211, -0.0226,  0.0239],\n",
            "        [ 0.0305,  0.0295, -0.0079,  ...,  0.0077,  0.0351,  0.0051],\n",
            "        [ 0.0014, -0.0282,  0.0162,  ...,  0.0199,  0.0293,  0.0201]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_v.lora_down.weight', tensor([[-0.0366, -0.0319, -0.0302,  ...,  0.0155,  0.0103,  0.0023],\n",
            "        [ 0.0041, -0.0166, -0.0325,  ...,  0.0151, -0.0342,  0.0316],\n",
            "        [ 0.0252, -0.0250, -0.0264,  ..., -0.0209,  0.0218,  0.0049],\n",
            "        ...,\n",
            "        [ 0.0295, -0.0065,  0.0269,  ...,  0.0391,  0.0212, -0.0338],\n",
            "        [-0.0192,  0.0192, -0.0378,  ..., -0.0173,  0.0136, -0.0031],\n",
            "        [-0.0362, -0.0136, -0.0062,  ..., -0.0296,  0.0149,  0.0099]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_k.lora_down.weight', tensor([[-0.0187, -0.0140,  0.0162,  ..., -0.0064, -0.0004,  0.0100],\n",
            "        [-0.0159, -0.0135, -0.0159,  ...,  0.0019,  0.0190,  0.0125],\n",
            "        [ 0.0172, -0.0103,  0.0067,  ..., -0.0179,  0.0109, -0.0058],\n",
            "        ...,\n",
            "        [ 0.0166, -0.0063, -0.0203,  ..., -0.0197, -0.0149,  0.0190],\n",
            "        [-0.0085,  0.0137,  0.0214,  ...,  0.0192, -0.0137, -0.0025],\n",
            "        [-0.0175, -0.0213, -0.0181,  ..., -0.0071, -0.0057,  0.0010]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight', tensor([[-0.0141, -0.0218, -0.0044,  ..., -0.0180, -0.0144, -0.0235],\n",
            "        [-0.0230,  0.0106, -0.0333,  ...,  0.0015, -0.0009,  0.0147],\n",
            "        [-0.0013, -0.0119, -0.0386,  ...,  0.0040, -0.0131, -0.0076],\n",
            "        ...,\n",
            "        [ 0.0178,  0.0146, -0.0327,  ..., -0.0120,  0.0002,  0.0355],\n",
            "        [-0.0352, -0.0240, -0.0245,  ..., -0.0068,  0.0352, -0.0232],\n",
            "        [ 0.0279,  0.0282, -0.0266,  ..., -0.0092, -0.0315,  0.0187]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_q.lora_down.weight', tensor([[-0.0131,  0.0370, -0.0047,  ...,  0.0123, -0.0395, -0.0376],\n",
            "        [-0.0377, -0.0124, -0.0241,  ..., -0.0282, -0.0070, -0.0097],\n",
            "        [ 0.0251, -0.0309, -0.0045,  ...,  0.0066, -0.0351, -0.0067],\n",
            "        ...,\n",
            "        [-0.0259,  0.0040, -0.0022,  ..., -0.0073, -0.0293,  0.0195],\n",
            "        [-0.0207, -0.0383,  0.0131,  ...,  0.0337,  0.0356,  0.0199],\n",
            "        [-0.0128, -0.0299, -0.0378,  ...,  0.0292,  0.0011,  0.0155]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_v.lora_down.weight', tensor([[-0.0053, -0.0104,  0.0112,  ...,  0.0007, -0.0148, -0.0085],\n",
            "        [-0.0052,  0.0154,  0.0143,  ..., -0.0170, -0.0172, -0.0192],\n",
            "        [-0.0051,  0.0134,  0.0052,  ..., -0.0059, -0.0017, -0.0058],\n",
            "        ...,\n",
            "        [ 0.0173,  0.0074, -0.0178,  ...,  0.0009,  0.0217, -0.0190],\n",
            "        [ 0.0120, -0.0181,  0.0138,  ..., -0.0103, -0.0186, -0.0073],\n",
            "        [ 0.0196,  0.0118,  0.0107,  ..., -0.0099,  0.0172, -0.0182]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight', tensor([[-0.0340, -0.0297, -0.0064,  ..., -0.0196, -0.0115, -0.0283],\n",
            "        [ 0.0351, -0.0298,  0.0108,  ..., -0.0077,  0.0006, -0.0180],\n",
            "        [-0.0080,  0.0387,  0.0285,  ..., -0.0335,  0.0096, -0.0196],\n",
            "        ...,\n",
            "        [-0.0004,  0.0179,  0.0084,  ..., -0.0330, -0.0113, -0.0093],\n",
            "        [ 0.0135, -0.0034, -0.0051,  ..., -0.0212,  0.0125, -0.0356],\n",
            "        [-0.0180, -0.0005, -0.0385,  ...,  0.0101, -0.0161, -0.0301]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_ff_net_2.lora_down.weight', tensor([[-0.0118, -0.0118,  0.0109,  ..., -0.0195,  0.0034, -0.0053],\n",
            "        [-0.0151, -0.0051,  0.0074,  ...,  0.0134, -0.0011,  0.0126],\n",
            "        [-0.0169, -0.0189, -0.0148,  ...,  0.0083, -0.0113, -0.0112],\n",
            "        ...,\n",
            "        [-0.0173, -0.0051, -0.0120,  ...,  0.0127,  0.0195,  0.0128],\n",
            "        [ 0.0163,  0.0125,  0.0031,  ..., -0.0126,  0.0046,  0.0121],\n",
            "        [ 0.0116,  0.0009, -0.0028,  ..., -0.0034, -0.0080, -0.0006]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_3_1_transformer_blocks_1_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_proj_in.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_proj_in.lora_down.weight', tensor([[ 0.0379, -0.0251, -0.0282,  ...,  0.0039, -0.0063,  0.0062],\n",
            "        [ 0.0036,  0.0051, -0.0214,  ..., -0.0317,  0.0104,  0.0392],\n",
            "        [-0.0121, -0.0239, -0.0035,  ...,  0.0088, -0.0061,  0.0076],\n",
            "        ...,\n",
            "        [ 0.0169, -0.0131,  0.0052,  ...,  0.0286,  0.0126,  0.0200],\n",
            "        [ 0.0336, -0.0275, -0.0037,  ..., -0.0034, -0.0114, -0.0120],\n",
            "        [-0.0061, -0.0122, -0.0251,  ..., -0.0125,  0.0265, -0.0047]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_proj_in.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_proj_out.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_proj_out.lora_down.weight', tensor([[-0.0314, -0.0335, -0.0171,  ..., -0.0304, -0.0074,  0.0230],\n",
            "        [-0.0236,  0.0257, -0.0246,  ...,  0.0318, -0.0093, -0.0277],\n",
            "        [ 0.0017, -0.0275, -0.0173,  ..., -0.0242, -0.0358,  0.0083],\n",
            "        ...,\n",
            "        [-0.0092,  0.0106, -0.0007,  ..., -0.0101, -0.0264,  0.0270],\n",
            "        [ 0.0088,  0.0245,  0.0327,  ..., -0.0095, -0.0370, -0.0071],\n",
            "        [-0.0122, -0.0197,  0.0154,  ...,  0.0141,  0.0306, -0.0068]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_proj_out.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_k.lora_down.weight', tensor([[-0.0304, -0.0350,  0.0051,  ...,  0.0300,  0.0115, -0.0032],\n",
            "        [ 0.0080,  0.0072, -0.0205,  ..., -0.0112, -0.0219,  0.0393],\n",
            "        [-0.0342, -0.0340, -0.0319,  ..., -0.0239,  0.0188, -0.0215],\n",
            "        ...,\n",
            "        [-0.0022, -0.0351, -0.0291,  ..., -0.0098,  0.0079,  0.0289],\n",
            "        [ 0.0193, -0.0125, -0.0295,  ..., -0.0363,  0.0216, -0.0107],\n",
            "        [ 0.0367,  0.0350, -0.0006,  ..., -0.0085, -0.0155,  0.0134]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight', tensor([[-0.0190,  0.0022, -0.0353,  ..., -0.0146, -0.0106, -0.0290],\n",
            "        [-0.0132, -0.0247,  0.0255,  ..., -0.0115, -0.0283,  0.0306],\n",
            "        [-0.0041,  0.0175,  0.0323,  ..., -0.0148, -0.0167,  0.0123],\n",
            "        ...,\n",
            "        [ 0.0029, -0.0162, -0.0029,  ...,  0.0046, -0.0187, -0.0004],\n",
            "        [-0.0249, -0.0332, -0.0363,  ...,  0.0001, -0.0376,  0.0192],\n",
            "        [ 0.0274, -0.0115,  0.0142,  ...,  0.0130, -0.0164, -0.0221]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_q.lora_down.weight', tensor([[ 0.0056, -0.0274,  0.0123,  ...,  0.0022, -0.0099, -0.0216],\n",
            "        [-0.0315,  0.0087,  0.0323,  ...,  0.0289, -0.0360, -0.0368],\n",
            "        [-0.0155, -0.0117, -0.0300,  ...,  0.0036, -0.0327, -0.0134],\n",
            "        ...,\n",
            "        [ 0.0007,  0.0197,  0.0162,  ...,  0.0231,  0.0173,  0.0152],\n",
            "        [-0.0287, -0.0124,  0.0163,  ..., -0.0103, -0.0232, -0.0157],\n",
            "        [-0.0007,  0.0068,  0.0082,  ...,  0.0113, -0.0183,  0.0133]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_v.lora_down.weight', tensor([[-0.0030,  0.0229, -0.0043,  ..., -0.0269,  0.0200, -0.0373],\n",
            "        [-0.0154,  0.0041, -0.0317,  ...,  0.0110,  0.0233,  0.0359],\n",
            "        [ 0.0163, -0.0023, -0.0048,  ...,  0.0068,  0.0330,  0.0295],\n",
            "        ...,\n",
            "        [ 0.0188,  0.0029,  0.0119,  ..., -0.0066,  0.0208,  0.0019],\n",
            "        [-0.0161, -0.0363,  0.0350,  ..., -0.0291, -0.0015,  0.0336],\n",
            "        [ 0.0124,  0.0080, -0.0126,  ...,  0.0192,  0.0298, -0.0320]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_k.lora_down.weight', tensor([[ 0.0108,  0.0210, -0.0059,  ...,  0.0200, -0.0108,  0.0177],\n",
            "        [ 0.0169,  0.0019,  0.0098,  ...,  0.0136,  0.0170,  0.0195],\n",
            "        [-0.0121,  0.0124,  0.0211,  ..., -0.0010, -0.0213,  0.0089],\n",
            "        ...,\n",
            "        [-0.0181, -0.0012,  0.0057,  ..., -0.0079,  0.0200, -0.0211],\n",
            "        [-0.0185,  0.0194, -0.0167,  ...,  0.0004,  0.0080, -0.0127],\n",
            "        [ 0.0190, -0.0058,  0.0061,  ...,  0.0069, -0.0171,  0.0038]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight', tensor([[-0.0357,  0.0021, -0.0030,  ..., -0.0187, -0.0240,  0.0045],\n",
            "        [-0.0321,  0.0078, -0.0048,  ..., -0.0059, -0.0147, -0.0285],\n",
            "        [-0.0346, -0.0272,  0.0084,  ..., -0.0207, -0.0340, -0.0005],\n",
            "        ...,\n",
            "        [-0.0351, -0.0050, -0.0079,  ...,  0.0131, -0.0022,  0.0015],\n",
            "        [-0.0088, -0.0002, -0.0050,  ...,  0.0231,  0.0118, -0.0018],\n",
            "        [ 0.0060,  0.0113, -0.0280,  ...,  0.0077, -0.0379, -0.0147]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_q.lora_down.weight', tensor([[ 0.0049,  0.0318, -0.0201,  ...,  0.0041,  0.0041, -0.0238],\n",
            "        [-0.0242, -0.0213, -0.0339,  ...,  0.0359,  0.0362, -0.0229],\n",
            "        [-0.0226, -0.0146, -0.0182,  ...,  0.0367,  0.0309,  0.0282],\n",
            "        ...,\n",
            "        [-0.0123,  0.0356, -0.0210,  ..., -0.0314, -0.0241,  0.0246],\n",
            "        [ 0.0230, -0.0378, -0.0382,  ...,  0.0344,  0.0107,  0.0188],\n",
            "        [ 0.0177, -0.0148, -0.0247,  ...,  0.0243,  0.0078, -0.0201]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_v.lora_down.weight', tensor([[-0.0088,  0.0096, -0.0119,  ...,  0.0093,  0.0147,  0.0172],\n",
            "        [ 0.0036,  0.0036, -0.0113,  ...,  0.0032, -0.0147,  0.0187],\n",
            "        [ 0.0071, -0.0094, -0.0176,  ...,  0.0084, -0.0012,  0.0136],\n",
            "        ...,\n",
            "        [ 0.0200,  0.0106,  0.0010,  ...,  0.0205, -0.0132,  0.0071],\n",
            "        [-0.0027,  0.0072,  0.0120,  ..., -0.0010, -0.0176,  0.0114],\n",
            "        [ 0.0217, -0.0077,  0.0111,  ..., -0.0037,  0.0077,  0.0191]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight', tensor([[-0.0125, -0.0293, -0.0307,  ..., -0.0200, -0.0149, -0.0049],\n",
            "        [-0.0230,  0.0086, -0.0312,  ..., -0.0052, -0.0018,  0.0122],\n",
            "        [ 0.0085, -0.0176,  0.0299,  ..., -0.0117, -0.0029,  0.0095],\n",
            "        ...,\n",
            "        [-0.0020,  0.0323, -0.0241,  ..., -0.0343,  0.0271,  0.0179],\n",
            "        [ 0.0263, -0.0074,  0.0039,  ..., -0.0098, -0.0161,  0.0162],\n",
            "        [ 0.0171, -0.0016,  0.0312,  ..., -0.0009, -0.0305, -0.0060]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_ff_net_2.lora_down.weight', tensor([[ 0.0068, -0.0063, -0.0072,  ...,  0.0086,  0.0193, -0.0137],\n",
            "        [ 0.0192, -0.0004,  0.0138,  ..., -0.0155, -0.0119, -0.0172],\n",
            "        [ 0.0177, -0.0198, -0.0006,  ...,  0.0173, -0.0073, -0.0095],\n",
            "        ...,\n",
            "        [-0.0084,  0.0157, -0.0132,  ..., -0.0076, -0.0120,  0.0160],\n",
            "        [-0.0116,  0.0194, -0.0006,  ...,  0.0070, -0.0194,  0.0140],\n",
            "        [-0.0030, -0.0040,  0.0109,  ..., -0.0060, -0.0106, -0.0090]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_0_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_k.lora_down.weight', tensor([[-0.0382,  0.0188, -0.0181,  ...,  0.0267, -0.0007,  0.0021],\n",
            "        [ 0.0254,  0.0370, -0.0215,  ...,  0.0256,  0.0339,  0.0284],\n",
            "        [-0.0164,  0.0190,  0.0390,  ...,  0.0014,  0.0307,  0.0142],\n",
            "        ...,\n",
            "        [-0.0315, -0.0170,  0.0292,  ...,  0.0109, -0.0006, -0.0336],\n",
            "        [-0.0188,  0.0391, -0.0054,  ...,  0.0119,  0.0130, -0.0365],\n",
            "        [ 0.0160, -0.0022, -0.0367,  ..., -0.0356,  0.0127,  0.0367]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight', tensor([[ 0.0003,  0.0026,  0.0318,  ..., -0.0195,  0.0310,  0.0160],\n",
            "        [ 0.0099,  0.0230,  0.0262,  ..., -0.0357, -0.0094,  0.0385],\n",
            "        [-0.0263, -0.0328, -0.0012,  ...,  0.0084,  0.0069, -0.0307],\n",
            "        ...,\n",
            "        [-0.0008,  0.0361, -0.0306,  ..., -0.0266,  0.0250, -0.0283],\n",
            "        [-0.0008, -0.0125,  0.0172,  ..., -0.0279, -0.0288,  0.0189],\n",
            "        [-0.0120,  0.0285, -0.0152,  ...,  0.0181, -0.0039, -0.0031]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_q.lora_down.weight', tensor([[ 0.0234,  0.0024,  0.0279,  ...,  0.0108,  0.0392, -0.0210],\n",
            "        [-0.0046,  0.0133,  0.0032,  ...,  0.0038,  0.0329, -0.0288],\n",
            "        [-0.0175, -0.0115, -0.0282,  ..., -0.0193,  0.0253, -0.0174],\n",
            "        ...,\n",
            "        [-0.0324, -0.0306,  0.0213,  ...,  0.0169,  0.0251, -0.0136],\n",
            "        [ 0.0170, -0.0087, -0.0125,  ..., -0.0273,  0.0223,  0.0050],\n",
            "        [ 0.0113, -0.0002,  0.0189,  ..., -0.0241, -0.0370, -0.0363]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_v.lora_down.weight', tensor([[-3.7628e-02, -2.0844e-02, -2.7039e-02,  ..., -7.0632e-05,\n",
            "         -3.8109e-03,  1.5343e-02],\n",
            "        [-2.2675e-02, -2.2232e-02,  1.6129e-02,  ..., -4.8676e-03,\n",
            "         -1.0841e-02,  5.8403e-03],\n",
            "        [-3.9215e-02, -3.3051e-02,  6.1722e-03,  ..., -1.7502e-02,\n",
            "          3.4241e-02, -2.1713e-02],\n",
            "        ...,\n",
            "        [-1.2680e-02, -1.1024e-02,  3.1433e-02,  ...,  3.6926e-02,\n",
            "         -7.0143e-04,  2.0447e-02],\n",
            "        [ 3.5915e-03, -1.8997e-02,  3.8177e-02,  ..., -2.1942e-02,\n",
            "          2.7557e-02,  4.7226e-03],\n",
            "        [ 5.2643e-03,  1.7715e-02, -3.4088e-02,  ..., -2.5223e-02,\n",
            "         -2.3209e-02,  5.9166e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_k.lora_down.weight', tensor([[ 1.7502e-02,  1.3924e-02, -1.5884e-02,  ...,  1.3687e-02,\n",
            "          5.9700e-04,  4.6043e-03],\n",
            "        [-6.2981e-03, -1.1765e-02,  6.0349e-03,  ..., -1.3321e-02,\n",
            "         -3.4065e-03,  3.4904e-03],\n",
            "        [-1.5274e-02, -2.8210e-03,  2.3746e-03,  ..., -3.6316e-03,\n",
            "          2.2018e-02, -1.8524e-02],\n",
            "        ...,\n",
            "        [-7.0686e-03, -1.0841e-02, -1.1650e-02,  ..., -7.1049e-04,\n",
            "         -3.5906e-04,  1.2695e-02],\n",
            "        [-1.0939e-03,  2.1515e-02,  2.1271e-02,  ..., -1.2732e-03,\n",
            "          3.2692e-03, -1.6129e-02],\n",
            "        [-3.6359e-05, -1.0193e-02,  1.8860e-02,  ..., -1.5564e-02,\n",
            "         -1.3094e-03, -1.3901e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight', tensor([[ 0.0199,  0.0033,  0.0053,  ...,  0.0223, -0.0285, -0.0022],\n",
            "        [ 0.0282,  0.0259, -0.0114,  ...,  0.0018,  0.0320, -0.0271],\n",
            "        [-0.0037,  0.0275,  0.0122,  ..., -0.0339, -0.0171, -0.0064],\n",
            "        ...,\n",
            "        [ 0.0110, -0.0299,  0.0329,  ..., -0.0053, -0.0137, -0.0062],\n",
            "        [ 0.0284, -0.0051,  0.0299,  ...,  0.0347, -0.0156, -0.0391],\n",
            "        [-0.0078, -0.0128,  0.0342,  ..., -0.0157, -0.0032, -0.0198]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_q.lora_down.weight', tensor([[-0.0361, -0.0215, -0.0154,  ..., -0.0267,  0.0154, -0.0273],\n",
            "        [ 0.0129, -0.0149, -0.0318,  ..., -0.0374, -0.0121,  0.0308],\n",
            "        [ 0.0178, -0.0251,  0.0042,  ...,  0.0038, -0.0374, -0.0019],\n",
            "        ...,\n",
            "        [ 0.0004,  0.0377,  0.0132,  ...,  0.0283,  0.0002, -0.0145],\n",
            "        [-0.0007, -0.0287, -0.0125,  ..., -0.0213,  0.0203, -0.0302],\n",
            "        [-0.0190, -0.0201,  0.0180,  ...,  0.0031,  0.0258,  0.0125]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_v.lora_down.weight', tensor([[ 0.0018, -0.0096,  0.0063,  ...,  0.0209,  0.0009,  0.0014],\n",
            "        [-0.0134,  0.0170,  0.0206,  ...,  0.0093, -0.0102, -0.0154],\n",
            "        [ 0.0061,  0.0016, -0.0127,  ..., -0.0002,  0.0010,  0.0047],\n",
            "        ...,\n",
            "        [ 0.0118,  0.0124,  0.0190,  ...,  0.0129,  0.0164, -0.0211],\n",
            "        [ 0.0066, -0.0067, -0.0184,  ...,  0.0189,  0.0074, -0.0045],\n",
            "        [ 0.0080, -0.0186,  0.0201,  ...,  0.0059, -0.0015, -0.0174]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight', tensor([[ 0.0302,  0.0352,  0.0198,  ...,  0.0276, -0.0190, -0.0102],\n",
            "        [ 0.0002, -0.0047, -0.0346,  ..., -0.0125,  0.0187,  0.0226],\n",
            "        [-0.0072, -0.0255, -0.0390,  ...,  0.0050,  0.0074, -0.0197],\n",
            "        ...,\n",
            "        [-0.0020,  0.0349,  0.0119,  ..., -0.0288, -0.0284, -0.0250],\n",
            "        [-0.0059, -0.0003,  0.0316,  ...,  0.0135,  0.0243,  0.0241],\n",
            "        [ 0.0194,  0.0177, -0.0339,  ..., -0.0326, -0.0121,  0.0180]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_ff_net_2.lora_down.weight', tensor([[-0.0176, -0.0073,  0.0044,  ..., -0.0083, -0.0069, -0.0063],\n",
            "        [-0.0118, -0.0072, -0.0008,  ..., -0.0049,  0.0145,  0.0022],\n",
            "        [-0.0040,  0.0166, -0.0071,  ..., -0.0038, -0.0035,  0.0070],\n",
            "        ...,\n",
            "        [ 0.0169, -0.0018, -0.0057,  ...,  0.0045, -0.0139, -0.0179],\n",
            "        [-0.0128, -0.0112, -0.0023,  ...,  0.0159,  0.0092,  0.0045],\n",
            "        [ 0.0011, -0.0096, -0.0137,  ...,  0.0123, -0.0101,  0.0156]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_4_1_transformer_blocks_1_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_proj_in.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_proj_in.lora_down.weight', tensor([[-1.7166e-02,  9.1095e-03,  2.0385e-05,  ...,  3.4546e-02,\n",
            "          2.6917e-02,  3.1158e-02],\n",
            "        [ 1.2291e-02, -2.0493e-02, -1.9424e-02,  ..., -2.3499e-02,\n",
            "          6.4735e-03, -3.3020e-02],\n",
            "        [ 3.0701e-02,  3.2684e-02, -1.6388e-02,  ...,  3.7292e-02,\n",
            "         -1.7226e-04, -1.1795e-02],\n",
            "        ...,\n",
            "        [-3.9398e-02,  2.2217e-02, -1.0719e-02,  ...,  7.5226e-03,\n",
            "          3.6835e-02,  1.4320e-02],\n",
            "        [ 1.4526e-02, -5.0201e-03, -5.9748e-04,  ...,  1.8738e-02,\n",
            "         -1.5671e-02, -2.7847e-03],\n",
            "        [-1.7761e-02, -1.7670e-02,  2.1805e-02,  ..., -3.8788e-02,\n",
            "         -2.0966e-02, -9.7351e-03]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_proj_in.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_proj_out.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_proj_out.lora_down.weight', tensor([[ 0.0129,  0.0155,  0.0198,  ..., -0.0309, -0.0015, -0.0381],\n",
            "        [-0.0162, -0.0339,  0.0337,  ..., -0.0071,  0.0289,  0.0067],\n",
            "        [-0.0166, -0.0093, -0.0376,  ...,  0.0005,  0.0224,  0.0238],\n",
            "        ...,\n",
            "        [ 0.0277,  0.0299, -0.0218,  ...,  0.0123,  0.0201, -0.0327],\n",
            "        [-0.0060,  0.0080,  0.0162,  ..., -0.0227,  0.0076, -0.0150],\n",
            "        [ 0.0349, -0.0389,  0.0187,  ...,  0.0371, -0.0355,  0.0162]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_proj_out.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_k.lora_down.weight', tensor([[-0.0127,  0.0095,  0.0335,  ...,  0.0344,  0.0133,  0.0016],\n",
            "        [-0.0285,  0.0116,  0.0331,  ...,  0.0268, -0.0355,  0.0387],\n",
            "        [ 0.0053,  0.0108,  0.0303,  ..., -0.0298, -0.0175,  0.0022],\n",
            "        ...,\n",
            "        [-0.0193, -0.0072, -0.0056,  ..., -0.0293,  0.0297,  0.0099],\n",
            "        [ 0.0182,  0.0190, -0.0015,  ..., -0.0324, -0.0319, -0.0012],\n",
            "        [-0.0169,  0.0395,  0.0360,  ..., -0.0263,  0.0293, -0.0334]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight', tensor([[-0.0393,  0.0340, -0.0253,  ...,  0.0164,  0.0136,  0.0138],\n",
            "        [-0.0292,  0.0379,  0.0161,  ...,  0.0082,  0.0131, -0.0376],\n",
            "        [-0.0011, -0.0348, -0.0074,  ..., -0.0308, -0.0204,  0.0006],\n",
            "        ...,\n",
            "        [-0.0335,  0.0256,  0.0107,  ...,  0.0168,  0.0035, -0.0104],\n",
            "        [-0.0270, -0.0039,  0.0112,  ..., -0.0135, -0.0129,  0.0395],\n",
            "        [-0.0354,  0.0372, -0.0119,  ...,  0.0237,  0.0258, -0.0103]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_q.lora_down.weight', tensor([[-2.1011e-02,  8.7662e-03, -6.3438e-03,  ..., -1.2985e-02,\n",
            "          2.7008e-03,  7.1526e-03],\n",
            "        [ 1.8677e-02,  1.6037e-02, -1.7746e-02,  ...,  5.6553e-04,\n",
            "          2.0157e-02, -2.0340e-02],\n",
            "        [-3.1097e-02, -2.9831e-02,  1.6510e-02,  ...,  3.5278e-02,\n",
            "          2.4811e-02, -1.4595e-02],\n",
            "        ...,\n",
            "        [ 3.2257e-02,  2.6779e-02, -2.9388e-02,  ...,  3.3478e-02,\n",
            "         -2.1088e-02,  1.9333e-02],\n",
            "        [ 2.0798e-02, -8.8120e-03,  2.0996e-02,  ..., -3.1233e-05,\n",
            "         -2.6611e-02, -1.8280e-02],\n",
            "        [-5.0201e-03, -9.0332e-03, -7.9269e-03,  ...,  4.5319e-03,\n",
            "         -3.2104e-02,  3.8544e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_v.lora_down.weight', tensor([[-0.0360,  0.0291,  0.0203,  ..., -0.0235, -0.0023, -0.0186],\n",
            "        [-0.0383,  0.0175, -0.0133,  ...,  0.0293, -0.0215, -0.0310],\n",
            "        [ 0.0161, -0.0139,  0.0308,  ...,  0.0146,  0.0243, -0.0337],\n",
            "        ...,\n",
            "        [-0.0354,  0.0026, -0.0158,  ..., -0.0072,  0.0014,  0.0312],\n",
            "        [-0.0178,  0.0201,  0.0216,  ...,  0.0377, -0.0066, -0.0104],\n",
            "        [-0.0314, -0.0375,  0.0322,  ...,  0.0184,  0.0283, -0.0079]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_k.lora_down.weight', tensor([[ 5.2948e-03,  1.7929e-02, -8.8425e-03,  ..., -2.9488e-03,\n",
            "         -9.4528e-03,  2.0370e-02],\n",
            "        [ 1.1604e-02, -1.4282e-02,  6.1264e-03,  ..., -1.6785e-02,\n",
            "         -1.2985e-02, -2.3842e-07],\n",
            "        [-4.2953e-03,  2.1332e-02,  1.8396e-03,  ..., -2.1912e-02,\n",
            "         -1.1253e-02,  1.8845e-02],\n",
            "        ...,\n",
            "        [-6.6109e-03,  2.2064e-02,  2.1362e-02,  ...,  1.6113e-02,\n",
            "         -1.0284e-02, -1.4153e-02],\n",
            "        [ 1.5678e-03,  1.1238e-02,  6.4774e-03,  ...,  1.6190e-02,\n",
            "         -1.7746e-02, -8.7585e-03],\n",
            "        [ 1.2451e-02,  1.6251e-02,  1.0061e-04,  ..., -1.9272e-02,\n",
            "          1.3847e-02, -1.0994e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight', tensor([[ 0.0133,  0.0152, -0.0228,  ..., -0.0068, -0.0099,  0.0203],\n",
            "        [-0.0349,  0.0359,  0.0106,  ...,  0.0169,  0.0103,  0.0080],\n",
            "        [-0.0011, -0.0112,  0.0348,  ..., -0.0213,  0.0298, -0.0083],\n",
            "        ...,\n",
            "        [ 0.0041, -0.0355,  0.0005,  ..., -0.0167, -0.0044,  0.0393],\n",
            "        [ 0.0325,  0.0043, -0.0308,  ..., -0.0235,  0.0374,  0.0319],\n",
            "        [ 0.0370, -0.0341, -0.0120,  ..., -0.0358,  0.0006, -0.0325]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_q.lora_down.weight', tensor([[-0.0266,  0.0117, -0.0175,  ...,  0.0384,  0.0324,  0.0103],\n",
            "        [ 0.0050, -0.0299,  0.0123,  ...,  0.0328,  0.0288, -0.0165],\n",
            "        [ 0.0143,  0.0166,  0.0269,  ...,  0.0022, -0.0207,  0.0247],\n",
            "        ...,\n",
            "        [ 0.0294,  0.0208, -0.0095,  ..., -0.0021, -0.0187,  0.0335],\n",
            "        [-0.0019, -0.0159, -0.0387,  ...,  0.0285,  0.0305,  0.0070],\n",
            "        [ 0.0007, -0.0349,  0.0331,  ..., -0.0200, -0.0100,  0.0389]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_v.lora_down.weight', tensor([[-0.0136, -0.0015,  0.0034,  ...,  0.0034, -0.0061, -0.0191],\n",
            "        [-0.0206,  0.0105,  0.0046,  ..., -0.0098,  0.0154,  0.0084],\n",
            "        [ 0.0010, -0.0221,  0.0010,  ..., -0.0200, -0.0166,  0.0140],\n",
            "        ...,\n",
            "        [ 0.0208,  0.0090,  0.0083,  ..., -0.0156,  0.0024,  0.0141],\n",
            "        [-0.0190, -0.0210, -0.0036,  ...,  0.0185, -0.0045,  0.0171],\n",
            "        [-0.0155, -0.0090,  0.0012,  ...,  0.0102,  0.0123,  0.0216]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight', tensor([[-0.0251, -0.0203,  0.0012,  ..., -0.0367, -0.0031, -0.0211],\n",
            "        [ 0.0163, -0.0071, -0.0363,  ..., -0.0227, -0.0195, -0.0018],\n",
            "        [-0.0110,  0.0165,  0.0031,  ...,  0.0111,  0.0076, -0.0273],\n",
            "        ...,\n",
            "        [-0.0118,  0.0335, -0.0226,  ...,  0.0032, -0.0138, -0.0080],\n",
            "        [-0.0125, -0.0161, -0.0345,  ..., -0.0118,  0.0319, -0.0109],\n",
            "        [ 0.0368,  0.0125,  0.0287,  ...,  0.0011, -0.0023, -0.0072]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_ff_net_2.lora_down.weight', tensor([[ 0.0124,  0.0174, -0.0051,  ..., -0.0192,  0.0121,  0.0035],\n",
            "        [ 0.0057,  0.0138,  0.0049,  ...,  0.0084, -0.0146, -0.0065],\n",
            "        [ 0.0074,  0.0036, -0.0154,  ..., -0.0161, -0.0086, -0.0024],\n",
            "        ...,\n",
            "        [ 0.0182, -0.0005, -0.0114,  ...,  0.0030, -0.0012, -0.0041],\n",
            "        [ 0.0176,  0.0057,  0.0039,  ..., -0.0055,  0.0044,  0.0085],\n",
            "        [-0.0022,  0.0135, -0.0121,  ...,  0.0157, -0.0098,  0.0031]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_0_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_k.lora_down.weight', tensor([[-0.0250,  0.0022, -0.0342,  ...,  0.0220,  0.0085,  0.0198],\n",
            "        [ 0.0190,  0.0240, -0.0143,  ..., -0.0245, -0.0089,  0.0139],\n",
            "        [-0.0277,  0.0122, -0.0262,  ...,  0.0183,  0.0282,  0.0239],\n",
            "        ...,\n",
            "        [-0.0042,  0.0210,  0.0186,  ..., -0.0125,  0.0194, -0.0200],\n",
            "        [ 0.0141,  0.0031,  0.0057,  ..., -0.0150,  0.0197,  0.0175],\n",
            "        [-0.0208, -0.0174, -0.0192,  ..., -0.0393, -0.0097,  0.0198]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight', tensor([[ 0.0073,  0.0076, -0.0207,  ...,  0.0068, -0.0215, -0.0261],\n",
            "        [-0.0273, -0.0258,  0.0057,  ...,  0.0370, -0.0336,  0.0268],\n",
            "        [ 0.0059, -0.0261,  0.0055,  ...,  0.0378,  0.0099,  0.0381],\n",
            "        ...,\n",
            "        [-0.0264, -0.0054, -0.0178,  ...,  0.0096, -0.0330, -0.0063],\n",
            "        [-0.0050,  0.0187,  0.0024,  ...,  0.0158, -0.0365, -0.0290],\n",
            "        [-0.0108,  0.0300, -0.0067,  ...,  0.0203, -0.0045, -0.0181]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_q.lora_down.weight', tensor([[ 0.0347,  0.0160,  0.0324,  ..., -0.0064,  0.0367, -0.0139],\n",
            "        [-0.0380, -0.0216,  0.0302,  ..., -0.0338, -0.0241,  0.0211],\n",
            "        [-0.0106, -0.0091,  0.0339,  ...,  0.0368,  0.0287, -0.0059],\n",
            "        ...,\n",
            "        [ 0.0177,  0.0007, -0.0210,  ...,  0.0055, -0.0109,  0.0253],\n",
            "        [-0.0061,  0.0137,  0.0370,  ..., -0.0089,  0.0390, -0.0346],\n",
            "        [-0.0305, -0.0276, -0.0127,  ..., -0.0356, -0.0156,  0.0279]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_v.lora_down.weight', tensor([[ 1.2732e-03,  1.6336e-03,  3.1319e-03,  ..., -9.1248e-03,\n",
            "          3.4515e-02, -3.3630e-02],\n",
            "        [-1.0826e-02,  4.2379e-05,  3.8696e-02,  ...,  1.0155e-02,\n",
            "          5.1346e-03,  1.0811e-02],\n",
            "        [ 1.8433e-02,  1.6022e-02, -2.3193e-02,  ...,  3.8513e-02,\n",
            "         -2.1332e-02, -1.9577e-02],\n",
            "        ...,\n",
            "        [-1.4961e-02,  3.7231e-02, -1.9882e-02,  ..., -8.3084e-03,\n",
            "          1.5366e-02,  1.9722e-03],\n",
            "        [-1.6830e-02, -4.0207e-03,  3.0121e-02,  ..., -2.8259e-02,\n",
            "          2.7313e-02, -5.7487e-03],\n",
            "        [ 1.0414e-03,  2.3331e-02,  4.3716e-03,  ...,  1.9455e-02,\n",
            "         -1.1086e-02,  1.0590e-02]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn1_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_k.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_k.lora_down.weight', tensor([[-0.0193, -0.0193,  0.0101,  ...,  0.0044, -0.0013, -0.0201],\n",
            "        [-0.0163, -0.0198, -0.0057,  ...,  0.0204,  0.0166,  0.0085],\n",
            "        [-0.0035,  0.0046, -0.0009,  ...,  0.0106,  0.0081, -0.0068],\n",
            "        ...,\n",
            "        [-0.0076,  0.0009, -0.0047,  ..., -0.0094, -0.0151, -0.0050],\n",
            "        [ 0.0129, -0.0144,  0.0067,  ...,  0.0181, -0.0064,  0.0186],\n",
            "        [ 0.0120,  0.0200, -0.0180,  ...,  0.0136,  0.0011,  0.0124]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_k.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_out_0.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight', tensor([[ 0.0077,  0.0266, -0.0143,  ...,  0.0055, -0.0027, -0.0300],\n",
            "        [-0.0176,  0.0234,  0.0273,  ...,  0.0268, -0.0293, -0.0329],\n",
            "        [ 0.0089, -0.0020, -0.0100,  ...,  0.0261, -0.0212,  0.0002],\n",
            "        ...,\n",
            "        [-0.0244,  0.0044,  0.0369,  ..., -0.0090,  0.0143, -0.0262],\n",
            "        [ 0.0045, -0.0278, -0.0143,  ..., -0.0261,  0.0343,  0.0201],\n",
            "        [ 0.0382, -0.0227, -0.0067,  ...,  0.0300,  0.0237,  0.0085]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_q.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_q.lora_down.weight', tensor([[ 0.0116,  0.0163, -0.0389,  ...,  0.0248, -0.0080,  0.0356],\n",
            "        [-0.0297,  0.0213, -0.0018,  ...,  0.0110, -0.0038,  0.0172],\n",
            "        [ 0.0117,  0.0012, -0.0069,  ..., -0.0191,  0.0205,  0.0382],\n",
            "        ...,\n",
            "        [ 0.0023, -0.0054, -0.0317,  ...,  0.0213, -0.0229,  0.0380],\n",
            "        [-0.0042, -0.0302,  0.0390,  ..., -0.0390,  0.0014, -0.0334],\n",
            "        [ 0.0374,  0.0216, -0.0042,  ..., -0.0339, -0.0081, -0.0344]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_q.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_v.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_v.lora_down.weight', tensor([[-0.0127,  0.0165,  0.0046,  ..., -0.0019,  0.0145,  0.0128],\n",
            "        [ 0.0075,  0.0080, -0.0016,  ...,  0.0028,  0.0159,  0.0159],\n",
            "        [-0.0203,  0.0103, -0.0167,  ..., -0.0085, -0.0028, -0.0002],\n",
            "        ...,\n",
            "        [-0.0030,  0.0059, -0.0137,  ..., -0.0098,  0.0143,  0.0074],\n",
            "        [-0.0170, -0.0058, -0.0024,  ...,  0.0026, -0.0020,  0.0065],\n",
            "        [ 0.0133,  0.0139, -0.0034,  ...,  0.0069, -0.0070,  0.0158]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_attn2_to_v.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_ff_net_0_proj.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight', tensor([[-0.0045,  0.0264, -0.0160,  ...,  0.0361,  0.0343, -0.0038],\n",
            "        [ 0.0213,  0.0181,  0.0334,  ..., -0.0344, -0.0119,  0.0244],\n",
            "        [-0.0118,  0.0203, -0.0240,  ..., -0.0069,  0.0233, -0.0049],\n",
            "        ...,\n",
            "        [-0.0194,  0.0041,  0.0092,  ...,  0.0312, -0.0358, -0.0163],\n",
            "        [-0.0079, -0.0024,  0.0037,  ..., -0.0291, -0.0007, -0.0058],\n",
            "        [ 0.0350, -0.0026,  0.0374,  ...,  0.0045,  0.0153, -0.0192]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_ff_net_2.alpha', tensor(16., dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_ff_net_2.lora_down.weight', tensor([[ 0.0161,  0.0059, -0.0093,  ...,  0.0005,  0.0040,  0.0042],\n",
            "        [ 0.0054,  0.0120, -0.0021,  ..., -0.0186, -0.0138,  0.0173],\n",
            "        [ 0.0070, -0.0081, -0.0030,  ...,  0.0094,  0.0108,  0.0147],\n",
            "        ...,\n",
            "        [ 0.0033,  0.0169, -0.0170,  ..., -0.0066,  0.0141, -0.0024],\n",
            "        [-0.0186, -0.0087,  0.0024,  ..., -0.0098, -0.0177, -0.0035],\n",
            "        [ 0.0081,  0.0174, -0.0107,  ...,  0.0107, -0.0018,  0.0148]],\n",
            "       dtype=torch.float16)), ('lora_unet_output_blocks_5_1_transformer_blocks_1_ff_net_2.lora_up.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Charger le modèle de base sans LoRA\n",
        "model_path = \"/content/drive/MyDrive/Downloaded_models/v6.safetensors.aria2\"  # Remplace ce chemin par celui de ton modèle Pony Diffusion v6\n",
        "\n",
        "# Charger le pipeline Stable Diffusion sans LoRA\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\n",
        "\n",
        "# S'assurer que le modèle est sur le bon device (GPU ou CPU)\n",
        "pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Générer une image à partir d'un prompt\n",
        "prompt = \"A cute pony in a fantasy landscape\"  # Remplace le prompt par ce que tu veux tester\n",
        "\n",
        "# Générer l'image\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "# Afficher l'image générée\n",
        "image.show()\n"
      ],
      "metadata": {
        "id": "-bl8j0x7CeC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "227865e2de634dba9480fdba072b88ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be344c1b64a24603b089ab324d1d6f08",
              "IPY_MODEL_ac1d66e732b04b869ef23e99933599c1",
              "IPY_MODEL_37cd0c3d610c47448fe9126353e324b8"
            ],
            "layout": "IPY_MODEL_3ff09ac318714a95a14e99c9906c5d9b"
          }
        },
        "be344c1b64a24603b089ab324d1d6f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9f4f0c9fd964c2fafa9701b32174e31",
            "placeholder": "​",
            "style": "IPY_MODEL_38d48603b3544d7eb924b7ee04058575",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "ac1d66e732b04b869ef23e99933599c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c10279452f240828cb9be69dd577b98",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_749ef4abf2394697a2da0e16b022d0ec",
            "value": 6
          }
        },
        "37cd0c3d610c47448fe9126353e324b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8feb177b61c24b94b5940a7185b1ac20",
            "placeholder": "​",
            "style": "IPY_MODEL_d5853d4bed664a84911a1c912db08cfc",
            "value": " 6/6 [00:15&lt;00:00,  1.98s/it]"
          }
        },
        "3ff09ac318714a95a14e99c9906c5d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f4f0c9fd964c2fafa9701b32174e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d48603b3544d7eb924b7ee04058575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c10279452f240828cb9be69dd577b98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749ef4abf2394697a2da0e16b022d0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8feb177b61c24b94b5940a7185b1ac20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5853d4bed664a84911a1c912db08cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}