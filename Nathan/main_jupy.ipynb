{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What i did : \n",
    "\n",
    "- Use of Lama for fine- tuning datas\n",
    "- Making sillytavern works with chatgpt\n",
    "- Creation of a script to select the best part from an audio\n",
    "- Solve issues with TTS library ( either previous torch 2.5.1 and after 2.6.1) and make a script with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the main notebook for test and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Your input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here.\\nYour input text here'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load the tokenizer and model with the use_auth_token parameter\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\",\n",
    "    \n",
    ")\n",
    "\n",
    "# Now you can use the pipeline for text generation\n",
    "result = pipe(\n",
    "    \"Your input text here\",\n",
    "    max_length=100,  # Increase this value for longer text generation\n",
    "    num_return_sequences=1,  # Number of sequences to generate\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'How are you doing? I hope you are doing well. Today we are going to learn about the famous Indian singer and actor, A.R. Rahman. He has won many awards and is one of the most respected and successful musicians in India. He is a recipient of the Padma Shri in 2008 and the Padma Bhushan in 2010. He is a recipient of the Grammy Award for Best Compilation Soundtrack Album for Visual Media in 2006. He has'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe(\n",
    "    \"How are you doing?\",\n",
    "    max_length=100,  # Increase this value for longer text generation\n",
    "    num_return_sequences=1,  # Number of sequences to generate\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
